pmid,abstract,sentence,count,location,design,label
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. ",12,1,Retrospective Studies,1
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. ",12,2,Retrospective Studies,3
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.",This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. ,12,3,Retrospective Studies,0
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. ",12,4,Retrospective Studies,0
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. ",12,5,Retrospective Studies,0
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.",Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. ,12,6,Retrospective Studies,0
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. ",12,7,Retrospective Studies,3
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). ",12,8,Retrospective Studies,3
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). ",12,9,Retrospective Studies,3
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ",12,10,Retrospective Studies,0
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.",ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. ,12,11,Retrospective Studies,0
26261577,"Aldosterone-to-renin ratio (ARR) is a screening test for primary aldosteronism, but it was impacted by a bunch of clinical covariates. The ARR is associated with chronic kidney disease (CKD), renal artery stenosis, renin adenoma. This study aims to investigate relationship between ARR and primary aldosteronism in CKD patients. A retrospective observational analysis involves 253 attendees from Urology Department of Chengdu Military General Hospital (China), comprising 146 patients with confirmed primary aldosteronism, 56 patients with essential hypertension, and 55 patients with chronic kidney disease accounting for primary kidney disease. Blood samples were drawn from patients with particular restriction for measuring serum aldosteronism, plasma renin activity, and serum potassium. Receiver operating characteristic (ROC) curve of ARR was tested to establish cutoff values and to assess sensitivity and specificity. The results showed that LogARR values were significantly higher (P < 0.001), and PRA and serum potassium values were significantly lower (P < 0.001) in primary aldosteronism patients. By contrast, significantly higher serum aldosterone and plasma renin were observed in CKDs compared with the other two groups (P < 0.001). There was a significantly positive correlation between LogARR and serum potassium (r = -0.0345, P < 0.001, R(2) = 0.093). The AUC for plasma renin activity, logARR, and serum aldosterone are 0.855, 0.84, and 0.501, respectively. ROC curve of logARR and plasma renin activity in detection of primary aldosteronism with higher sensitivity and specificity. In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.","In conclusion, this study indicated that the ARR act as the biomarker for the primary aldosteronism, and could distinguish from chronic kidney disease.",12,12,Retrospective Studies,3
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.",Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. ,11,1,Retrospective Studies,0
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.","Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. ",11,2,Retrospective Studies,0
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.","Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n\u200a=\u200a200) or RAPN (n\u200a=\u200a505) for renal tumors between January 2000 and September 2016. ",11,3,Retrospective Studies,0
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.","After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. ",11,4,Retrospective Studies,0
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.",The primary endpoint was postoperative NRS of pain intensity. ,11,5,Retrospective Studies,0
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.","The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.",11,6,Retrospective Studies,0
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.",Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups.,11,7,Retrospective Studies,3
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.",There was no significant difference in NRS of pain intensity between the 2 groups. ,11,8,Retrospective Studies,3
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.","Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3\u200amg of morphine-equivalent dose), but not from POD 1 to POD 4. ",11,9,Retrospective Studies,3
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.",The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. ,11,10,Retrospective Studies,3
28723795,"Robot-assisted partial nephrectomy (RAPN) has emerged as an alternative to laparoscopic partial nephrectomy (LPN) for removal of renal tumors. Several advantages of robotic surgery have been reported, but there is no comparative study on postoperative pain between the 2 techniques. Therefore, we compared the postoperative numerical rating scale (NRS) of pain intensity between patients who underwent LPN and those who underwent RAPN.We included 705 patients who underwent either LPN (n = 200) or RAPN (n = 505) for renal tumors between January 2000 and September 2016. After 1:1 propensity score matching, the final analysis included 142 patients each in the LPN and RAPN groups. The primary endpoint was postoperative NRS of pain intensity. The secondary endpoints were opioid requirement, opioid-related complications, and duration of hospital stay.Preoperative and intraoperative values of propensity score matched patients (n = 284) were not significantly different between the LPN and RAPN groups. There was no significant difference in NRS of pain intensity between the 2 groups. Opioid requirement was different between the 2 groups on postoperative day (POD) 0 (12.4 vs 11.3 mg of morphine-equivalent dose), but not from POD 1 to POD 4. The incidence of opioid-related complications and duration of hospital stay were not significantly different between the 2 groups.Postoperative pain was not significantly different between patients who underwent RAPN and those who underwent LPN. This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.",This result provides a potentially useful knowledge of postoperative pain characteristics in RAPN and LPN.,11,11,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. ,12,1,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.","In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). ",12,2,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. ,12,3,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. ,12,4,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.","Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. ",12,5,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",A 32-element cardiac coil was used for data acquisition. ,12,6,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). ,12,7,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. ,12,8,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. ,12,9,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",The diameter of the CS ostium in the superoinferior direction (1.13\xa0±\xa00.26\xa0cm) was larger than in the anteroposterior direction (0.82\xa0±\xa00.19\xa0cm) (P\xa0<\xa00.05). ,12,10,Retrospective Studies,3
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.","The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0\xa0±\xa00.0, 3.4\xa0±\xa00.5, 3.4\xa0±\xa00.5, 3.0\xa0±\xa00.8, and 3.3\xa0±\xa00.5, respectively. ",12,11,Retrospective Studies,0
21120611,"This study was designed to evaluate the value of contrast-enhanced whole-heart coronary MRA (CMRA) at 3.0T in depicting the cardiac venous anatomy. In cardiac resynchronization therapy (CRT), left ventricular (LV) pacing is achieved by positioning the LV lead in one of the tributaries of the coronary sinus (CS). Pre-implantation knowledge of the venous anatomy may help determine whether transvenous LV lead placement for CRT is feasible. Images of 51 subjects undergoing contrast-enhanced whole-heart CMRA at 3.0T were retrospectively analyzed. Data acquisition was performed using electrocardiography-triggered, navigator-gated, inversion-recovery prepared, segmented gradient-echo sequence. A 32-element cardiac coil was used for data acquisition. The visibility of the cardiac veins was graded visually using a 4-point scale (1: poor-4: excellent). The paired Student t test was used to evaluate differences in diameters of the ostium of the CS in anteroposterior and superoinferior direction. The cardiac veins were finally evaluated in 48 subjects with three anatomic variations. The diameter of the CS ostium in the superoinferior direction (1.13 ± 0.26 cm) was larger than in the anteroposterior direction (0.82 ± 0.19 cm) (P < 0.05). The mean visibility score of CS, posterior interventricular vein, posterior vein of the left ventricle, left marginal vein, and anterior interventricular vein was 4.0 ± 0.0, 3.4 ± 0.5, 3.4 ± 0.5, 3.0 ± 0.8, and 3.3 ± 0.5, respectively. In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.","In conclusion, contrast-enhanced whole-heart CMRA at 3.0T can depict the normal and variant cardiac venous anatomy.",12,12,Retrospective Studies,0
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.","Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. ",8,1,Retrospective Studies,0
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.","We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. ",8,2,Retrospective Studies,0
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.","Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. ",8,3,Retrospective Studies,0
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.",Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). ,8,4,Retrospective Studies,0
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.","C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. ",8,5,Retrospective Studies,3
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.","Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. ",8,6,Retrospective Studies,0
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.",Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. ,8,7,Retrospective Studies,0
21786334,"Correlation of serologic titers for Chlamydia trachomatis with other tests has been based on direct fluorescence antibody (DFA) testing and culture, but not on nucleic acid-based tests that are used for screening. We retrospectively reviewed the specificity of antibodies against C. trachomatis, Chlamydia psittaci, and Chlamydophila pneumoniae by microimmunofluorescence (MIF) when compared with DFA, culture, nucleic acid probe, and transcription-mediated amplification. Over a 6-year period, 226 cases had both MIF and one of these other methods performed for comparison. Agreement between C. trachomatis antigen or nucleic acid detection and MIF results was 87% (197/226). C. trachomatis serology had a negative predictive value of 98%, and 10.6% of cases were positive by serology and negative by antigen testing. Of the 13 patients who had a positive C. trachomatis antigen or nucleic acid test result, 9 had IgG and/or IgM titers highest against C. trachomatis, 3 had IgG titers highest against C. pneumoniae, and 1 had undetectable titers for the three chlamydial species. Twenty-five patients had positive IgG and/or IgM titers to C. trachomatis but negative antigen test results. Serologic testing can increase the sensitivity of detecting C. trachomatis infections.",Serologic testing can increase the sensitivity of detecting C. trachomatis infections.,8,8,Retrospective Studies,1
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.","Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. ",9,1,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.",We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. ,9,2,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.",The median age of the patients was 45 yr. ,9,3,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.","Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. ",9,4,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.","The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). ",9,5,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.",The most common procedure was lobectomy (48%). ,9,6,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.","Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). ",9,7,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.",Negative sputum culture conversion was achieved and maintained in all except two mortalities. ,9,8,Retrospective Studies,0
18583873,"Although the treatment of pulmonary diseases due to nontuberculous mycobacteria (NTM) requires the long-term use of antibiotics in combination, the treatment success rates are unsatisfactory. We evaluated the clinical characteristics and surgical outcomes of 23 patients with NTM lung diseases who had underwent pulmonary resection. The median age of the patients was 45 yr. Of the 23 patients, 10 had Mycobacterium avium-intracellulare complex infection, 12 had M. abscessus infection, and one had M. xenopi infection. The indications for surgery were antibiotic therapy failure (n=11), remnant cavitary lesion with high probability of relapse (n=8), and massive hemoptysis (n=4). The most common procedure was lobectomy (48%). Postoperative complications occurred in eight patients (35%), including postoperative pneumonia (n=3) and late bronchopleural fistula (n=2). Negative sputum culture conversion was achieved and maintained in all except two mortalities. Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.","Although it is associated with a relatively high complication rate, patients with NTM lung disease whose disease is localized to one lung and who can tolerate resectional surgery might be considered for surgery, if there has been poor response to drug therapy or if the patients develop significant disease-related complications such as hemoptysis.",9,9,Retrospective Studies,3
26235506,"Elevation in liver transaminases is common in patients treated with the marine antitumor agent trabectedin. However, the impact of trabectedin-related transaminase elevations on treatment outcomes is unclear. This retrospective study investigated the correlation between liver tests abnormalities and treatment outcomes in patients with unresectable advanced or metastatic soft tissue sarcomas (STS) treated with trabectedin 1.5 mg/m(2) once every 3 weeks at three reference centers in Italy. The effect of grade 3/4 elevations in alanine aminotransferase (ALT) or aspartate aminotransferase (AST) during the first two cycles and at any time during trabectedin treatment on progression-free survival (PFS) and overall survival (OS) were analyzed. Liver tests abnormalities during the first two cycles of chemotherapy or at any time during trabectedin treatment did not significantly affect PFS or OS. Nor were survival outcomes significantly different in the subgroups of patients with or without ALT/AST increases or with ALT/AST elevations ≥ 15 × the upper limit of normal (ULN) versus those with ALT/AST elevation < 15 × ULN. Although liver tests abnormalities are common in patients treated with trabectedin, elevations in ALT and AST are usually transient, occur during the first two cycles of treatment, and do not appear to affect survival.",Elevation in liver transaminases is common in patients treated with the marine antitumor agent trabectedin. ,7,1,Retrospective Studies,3
26235506,"Elevation in liver transaminases is common in patients treated with the marine antitumor agent trabectedin. However, the impact of trabectedin-related transaminase elevations on treatment outcomes is unclear. This retrospective study investigated the correlation between liver tests abnormalities and treatment outcomes in patients with unresectable advanced or metastatic soft tissue sarcomas (STS) treated with trabectedin 1.5 mg/m(2) once every 3 weeks at three reference centers in Italy. The effect of grade 3/4 elevations in alanine aminotransferase (ALT) or aspartate aminotransferase (AST) during the first two cycles and at any time during trabectedin treatment on progression-free survival (PFS) and overall survival (OS) were analyzed. Liver tests abnormalities during the first two cycles of chemotherapy or at any time during trabectedin treatment did not significantly affect PFS or OS. Nor were survival outcomes significantly different in the subgroups of patients with or without ALT/AST increases or with ALT/AST elevations ≥ 15 × the upper limit of normal (ULN) versus those with ALT/AST elevation < 15 × ULN. Although liver tests abnormalities are common in patients treated with trabectedin, elevations in ALT and AST are usually transient, occur during the first two cycles of treatment, and do not appear to affect survival.","However, the impact of trabectedin-related transaminase elevations on treatment outcomes is unclear. ",7,2,Retrospective Studies,0
26235506,"Elevation in liver transaminases is common in patients treated with the marine antitumor agent trabectedin. However, the impact of trabectedin-related transaminase elevations on treatment outcomes is unclear. This retrospective study investigated the correlation between liver tests abnormalities and treatment outcomes in patients with unresectable advanced or metastatic soft tissue sarcomas (STS) treated with trabectedin 1.5 mg/m(2) once every 3 weeks at three reference centers in Italy. The effect of grade 3/4 elevations in alanine aminotransferase (ALT) or aspartate aminotransferase (AST) during the first two cycles and at any time during trabectedin treatment on progression-free survival (PFS) and overall survival (OS) were analyzed. Liver tests abnormalities during the first two cycles of chemotherapy or at any time during trabectedin treatment did not significantly affect PFS or OS. Nor were survival outcomes significantly different in the subgroups of patients with or without ALT/AST increases or with ALT/AST elevations ≥ 15 × the upper limit of normal (ULN) versus those with ALT/AST elevation < 15 × ULN. Although liver tests abnormalities are common in patients treated with trabectedin, elevations in ALT and AST are usually transient, occur during the first two cycles of treatment, and do not appear to affect survival.",This retrospective study investigated the correlation between liver tests abnormalities and treatment outcomes in patients with unresectable advanced or metastatic soft tissue sarcomas (STS) treated with trabectedin 1.5 mg/m(2) once every 3 weeks at three reference centers in Italy. ,7,3,Retrospective Studies,0
26235506,"Elevation in liver transaminases is common in patients treated with the marine antitumor agent trabectedin. However, the impact of trabectedin-related transaminase elevations on treatment outcomes is unclear. This retrospective study investigated the correlation between liver tests abnormalities and treatment outcomes in patients with unresectable advanced or metastatic soft tissue sarcomas (STS) treated with trabectedin 1.5 mg/m(2) once every 3 weeks at three reference centers in Italy. The effect of grade 3/4 elevations in alanine aminotransferase (ALT) or aspartate aminotransferase (AST) during the first two cycles and at any time during trabectedin treatment on progression-free survival (PFS) and overall survival (OS) were analyzed. Liver tests abnormalities during the first two cycles of chemotherapy or at any time during trabectedin treatment did not significantly affect PFS or OS. Nor were survival outcomes significantly different in the subgroups of patients with or without ALT/AST increases or with ALT/AST elevations ≥ 15 × the upper limit of normal (ULN) versus those with ALT/AST elevation < 15 × ULN. Although liver tests abnormalities are common in patients treated with trabectedin, elevations in ALT and AST are usually transient, occur during the first two cycles of treatment, and do not appear to affect survival.",The effect of grade 3/4 elevations in alanine aminotransferase (ALT) or aspartate aminotransferase (AST) during the first two cycles and at any time during trabectedin treatment on progression-free survival (PFS) and overall survival (OS) were analyzed. ,7,4,Retrospective Studies,0
26235506,"Elevation in liver transaminases is common in patients treated with the marine antitumor agent trabectedin. However, the impact of trabectedin-related transaminase elevations on treatment outcomes is unclear. This retrospective study investigated the correlation between liver tests abnormalities and treatment outcomes in patients with unresectable advanced or metastatic soft tissue sarcomas (STS) treated with trabectedin 1.5 mg/m(2) once every 3 weeks at three reference centers in Italy. The effect of grade 3/4 elevations in alanine aminotransferase (ALT) or aspartate aminotransferase (AST) during the first two cycles and at any time during trabectedin treatment on progression-free survival (PFS) and overall survival (OS) were analyzed. Liver tests abnormalities during the first two cycles of chemotherapy or at any time during trabectedin treatment did not significantly affect PFS or OS. Nor were survival outcomes significantly different in the subgroups of patients with or without ALT/AST increases or with ALT/AST elevations ≥ 15 × the upper limit of normal (ULN) versus those with ALT/AST elevation < 15 × ULN. Although liver tests abnormalities are common in patients treated with trabectedin, elevations in ALT and AST are usually transient, occur during the first two cycles of treatment, and do not appear to affect survival.",Liver tests abnormalities during the first two cycles of chemotherapy or at any time during trabectedin treatment did not significantly affect PFS or OS. ,7,5,Retrospective Studies,1
26235506,"Elevation in liver transaminases is common in patients treated with the marine antitumor agent trabectedin. However, the impact of trabectedin-related transaminase elevations on treatment outcomes is unclear. This retrospective study investigated the correlation between liver tests abnormalities and treatment outcomes in patients with unresectable advanced or metastatic soft tissue sarcomas (STS) treated with trabectedin 1.5 mg/m(2) once every 3 weeks at three reference centers in Italy. The effect of grade 3/4 elevations in alanine aminotransferase (ALT) or aspartate aminotransferase (AST) during the first two cycles and at any time during trabectedin treatment on progression-free survival (PFS) and overall survival (OS) were analyzed. Liver tests abnormalities during the first two cycles of chemotherapy or at any time during trabectedin treatment did not significantly affect PFS or OS. Nor were survival outcomes significantly different in the subgroups of patients with or without ALT/AST increases or with ALT/AST elevations ≥ 15 × the upper limit of normal (ULN) versus those with ALT/AST elevation < 15 × ULN. Although liver tests abnormalities are common in patients treated with trabectedin, elevations in ALT and AST are usually transient, occur during the first two cycles of treatment, and do not appear to affect survival.",Nor were survival outcomes significantly different in the subgroups of patients with or without ALT/AST increases or with ALT/AST elevations ≥ 15 × the upper limit of normal (ULN) versus those with ALT/AST elevation < 15\u2009×\u2009ULN. ,7,6,Retrospective Studies,3
25439394,The duration of untreated psychosis (DUP) is a key determinant in the severity of symptoms in patients with schizophrenia. DUP is a modifiable factor that if reduced can improve patient outcome and treatment response. We sought to decrease DUP in rural Argentina by instituting annual training of local health agents to better identify signs of mental illness and offer earlier intervention. DUP was estimated using Schedules of Clinical Assessment in Neuropsychiatry (SCAN). Ongoing training was correlated with a reduction in DUP. Reducing DUP through better screening can decrease the psychosocial burden of disease and improve the trajectory of psychosis.,The duration of untreated psychosis (DUP) is a key determinant in the severity of symptoms in patients with schizophrenia. ,6,1,Retrospective Studies,3
25439394,The duration of untreated psychosis (DUP) is a key determinant in the severity of symptoms in patients with schizophrenia. DUP is a modifiable factor that if reduced can improve patient outcome and treatment response. We sought to decrease DUP in rural Argentina by instituting annual training of local health agents to better identify signs of mental illness and offer earlier intervention. DUP was estimated using Schedules of Clinical Assessment in Neuropsychiatry (SCAN). Ongoing training was correlated with a reduction in DUP. Reducing DUP through better screening can decrease the psychosocial burden of disease and improve the trajectory of psychosis.,DUP is a modifiable factor that if reduced can improve patient outcome and treatment response. ,6,2,Retrospective Studies,1
25439394,The duration of untreated psychosis (DUP) is a key determinant in the severity of symptoms in patients with schizophrenia. DUP is a modifiable factor that if reduced can improve patient outcome and treatment response. We sought to decrease DUP in rural Argentina by instituting annual training of local health agents to better identify signs of mental illness and offer earlier intervention. DUP was estimated using Schedules of Clinical Assessment in Neuropsychiatry (SCAN). Ongoing training was correlated with a reduction in DUP. Reducing DUP through better screening can decrease the psychosocial burden of disease and improve the trajectory of psychosis.,We sought to decrease DUP in rural Argentina by instituting annual training of local health agents to better identify signs of mental illness and offer earlier intervention. ,6,3,Retrospective Studies,0
25439394,The duration of untreated psychosis (DUP) is a key determinant in the severity of symptoms in patients with schizophrenia. DUP is a modifiable factor that if reduced can improve patient outcome and treatment response. We sought to decrease DUP in rural Argentina by instituting annual training of local health agents to better identify signs of mental illness and offer earlier intervention. DUP was estimated using Schedules of Clinical Assessment in Neuropsychiatry (SCAN). Ongoing training was correlated with a reduction in DUP. Reducing DUP through better screening can decrease the psychosocial burden of disease and improve the trajectory of psychosis.,DUP was estimated using Schedules of Clinical Assessment in Neuropsychiatry (SCAN). ,6,4,Retrospective Studies,0
25439394,The duration of untreated psychosis (DUP) is a key determinant in the severity of symptoms in patients with schizophrenia. DUP is a modifiable factor that if reduced can improve patient outcome and treatment response. We sought to decrease DUP in rural Argentina by instituting annual training of local health agents to better identify signs of mental illness and offer earlier intervention. DUP was estimated using Schedules of Clinical Assessment in Neuropsychiatry (SCAN). Ongoing training was correlated with a reduction in DUP. Reducing DUP through better screening can decrease the psychosocial burden of disease and improve the trajectory of psychosis.,Ongoing training was correlated with a reduction in DUP. ,6,5,Retrospective Studies,3
25439394,The duration of untreated psychosis (DUP) is a key determinant in the severity of symptoms in patients with schizophrenia. DUP is a modifiable factor that if reduced can improve patient outcome and treatment response. We sought to decrease DUP in rural Argentina by instituting annual training of local health agents to better identify signs of mental illness and offer earlier intervention. DUP was estimated using Schedules of Clinical Assessment in Neuropsychiatry (SCAN). Ongoing training was correlated with a reduction in DUP. Reducing DUP through better screening can decrease the psychosocial burden of disease and improve the trajectory of psychosis.,Reducing DUP through better screening can decrease the psychosocial burden of disease and improve the trajectory of psychosis.,6,6,Retrospective Studies,1
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.",The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. ,8,1,Retrospective Studies,0
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.","With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. ",8,2,Retrospective Studies,3
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.","Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. ",8,3,Retrospective Studies,0
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.","Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. ",8,4,Retrospective Studies,0
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.","Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. ",8,5,Retrospective Studies,0
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.",Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. ,8,6,Retrospective Studies,0
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.","These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. ",8,7,Retrospective Studies,2
27725639,"The oligometastatic state has been proposed as an intermediate stage of cancer spread between localized disease and widespread metastases. With improvements in diagnostic modalities such as functional imaging, oligometastatic prostate cancer is being diagnosed with greater frequency than ever before. Furthermore, the paradigm for treatment of advanced prostate cancers is shifting toward a more aggressive approach. Many questions surround the understanding of the process and consequences of oligometastasis, meaning that the contemporary literature offers a wide variety of definitions of oligometastatic prostate cancer. Until genomic data exist to provide a biological component to the definition of oligometastatic disease, a clinical diagnosis made on the basis of up to five extrapelvic lesions is reasonable for use. Retrospective studies suggest that interventions such as radical prostatectomy and local or metastasis-directed radiotherapy can be performed in the metastatic setting with minimal risk of toxic effects. These therapies seem to decrease the need for subsequent palliative interventions, but insufficient data are available to draw reliable conclusions regarding their effect on survival. Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.","Thus, a protocol for clinicians to manage the patient presenting with oligometastatic prostate cancer would be a useful clinical tool.",8,8,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.",A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. ,8,1,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.",Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. ,8,2,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.",This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. ,8,3,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.",One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. ,8,4,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.",Pain treatment increased progressively in complexity. ,8,5,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.","Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. ",8,6,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.",Management of pain after limb-sparing surgery has evolved over the 26 years of this review. ,8,7,Retrospective Studies,0
21620310,"A significant proportion of patients report long-term pain that is >=5 on a 0-10 intensity scale after limb-sparing surgery for malignancies of the long bones. Patients experience several distinct types of pain after limb-sparing surgery which constitute a complex clinical entity. This retrospective study examined 26 years of experience in a pediatric institution (1981-2007) in pain management as long as 6 months after limb-sparing surgery and reviewed the historical evolution of pain interventions. One hundred fifty patients underwent 151 limb-salvage surgeries for bone cancer of the extremities in this series. Pain treatment increased progressively in complexity. Therapies included opioids, nonsteroidal antiinflammatory drugs, acetaminophen-opioid combinations, postoperative continuous epidural infusion, anticonvulsants and tricyclic antidepressants for neuropathic pain, local anesthetic wound catheters, and continuous peripheral nerve block catheters. Management of pain after limb-sparing surgery has evolved over the 26 years of this review. It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.","It currently relies on multiple ""layers"" of pharmacologic and nonpharmacologic strategies to address the complex mixed nociceptive and neuropathic mechanisms of pain in this patient population.",8,8,Retrospective Studies,0
15224168,"The purpose of this retrospective study was to investigate the incidence of bone metastases as the first sign of metastatic spread in patients with primary solid malignant tumours. Between January 1987 and December 1998, we treated 867 patients suffering from primary solid malignant tumours. Their average age was 67 (range: 30-96) years and all were thoroughly investigated with a complete physical examination and laboratory tests as well as imaging studies and bone scans. No bone metastases were found at the time of the initial diagnosis, and the patients were then re-assessed every 6 months for the first 5 years and then once a year. We found that, regardless of treatment, bone metastases appeared in a certain number of patients and that after excluding patients with prostate cancer a bone metastasis was the first sign of ""recurrence"" in 1.3% of the patients with a known primary solid malignant tumour.",The purpose of this retrospective study was to investigate the incidence of bone metastases as the first sign of metastatic spread in patients with primary solid malignant tumours. ,5,1,Retrospective Studies,0
15224168,"The purpose of this retrospective study was to investigate the incidence of bone metastases as the first sign of metastatic spread in patients with primary solid malignant tumours. Between January 1987 and December 1998, we treated 867 patients suffering from primary solid malignant tumours. Their average age was 67 (range: 30-96) years and all were thoroughly investigated with a complete physical examination and laboratory tests as well as imaging studies and bone scans. No bone metastases were found at the time of the initial diagnosis, and the patients were then re-assessed every 6 months for the first 5 years and then once a year. We found that, regardless of treatment, bone metastases appeared in a certain number of patients and that after excluding patients with prostate cancer a bone metastasis was the first sign of ""recurrence"" in 1.3% of the patients with a known primary solid malignant tumour.","Between January 1987 and December 1998, we treated 867 patients suffering from primary solid malignant tumours. ",5,2,Retrospective Studies,0
15224168,"The purpose of this retrospective study was to investigate the incidence of bone metastases as the first sign of metastatic spread in patients with primary solid malignant tumours. Between January 1987 and December 1998, we treated 867 patients suffering from primary solid malignant tumours. Their average age was 67 (range: 30-96) years and all were thoroughly investigated with a complete physical examination and laboratory tests as well as imaging studies and bone scans. No bone metastases were found at the time of the initial diagnosis, and the patients were then re-assessed every 6 months for the first 5 years and then once a year. We found that, regardless of treatment, bone metastases appeared in a certain number of patients and that after excluding patients with prostate cancer a bone metastasis was the first sign of ""recurrence"" in 1.3% of the patients with a known primary solid malignant tumour.",Their average age was 67 (range: 30-96) years and all were thoroughly investigated with a complete physical examination and laboratory tests as well as imaging studies and bone scans. ,5,3,Retrospective Studies,0
15224168,"The purpose of this retrospective study was to investigate the incidence of bone metastases as the first sign of metastatic spread in patients with primary solid malignant tumours. Between January 1987 and December 1998, we treated 867 patients suffering from primary solid malignant tumours. Their average age was 67 (range: 30-96) years and all were thoroughly investigated with a complete physical examination and laboratory tests as well as imaging studies and bone scans. No bone metastases were found at the time of the initial diagnosis, and the patients were then re-assessed every 6 months for the first 5 years and then once a year. We found that, regardless of treatment, bone metastases appeared in a certain number of patients and that after excluding patients with prostate cancer a bone metastasis was the first sign of ""recurrence"" in 1.3% of the patients with a known primary solid malignant tumour.","No bone metastases were found at the time of the initial diagnosis, and the patients were then re-assessed every 6 months for the first 5 years and then once a year. ",5,4,Retrospective Studies,0
15224168,"The purpose of this retrospective study was to investigate the incidence of bone metastases as the first sign of metastatic spread in patients with primary solid malignant tumours. Between January 1987 and December 1998, we treated 867 patients suffering from primary solid malignant tumours. Their average age was 67 (range: 30-96) years and all were thoroughly investigated with a complete physical examination and laboratory tests as well as imaging studies and bone scans. No bone metastases were found at the time of the initial diagnosis, and the patients were then re-assessed every 6 months for the first 5 years and then once a year. We found that, regardless of treatment, bone metastases appeared in a certain number of patients and that after excluding patients with prostate cancer a bone metastasis was the first sign of ""recurrence"" in 1.3% of the patients with a known primary solid malignant tumour.","We found that, regardless of treatment, bone metastases appeared in a certain number of patients and that after excluding patients with prostate cancer a bone metastasis was the first sign of ""recurrence"" in 1.3% of the patients with a known primary solid malignant tumour.",5,5,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.","Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. ",13,1,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.",This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. ,13,2,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.","A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. ",13,3,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.",Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). ,13,4,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.","Following propensity score matching analysis, 2582 patients were included in each group. ",13,5,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.",Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. ,13,6,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.","Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. ",13,7,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.",The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P\u200a<\u200a0.001). ,13,8,Retrospective Studies,0
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.","There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. ",13,9,Retrospective Studies,3
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.",Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P\u200a<\u200a0.001). ,13,10,Retrospective Studies,3
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.","However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P\u200a<\u200a0.001). ",13,11,Retrospective Studies,3
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.",Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. ,13,12,Retrospective Studies,1
26886620,"Although remifentanil is used widely by many clinicians during general anesthesia, there are recent evidences of opioid-induced hyperalgesia as an adverse effect. This study aimed to determine if intraoperative remifentanil infusion caused increased pain during the postoperative period in patients who underwent a thyroidectomy. A total of 7511 patients aged ≥ 20 years, who underwent thyroidectomy between January 2009 and December 2013 at the Asan Medical Center were retrospectively analyzed. Enrolled patients were divided into 2 groups: group N (no intraoperative remifentanil and only volatile maintenance anesthesia) and group R (intraoperative remifentanil infusion including total intravenous anesthesia and balanced anesthesia). Following propensity score matching analysis, 2582 patients were included in each group. Pain scores based on numeric rating scales (NRS) were compared between the 2 groups at the postoperative anesthetic care unit and at the ward until 3 days postoperation. Incidences of postoperative complications, such as nausea, itching, and shivering were also compared. The estimated NRS pain score on the day of surgery was 5.08 (95% confidence interval [CI] 4.97-5.19) in group N patients and 6.73 (95% CI 6.65-6.80) in group R patients (P < 0.001). There were no statistically significant differences in NRS scores on postoperative days 1, 2, and 3 between the 2 groups. Postoperative nausea was less frequent in group R (31.4%) than in group N (53.5%) (P < 0.001). However, the incidence of itching was higher in group R (4.3%) than in group N (0.7%) (P < 0.001). Continuous infusion of remifentanil during general anesthesia can cause higher intensity of postoperative pain and more frequent itching than general anesthesia without remifentanil infusion immediately after thyroidectomy. Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.","Considering the advantages and disadvantages of continuous remifentanil infusion, volatile anesthesia without opioid may be a good choice for minor surgeries, such as thyroidectomy.",13,13,Retrospective Studies,0
22734892,"Autologous stem cell transplantation (ASCT) is the gold standard therapy for suitable multiple myeloma (MM) patients after induction with high dose therapy. To date, the evidence of a reliable marker of prognosis in these cases remains scarce. Our aim was to evaluate appearance of unrelated atypical serum immunofixation patterns (ASIPs) as a marker of prognosis in MM patients submitted to ASCT. We retrospectively analysed data from 65 patients. Interestingly, we observed that presence of ASIPs was associated with longer progression-free survival and longer overall survival. Our results suggested that presence of ASIPs could be a novel marker of good prognosis in MM patients submitted to ASCT.",Autologous stem cell transplantation (ASCT) is the gold standard therapy for suitable multiple myeloma (MM) patients after induction with high dose therapy. ,6,1,Retrospective Studies,0
22734892,"Autologous stem cell transplantation (ASCT) is the gold standard therapy for suitable multiple myeloma (MM) patients after induction with high dose therapy. To date, the evidence of a reliable marker of prognosis in these cases remains scarce. Our aim was to evaluate appearance of unrelated atypical serum immunofixation patterns (ASIPs) as a marker of prognosis in MM patients submitted to ASCT. We retrospectively analysed data from 65 patients. Interestingly, we observed that presence of ASIPs was associated with longer progression-free survival and longer overall survival. Our results suggested that presence of ASIPs could be a novel marker of good prognosis in MM patients submitted to ASCT.","To date, the evidence of a reliable marker of prognosis in these cases remains scarce. ",6,2,Retrospective Studies,0
22734892,"Autologous stem cell transplantation (ASCT) is the gold standard therapy for suitable multiple myeloma (MM) patients after induction with high dose therapy. To date, the evidence of a reliable marker of prognosis in these cases remains scarce. Our aim was to evaluate appearance of unrelated atypical serum immunofixation patterns (ASIPs) as a marker of prognosis in MM patients submitted to ASCT. We retrospectively analysed data from 65 patients. Interestingly, we observed that presence of ASIPs was associated with longer progression-free survival and longer overall survival. Our results suggested that presence of ASIPs could be a novel marker of good prognosis in MM patients submitted to ASCT.",Our aim was to evaluate appearance of unrelated atypical serum immunofixation patterns (ASIPs) as a marker of prognosis in MM patients submitted to ASCT. ,6,3,Retrospective Studies,0
22734892,"Autologous stem cell transplantation (ASCT) is the gold standard therapy for suitable multiple myeloma (MM) patients after induction with high dose therapy. To date, the evidence of a reliable marker of prognosis in these cases remains scarce. Our aim was to evaluate appearance of unrelated atypical serum immunofixation patterns (ASIPs) as a marker of prognosis in MM patients submitted to ASCT. We retrospectively analysed data from 65 patients. Interestingly, we observed that presence of ASIPs was associated with longer progression-free survival and longer overall survival. Our results suggested that presence of ASIPs could be a novel marker of good prognosis in MM patients submitted to ASCT.",We retrospectively analysed data from 65 patients. ,6,4,Retrospective Studies,0
22734892,"Autologous stem cell transplantation (ASCT) is the gold standard therapy for suitable multiple myeloma (MM) patients after induction with high dose therapy. To date, the evidence of a reliable marker of prognosis in these cases remains scarce. Our aim was to evaluate appearance of unrelated atypical serum immunofixation patterns (ASIPs) as a marker of prognosis in MM patients submitted to ASCT. We retrospectively analysed data from 65 patients. Interestingly, we observed that presence of ASIPs was associated with longer progression-free survival and longer overall survival. Our results suggested that presence of ASIPs could be a novel marker of good prognosis in MM patients submitted to ASCT.","Interestingly, we observed that presence of ASIPs was associated with longer progression-free survival and longer overall survival. ",6,5,Retrospective Studies,3
22734892,"Autologous stem cell transplantation (ASCT) is the gold standard therapy for suitable multiple myeloma (MM) patients after induction with high dose therapy. To date, the evidence of a reliable marker of prognosis in these cases remains scarce. Our aim was to evaluate appearance of unrelated atypical serum immunofixation patterns (ASIPs) as a marker of prognosis in MM patients submitted to ASCT. We retrospectively analysed data from 65 patients. Interestingly, we observed that presence of ASIPs was associated with longer progression-free survival and longer overall survival. Our results suggested that presence of ASIPs could be a novel marker of good prognosis in MM patients submitted to ASCT.",Our results suggested that presence of ASIPs could be a novel marker of good prognosis in MM patients submitted to ASCT.,6,6,Retrospective Studies,3
12091510,"The number of artificial cardiac pacemakers is increasing, as is the number of bodies being cremated. Because of the explosive potential of pacemakers when heated, a statutory question on the cremation form asks whether the deceased has a pacemaker and if so whether it has been removed. We sent a questionnaire to all the crematoria in the UK enquiring about the frequency, consequences and prevention of pacemaker explosions. We found that about half of all crematoria in the UK experience pacemaker explosions, that pacemaker explosions may cause structural damage and injury and that most crematoria staff are unaware of the explosive potential of implantable cardiac defibrillators. Crematoria staff rely on the accurate completion of cremation forms, and doctors who sign cremation forms have a legal obligation to provide such information.","The number of artificial cardiac pacemakers is increasing, as is the number of bodies being cremated. ",5,1,Retrospective Studies,0
12091510,"The number of artificial cardiac pacemakers is increasing, as is the number of bodies being cremated. Because of the explosive potential of pacemakers when heated, a statutory question on the cremation form asks whether the deceased has a pacemaker and if so whether it has been removed. We sent a questionnaire to all the crematoria in the UK enquiring about the frequency, consequences and prevention of pacemaker explosions. We found that about half of all crematoria in the UK experience pacemaker explosions, that pacemaker explosions may cause structural damage and injury and that most crematoria staff are unaware of the explosive potential of implantable cardiac defibrillators. Crematoria staff rely on the accurate completion of cremation forms, and doctors who sign cremation forms have a legal obligation to provide such information.","Because of the explosive potential of pacemakers when heated, a statutory question on the cremation form asks whether the deceased has a pacemaker and if so whether it has been removed. ",5,2,Retrospective Studies,0
12091510,"The number of artificial cardiac pacemakers is increasing, as is the number of bodies being cremated. Because of the explosive potential of pacemakers when heated, a statutory question on the cremation form asks whether the deceased has a pacemaker and if so whether it has been removed. We sent a questionnaire to all the crematoria in the UK enquiring about the frequency, consequences and prevention of pacemaker explosions. We found that about half of all crematoria in the UK experience pacemaker explosions, that pacemaker explosions may cause structural damage and injury and that most crematoria staff are unaware of the explosive potential of implantable cardiac defibrillators. Crematoria staff rely on the accurate completion of cremation forms, and doctors who sign cremation forms have a legal obligation to provide such information.","We sent a questionnaire to all the crematoria in the UK enquiring about the frequency, consequences and prevention of pacemaker explosions. ",5,3,Retrospective Studies,0
12091510,"The number of artificial cardiac pacemakers is increasing, as is the number of bodies being cremated. Because of the explosive potential of pacemakers when heated, a statutory question on the cremation form asks whether the deceased has a pacemaker and if so whether it has been removed. We sent a questionnaire to all the crematoria in the UK enquiring about the frequency, consequences and prevention of pacemaker explosions. We found that about half of all crematoria in the UK experience pacemaker explosions, that pacemaker explosions may cause structural damage and injury and that most crematoria staff are unaware of the explosive potential of implantable cardiac defibrillators. Crematoria staff rely on the accurate completion of cremation forms, and doctors who sign cremation forms have a legal obligation to provide such information.","We found that about half of all crematoria in the UK experience pacemaker explosions, that pacemaker explosions may cause structural damage and injury and that most crematoria staff are unaware of the explosive potential of implantable cardiac defibrillators. ",5,4,Retrospective Studies,2
12091510,"The number of artificial cardiac pacemakers is increasing, as is the number of bodies being cremated. Because of the explosive potential of pacemakers when heated, a statutory question on the cremation form asks whether the deceased has a pacemaker and if so whether it has been removed. We sent a questionnaire to all the crematoria in the UK enquiring about the frequency, consequences and prevention of pacemaker explosions. We found that about half of all crematoria in the UK experience pacemaker explosions, that pacemaker explosions may cause structural damage and injury and that most crematoria staff are unaware of the explosive potential of implantable cardiac defibrillators. Crematoria staff rely on the accurate completion of cremation forms, and doctors who sign cremation forms have a legal obligation to provide such information.","Crematoria staff rely on the accurate completion of cremation forms, and doctors who sign cremation forms have a legal obligation to provide such information.",5,5,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. ,12,1,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. ,12,2,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. ,12,3,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. ,12,4,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",Morphomic data were extracted from CT images using a custom algorithm. ,12,5,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. ,12,6,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.","A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. ",12,7,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.","The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. ",12,8,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). ,12,9,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. ,12,10,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.","Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. ",12,11,Retrospective Studies,0
29632017,"A consistent approach to the dosing of aminoglycosides across the modern body size distribution has been elusive. We evaluated whether radiologically derived measures of body composition could explain more of the interpatient variability in aminoglycoside pharmacokinetics (PK) than standard body size metrics. This retrospective study included adult patients treated with gentamicin or tobramycin with at least three drug concentrations and computed tomography (CT) imaging available. Aminoglycoside volume and clearance (CL) estimates were computed using a two-compartment model by Bayesian analysis. Morphomic data were extracted from CT images using a custom algorithm. Bivariable and multivariable linear regression were used to assess relationships between PK parameters and covariates. A total of 335 patients were included with a median (minimum, maximum) of 4 (3, 16) aminoglycoside concentrations per patient. The median (minimum, maximum) age, height, and weight of included patients were 57 (21, 93) years, 170 (145, 203) centimeters, and 81 (42, 187) kilograms. Both standard and morphomic measures poorly explained variability in volume (R2 < 0.06). Skeletal muscle area and volume explained more of the interpatient variability in CL than weight or sex. Higher precision was observed using a modified Cockcroft-Gault equation with skeletal muscle area at L3 (R2= 0.38) or L4 (R2= 0.37) than the standard Cockcroft-Gault equation using lean (R2= 0.23), adjusted (R2= 0.23), or total (R2= 0.22) body weights. These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.",These results highlight that skeletal muscle measurements from CT images obtained in the course of care can improve the precision of aminoglycoside CL estimation over current body size scalars.,12,12,Retrospective Studies,1
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.",Most patients suffering from cancer die of metastatic disease. ,9,1,Retrospective Studies,0
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.","Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. ",9,2,Retrospective Studies,0
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.",We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. ,9,3,Retrospective Studies,0
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.",Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ,9,4,Retrospective Studies,3
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.",ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. ,9,5,Retrospective Studies,0
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.","In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. ",9,6,Retrospective Studies,3
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.","We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ",9,7,Retrospective Studies,0
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.",ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. ,9,8,Retrospective Studies,1
25775533,"Most patients suffering from cancer die of metastatic disease. Surgical removal of solid tumors is performed as an initial attempt to cure patients; however, surgery is often accompanied with trauma, which can promote early recurrence by provoking detachment of tumor cells into the blood stream or inducing systemic inflammation or both. We have previously reported that administration of atrial natriuretic peptide (ANP) during the perioperative period reduces inflammatory response and has a prophylactic effect on postoperative cardiopulmonary complications in lung cancer surgery. Here we demonstrate that cancer recurrence after curative surgery was significantly lower in ANP-treated patients than in control patients (surgery alone). ANP is known to bind specifically to NPR1 [also called guanylyl cyclase-A (GC-A) receptor]. In mouse models, we found that metastasis of GC-A-nonexpressing tumor cells (i.e., B16 mouse melanoma cells) to the lung was increased in vascular endothelium-specific GC-A knockout mice and decreased in vascular endothelium-specific GC-A transgenic mice compared with control mice. We examined the effect of ANP on tumor metastasis in mice treated with lipopolysaccharide, which mimics systemic inflammation induced by surgical stress. ANP inhibited the adhesion of cancer cells to pulmonary arterial and micro-vascular endothelial cells by suppressing the E-selectin expression that is promoted by inflammation. These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.",These results suggest that ANP prevents cancer metastasis by inhibiting the adhesion of tumor cells to inflamed endothelial cells.,9,9,Retrospective Studies,1
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.","Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. ",9,1,Retrospective Studies,3
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.","However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. ",9,2,Retrospective Studies,0
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.","In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. ",9,3,Retrospective Studies,0
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.",We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. ,9,4,Retrospective Studies,0
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.","Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. ",9,5,Retrospective Studies,0
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.","The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. ",9,6,Retrospective Studies,3
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.","Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. ",9,7,Retrospective Studies,3
25291377,"Placental inflammatory response (PIR) is associated with adverse neonatal outcomes such as sepsis, cerebral palsy, low birth weight, preterm birth, and neonatal mortality. However, there is an urgent need for noninvasive and sensitive biomarkers for prediction of PIR. In this study, we evaluated the clinical usefulness of maternal serum inflammatory markers for prediction of PIR in women with impending preterm birth. We conducted a retrospective cohort study of 483 patients who delivered preterm neonates. Serum levels of leukocyte differential counts, C-reactive protein (CRP), and neutrophil to lymphocyte ratio (NLR) were compared between women with no placental inflammation and women with PIR. The mean neutrophil counts, CRP levels, and NLR in both the patients with histologic chorioamnionitis (HCA) alone and those with HCA with funisitis were significantly higher than those in women with no placental inflammation. Compared to leukocyte subset or CRP, NLR in women with funisitis was significantly higher than in women with HCA alone and showed higher predictive accuracy, along with 71.4% sensitivity, 77.9% specificity, 80.7% positive predictive value, and 67.8% negative predictive value for prediction of PIR. On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. NLR may be a predictive marker of PIR and could be used as a cost-effective parameter for identifying women at risk of PIR.","On Kaplan-Meier survival analysis, women with both an elevated level of CRP and a high NLR had a shorter admission-to-delivery interval compared to women with either an elevated level of CRP or a high NLR alone. ",9,8,Retrospective Studies,3
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.",BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. ,11,1,Retrospective Studies,0
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.",MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. ,11,2,Retrospective Studies,0
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.","According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. ",11,3,Retrospective Studies,0
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.",The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. ,11,4,Retrospective Studies,0
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.",RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). ,11,5,Retrospective Studies,0
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.","The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. ",11,6,Retrospective Studies,3
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.","Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). ",11,7,Retrospective Studies,3
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.","In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). ",11,8,Retrospective Studies,3
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.","In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). ",11,9,Retrospective Studies,3
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.",CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. ,11,10,Retrospective Studies,0
30547901,"BACKGROUND This study was conducted to observe the influence of different time intervals between prior cervical conization and posterior hysterectomy on postoperative infection in female patients with cervical intraepithelial neoplasia or cancer. MATERIAL AND METHODS Medical records of 170 patients who underwent hysterectomy following cervical conization between November 2010 and September 2016 at the Zhenjiang 4th Hospital were reviewed. According to the interval between hysterectomy and cervical conization, patients were classified into 1-2-week, 4-5-week, and 6-week groups. The outcomes of 46 patients who underwent conization with iodoform gauze inside the vagina were observed. RESULTS The total postoperative infection rate after hysterectomy was 25.3% (43/170). The expression levels of tumor necrosis factor-α (TNF-α), interleukin-6 (IL-6), and high mobility group box 1 (HMGB1) in the cervical secretions and tissues were found to gradually increase, peaking at 2 weeks after conization, then significantly decreasing 3-6 weeks onwards. Compared with the 1-2-week group, the 4-5-week and 6-week groups exhibited significantly lower infection rates (2/42, 4.8%, 4-5-week group; 0%, 0/33, 6-week group; vs. 41/95, 43.2%, 1-2-week group; p<0.001). In the 1-2-week group in particular, the postoperative infection rate after laparoscopic hysterectomy was significantly higher than the rate after abdominal hysterectomy (21/35, 60% vs. 20/60, 33%, p=0.0177). In addition, the vaginal and cervical wound infection rates after conization in patients treated with iodoform were significantly lower than the rates in those without iodoform treatment (p<0.05). CONCLUSIONS Hysterectomy should be performed at least 4 weeks after conization. Treatment with iodoform would be beneficial.",Treatment with iodoform would be beneficial.,11,11,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.",Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). ,13,1,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.","Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. ",13,2,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.",Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. ,13,3,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.","In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. ",13,4,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.",Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. ,13,5,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.","Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. ",13,6,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.","Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. ",13,7,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.","Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. ",13,8,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.","Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. ",13,9,Retrospective Studies,0
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.",RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. ,13,10,Retrospective Studies,1
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.",Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. ,13,11,Retrospective Studies,3
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.",Radiographic scoring at each time interval showed reduction in both number of nodules (P = \u200a<0.0001) and largest nodule diameter (P =\u200a <0.0001) in all patients for at least 18 months following B cell depletion. ,13,12,Retrospective Studies,3
25501085,"Pulmonary nodule formation is a frequent feature of granulomatosis with polyangiitis (GPA). Traditional induction therapy includes methotrexate or cyclophosphamide, however, pulmonary nodules generally respond slower than vasculitic components of disease. Efficacy of rituximab (RTX) solely for the treatment of pulmonary nodules has not been assessed. In this observational cohort study, we report patient outcomes with RTX in GPA patients with pulmonary nodules who failed to achieve remission following conventional immunosuppression. Patients (n = 5) with persistent pulmonary nodules were identified from our clinic database and retrospectively evaluated. Systemic manifestations, inflammatory markers, disease activity, concurrent immunosuppression, and absolute B cell numbers were recorded pre-RTX and at 6 monthly intervals following treatment. Chest radiographs at each time point were scored by an experienced radiologist, blinded to clinical details. Five patients with GPA and PR3-ANCA were evaluated (2 male, 3 female), mean age 34 (22-52) years. Pulmonary nodules (median 4, range 2-6), with or without cavitation were present in all patients. RTX induced initial B cell depletion (<5 cells/μL) in all patients but re-population was observed in 3 patients. Repeated RTX treatment in these 3 and persistent B cell depletion in the whole cohort was associated with further significant radiological improvement. Radiographic scoring at each time interval showed reduction in both number of nodules (P =  <0.0001) and largest nodule diameter (P =  <0.0001) in all patients for at least 18 months following B cell depletion. In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.","In summary, RTX therapy induces resolution of pulmonary granulomatous inflammation in GPA following prolonged B cell depletion.",13,13,Retrospective Studies,1
7647596,"Transesophageal echocardiography and 2-dimensional transthoracic echocardiography have proved to be extremely valuable in the diagnosis of cardiac masses. In this report, we review the echocardiographic findings, clinical history, and histopathologic findings in 21 patients with intracardiac masses who underwent transthoracic echocardiography, transesophageal echocardiography, or both, at our institution. Of these patients, 14 had benign masses and 7 had malignant tumors. The potential role of transesophageal echocardiography in the diagnosis and treatment of patients with intracardiac masses is discussed. We believe that transesophageal echocardiography offers the cardiologist and cardiovascular surgeon the capability of more accurate preoperative and intraoperative assessment of cardiac masses.",Transesophageal echocardiography and 2-dimensional transthoracic echocardiography have proved to be extremely valuable in the diagnosis of cardiac masses. ,5,1,Retrospective Studies,0
7647596,"Transesophageal echocardiography and 2-dimensional transthoracic echocardiography have proved to be extremely valuable in the diagnosis of cardiac masses. In this report, we review the echocardiographic findings, clinical history, and histopathologic findings in 21 patients with intracardiac masses who underwent transthoracic echocardiography, transesophageal echocardiography, or both, at our institution. Of these patients, 14 had benign masses and 7 had malignant tumors. The potential role of transesophageal echocardiography in the diagnosis and treatment of patients with intracardiac masses is discussed. We believe that transesophageal echocardiography offers the cardiologist and cardiovascular surgeon the capability of more accurate preoperative and intraoperative assessment of cardiac masses.","In this report, we review the echocardiographic findings, clinical history, and histopathologic findings in 21 patients with intracardiac masses who underwent transthoracic echocardiography, transesophageal echocardiography, or both, at our institution. ",5,2,Retrospective Studies,0
7647596,"Transesophageal echocardiography and 2-dimensional transthoracic echocardiography have proved to be extremely valuable in the diagnosis of cardiac masses. In this report, we review the echocardiographic findings, clinical history, and histopathologic findings in 21 patients with intracardiac masses who underwent transthoracic echocardiography, transesophageal echocardiography, or both, at our institution. Of these patients, 14 had benign masses and 7 had malignant tumors. The potential role of transesophageal echocardiography in the diagnosis and treatment of patients with intracardiac masses is discussed. We believe that transesophageal echocardiography offers the cardiologist and cardiovascular surgeon the capability of more accurate preoperative and intraoperative assessment of cardiac masses.","Of these patients, 14 had benign masses and 7 had malignant tumors. ",5,3,Retrospective Studies,0
7647596,"Transesophageal echocardiography and 2-dimensional transthoracic echocardiography have proved to be extremely valuable in the diagnosis of cardiac masses. In this report, we review the echocardiographic findings, clinical history, and histopathologic findings in 21 patients with intracardiac masses who underwent transthoracic echocardiography, transesophageal echocardiography, or both, at our institution. Of these patients, 14 had benign masses and 7 had malignant tumors. The potential role of transesophageal echocardiography in the diagnosis and treatment of patients with intracardiac masses is discussed. We believe that transesophageal echocardiography offers the cardiologist and cardiovascular surgeon the capability of more accurate preoperative and intraoperative assessment of cardiac masses.",The potential role of transesophageal echocardiography in the diagnosis and treatment of patients with intracardiac masses is discussed. ,5,4,Retrospective Studies,0
7647596,"Transesophageal echocardiography and 2-dimensional transthoracic echocardiography have proved to be extremely valuable in the diagnosis of cardiac masses. In this report, we review the echocardiographic findings, clinical history, and histopathologic findings in 21 patients with intracardiac masses who underwent transthoracic echocardiography, transesophageal echocardiography, or both, at our institution. Of these patients, 14 had benign masses and 7 had malignant tumors. The potential role of transesophageal echocardiography in the diagnosis and treatment of patients with intracardiac masses is discussed. We believe that transesophageal echocardiography offers the cardiologist and cardiovascular surgeon the capability of more accurate preoperative and intraoperative assessment of cardiac masses.",We believe that transesophageal echocardiography offers the cardiologist and cardiovascular surgeon the capability of more accurate preoperative and intraoperative assessment of cardiac masses.,5,5,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66).",13,1,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.",Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean).,13,2,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.",Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. ,13,3,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). ",13,4,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. ",13,5,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.",Secondary and tertiary hyperparathyroidism were seen. ,13,6,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. ",13,7,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. ",13,8,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. ",13,9,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.",Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. ,13,10,Retrospective Studies,0
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. ",13,11,Retrospective Studies,3
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.","There was no significant difference in bone mineral density (z score -0.31 v -0.77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. ",13,12,Retrospective Studies,3
11123392,"Bone mineral density was measured by dual energy x ray absorptiometry (DEXA) at the lumbar spine and femoral neck in 15 adults who had metabolic bone disease in association with coeliac disease (mean age at diagnosis 53.5 years, range 37 to 66). Results were expressed as a T score (the number of standard deviations by which patient's bone density differed from the sex matched young adult mean). Three patients had no skeletal symptoms and normal routine calcium biochemistry but severely reduced axial bone mineral density on DEXA. Eleven patients had symptomatic skeletal fractures, including fractures of proximal femur (3), vertebrae (4), and radius (6). Three patients had osteomalacia confirmed on bone biopsy, two of whom had characteristic biochemistry. Secondary and tertiary hyperparathyroidism were seen. Seventy five further patients (60 female) with coeliac disease (mean age 52.0 years, median duration of gluten-free diet 3.4 years) and 75 paired healthy age and sex matched controls were questioned on past fracture history. Patients with coeliac disease underwent detailed studies of calcium biochemistry, dietary intake, and bone mineral density. Sixteen had a past history of fractures (chi(2) = 10.7, p = 0.0004, v controls), which were of typical osteoporotic type. Ten patients had fracture before diagnosis of coeliac disease and six after diagnosis. Patients who had a fracture were older (56.3 v 50.3 years, p < 0.02, Wilcoxon rank sum test) than those with no fracture. There was no significant difference in bone mineral density (z score -0.31 v -0. 77), serum calcium (2.30 v 2.26 mmol/l), 25-hydroxyvitamin D (19.7 v 23.7 nmol/l), parathyroid hormone (2.6 v 3.1 pmol/l), or dietary calcium intake (1021.0 v 1033.0 mg/day) in patients with fracture compared with those without fracture. Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.",Metabolic bone disease is common in coeliac disease and is associated with premature osteoporotic fractures.,13,13,Retrospective Studies,3
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.",Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. ,10,1,RCT,0
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.","The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). ",10,2,RCT,0
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.",Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. ,10,3,RCT,0
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.","Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. ",10,4,RCT,3
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.","Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). ",10,5,RCT,3
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.",Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). ,10,6,RCT,3
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.",Tranexamic acid did not have a statistically significant effect on blood loss. ,10,7,RCT,3
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.",Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. ,10,8,RCT,3
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.","Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. ",10,9,RCT,3
7580360,"Sixty consecutive patients undergoing elective open-heart surgery were prospectively enrolled in a study to compare the efficacy of 3 different antifibrinolytic drugs to reduce postoperative bleeding and to reduce homologous blood requirements in combination with blood-saving techniques and restrictive indications for blood transfusion. The patients were randomized to 1 of 4 intraoperative treatment regimens: 1) control (no antifibrinolytic therapy); 2) epsilon-aminocaproic acid (10 g IV at induction of anesthesia, followed by infusion of 2 g/h for 5 hours); 3) tranexamic acid (10 mg/kg IV within 30 minutes after induction of anesthesia, followed by infusion of 1 mg/kg per hour for 10 hours); or 4) high-dose aprotinin (2 million KIU IV at induction of anesthesia and 2 million KIU added to the extracorporeal circuit, followed by infusion of 500 thousand KIU/h during surgery). Hemoconcentration and reinfusion of blood drained from the operative field and the extracorporeal circuit after operation were used in all patients. Indications for blood transfusion were hypotension, tachycardia, or both, with hemoglobin values < 8.5 g/dL; or severe anemia with hemoglobin values < 7 g/dL. Compared with the blood loss in the control group, patients receiving aprotinin and epsilon-aminocaproic acid showed significantly less postoperative blood loss at 1 hour (control, 128 +/- 94 mL; aprotinin, 54 +/- 47 mL, p = 0.01; and epsilon-aminocaproic acid, 69 +/- 35 mL, p = 0.03); this trend continued at 24 hours after operation (control, 724 +/- 280 mL; aprotinin, 344 +/- 106 mL, p < 0.0001; and epsilon-aminocaproic acid, 509 +/- 148 mL, p = 0.01). Aprotinin was significantly more efficient than epsilon-aminocaproic acid (p=0.002). Tranexamic acid did not have a statistically significant effect on blood loss. Homologous blood requirements were not significantly different among the groups; postoperative hematologic values and coagulation times were also comparable. Despite the efficacy of aprotinin and epsilon-aminocaproic acid shown in the present study, the blood requirements were not significantly different from those that are found when transfusions are restricted, autotransfusions are used, and blood from the operative field and extracorporeal circuit is concentrated and reinfused. Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.","Therefore, intraoperative antifibrinolysis may not be indicated in routine cardiac surgery when other blood-saving techniques are adopted.",10,10,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.",During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. ,11,1,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.",The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. ,11,2,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.",In acute cases there may not be time to perform a gas calibration. ,11,3,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.","We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. ",11,4,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.",We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). ,11,5,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.","Blood sampling was done at the beginning of bypass, and 30 minutes later. ",11,6,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.",After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. ,11,7,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.","Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. ",11,8,RCT,3
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.","Before in-vivo calibration, the CDI without gas calibration was completely unreliable. ",11,9,RCT,0
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.",After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. ,11,10,RCT,3
21848172,"During cardiopulmonary bypass blood gases can be analyzed with laboratory equipment or with an in-line monitor giving instant results. The manufacturer of the CDI 500 in-line blood gas monitor recommends gas calibration before use. In acute cases there may not be time to perform a gas calibration. We hypothesized that after calibration against laboratory results, the CDI values of pH, pO2, and pCO2 will keep the same level of accuracy, whether the CDI has been gas calibrated or not. We performed a prospective randomized observational study using a study group without gas calibration (29 patients) and a control group with gas calibration (29 patients). Blood sampling was done at the beginning of bypass, and 30 minutes later. After each blood sample the CDI was in-vivo calibrated to the values simultaneously obtained from the ABL. Before in-vivo calibration values from the CDI without gas calibration were significantly different from the ABL-values in accuracy as well as precision, whereas the results from the gas calibrated CDI were largely consistent with the ABL. Before in-vivo calibration, the CDI without gas calibration was completely unreliable. After in-vivo calibration there was no statistical difference between the values of the CDI with and without calibration. We recommend gas calibration of the CDI before use in the period before in-vivo calibration.",We recommend gas calibration of the CDI before use in the period before in-vivo calibration.,11,11,RCT,0
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.",The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. ,10,1,RCT,0
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.",A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. ,10,2,RCT,0
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.",Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). ,10,3,RCT,0
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.","Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. ",10,4,RCT,0
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.",We failed to detect a significant difference on fertilization rate between two groups. ,10,5,RCT,3
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.","However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). ",10,6,RCT,0
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.",The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). ,10,7,RCT,3
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.","It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. ",10,8,RCT,3
25337269,"The study was to investigate the effects of oxygen concentration at different levels for culturing pre-compaction embryos on human embryo development competence. A total of 1254 oocytes from 92 patients treated with conventional in vitro fertilization (IVF) were harvested in this study. Oocytes were randomly assigned to the atmospheric (~20%) or low (~5%) oxygen concentration groups on the retrieval day (day 0). Groups were compared with respect to fertilization rates, embryo development, and reproductive outcome. We failed to detect a significant difference on fertilization rate between two groups. However, the low oxygen group yielded more optimal embryos on day 3 when compared with the atmospheric group (72.4% vs. 64.2%). The low oxygen group had a significantly higher blastocyst formation rate than the atmospheric oxygen group (64.5% vs. 52.9%). It is seemly that the optimal blastocyst and frozen blastocyst rates was higher in the low oxygen group, but the data did not reach a statistical significance. Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. In conclusion, a low oxygen concentration may significantly improve the developmental potential of pre-compaction embryos, thus resulting in a positive effect on subsequent blastocyst cultivation and optimizing the treatment cycle.","Although the use of low oxygen will not affect the clinical outcome in the fresh cleavage-transfer cycles, but it will result in more favorable clinical outcomes in the subsequent warming blastocyst-transfer cycles, with statistically significantly higher clinical pregnancy rate (CPR) and implantation rate (IR) compared with atmospheric oxygen. ",10,9,RCT,1
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.",Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. ,8,1,RCT,1
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.","To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. ",8,2,RCT,1
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.","We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. ",8,3,RCT,0
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.","This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. ",8,4,RCT,0
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.","We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. ",8,5,RCT,0
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.","Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. ",8,6,RCT,3
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.",Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. ,8,7,RCT,0
29351650,"Well-documented associations between lifestyle behaviors and disease outcomes necessitate evidence-based health promotion interventions. To enhance potential efficacy and effectiveness, interventionists increasingly respond to community priorities, employ comprehensive theoretical frameworks, invest heavily to ensure cultural fit, implement evidence-based programming, and deploy research gold standards. We describe a project that followed all of these recommended strategies, but did not achieve desired outcomes. This community-based participatory research (CBPR) energy balance (diet and physical activity) intervention, conducted in Appalachian Kentucky among 900+ residents, employed a wait list control cluster randomized design. We engaged faith institutions, took an intergenerational approach, and modified two existing evidence-based interventions to enhance cultural relevance. Despite these efforts, fruit and vegetable consumption and physical activity did not change from baseline to post-test or differed significantly between intervention and wait list control groups. Barriers to engaging in optimal energy balance focused more on motivation and attitude than on structural and material barriers. The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.","The complex interplay of psychosocial, structural, and physiological processes offers significant challenges to groups with entrenched health challenges.",8,8,RCT,0
30389818,"This prospective randomized study assessed myocardial perfusion imaging with the high-sensitivity D.SPECT cadmium-zinc-telluride camera in a forward-leaning bikerlike position, which may potentially lower diaphragmatic attenuation and reduce breathing-related cardiac motion, in a manner comparable to the prone position proposed with other SPECT cameras. Methods: Patients referred for a stress-rest 99mTc-sestamibi protocol and positioned in the biker position, with the chest leaning forward on the D.SPECT camera-head at 35° from vertical, had an additional resting D.SPECT recording in the supine position (n = 40) or in the sitting position with the back rearward at 30° from vertical (n = 40). Segments with attenuation artifacts were defined as those with less than 65% uptake but with strictly normal contractility at gated SPECT and no defect reversibility from stress images. Results: The biker position was associated with lower heart-to-detector distances than the supine or sitting positions (both P < 0.001); lower cardiac motion amplitudes, assessed on panograms, than the supine position (P < 0.001); and fewer segments with attenuation artifacts than the supine position (on average, 1.10 ± 1.01 vs. 1.90 ± 1.74, P = 0.010) or the sitting position (0.75 ± 0.93 vs. 1.38 ± 1.60, P = 0.011). Conclusion: Myocardial perfusion images from D.SPECT are enhanced for patients positioned in a forward-leaning bikerlike position comparatively to sitting or supine positions, with a notably lower rate of attenuation artifacts.","This prospective randomized study assessed myocardial perfusion imaging with the high-sensitivity D.SPECT cadmium-zinc-telluride camera in a forward-leaning bikerlike position, which may potentially lower diaphragmatic attenuation and reduce breathing-related cardiac motion, in a manner comparable to the prone position proposed with other SPECT cameras. ",5,1,RCT,2
30389818,"This prospective randomized study assessed myocardial perfusion imaging with the high-sensitivity D.SPECT cadmium-zinc-telluride camera in a forward-leaning bikerlike position, which may potentially lower diaphragmatic attenuation and reduce breathing-related cardiac motion, in a manner comparable to the prone position proposed with other SPECT cameras. Methods: Patients referred for a stress-rest 99mTc-sestamibi protocol and positioned in the biker position, with the chest leaning forward on the D.SPECT camera-head at 35° from vertical, had an additional resting D.SPECT recording in the supine position (n = 40) or in the sitting position with the back rearward at 30° from vertical (n = 40). Segments with attenuation artifacts were defined as those with less than 65% uptake but with strictly normal contractility at gated SPECT and no defect reversibility from stress images. Results: The biker position was associated with lower heart-to-detector distances than the supine or sitting positions (both P < 0.001); lower cardiac motion amplitudes, assessed on panograms, than the supine position (P < 0.001); and fewer segments with attenuation artifacts than the supine position (on average, 1.10 ± 1.01 vs. 1.90 ± 1.74, P = 0.010) or the sitting position (0.75 ± 0.93 vs. 1.38 ± 1.60, P = 0.011). Conclusion: Myocardial perfusion images from D.SPECT are enhanced for patients positioned in a forward-leaning bikerlike position comparatively to sitting or supine positions, with a notably lower rate of attenuation artifacts.","Methods: Patients referred for a stress-rest 99mTc-sestamibi protocol and positioned in the biker position, with the chest leaning forward on the D.SPECT camera-head at 35° from vertical, had an additional resting D.SPECT recording in the supine position (n = 40) or in the sitting position with the back rearward at 30° from vertical (n = 40). ",5,2,RCT,0
30389818,"This prospective randomized study assessed myocardial perfusion imaging with the high-sensitivity D.SPECT cadmium-zinc-telluride camera in a forward-leaning bikerlike position, which may potentially lower diaphragmatic attenuation and reduce breathing-related cardiac motion, in a manner comparable to the prone position proposed with other SPECT cameras. Methods: Patients referred for a stress-rest 99mTc-sestamibi protocol and positioned in the biker position, with the chest leaning forward on the D.SPECT camera-head at 35° from vertical, had an additional resting D.SPECT recording in the supine position (n = 40) or in the sitting position with the back rearward at 30° from vertical (n = 40). Segments with attenuation artifacts were defined as those with less than 65% uptake but with strictly normal contractility at gated SPECT and no defect reversibility from stress images. Results: The biker position was associated with lower heart-to-detector distances than the supine or sitting positions (both P < 0.001); lower cardiac motion amplitudes, assessed on panograms, than the supine position (P < 0.001); and fewer segments with attenuation artifacts than the supine position (on average, 1.10 ± 1.01 vs. 1.90 ± 1.74, P = 0.010) or the sitting position (0.75 ± 0.93 vs. 1.38 ± 1.60, P = 0.011). Conclusion: Myocardial perfusion images from D.SPECT are enhanced for patients positioned in a forward-leaning bikerlike position comparatively to sitting or supine positions, with a notably lower rate of attenuation artifacts.",Segments with attenuation artifacts were defined as those with less than 65% uptake but with strictly normal contractility at gated SPECT and no defect reversibility from stress images. ,5,3,RCT,0
30389818,"This prospective randomized study assessed myocardial perfusion imaging with the high-sensitivity D.SPECT cadmium-zinc-telluride camera in a forward-leaning bikerlike position, which may potentially lower diaphragmatic attenuation and reduce breathing-related cardiac motion, in a manner comparable to the prone position proposed with other SPECT cameras. Methods: Patients referred for a stress-rest 99mTc-sestamibi protocol and positioned in the biker position, with the chest leaning forward on the D.SPECT camera-head at 35° from vertical, had an additional resting D.SPECT recording in the supine position (n = 40) or in the sitting position with the back rearward at 30° from vertical (n = 40). Segments with attenuation artifacts were defined as those with less than 65% uptake but with strictly normal contractility at gated SPECT and no defect reversibility from stress images. Results: The biker position was associated with lower heart-to-detector distances than the supine or sitting positions (both P < 0.001); lower cardiac motion amplitudes, assessed on panograms, than the supine position (P < 0.001); and fewer segments with attenuation artifacts than the supine position (on average, 1.10 ± 1.01 vs. 1.90 ± 1.74, P = 0.010) or the sitting position (0.75 ± 0.93 vs. 1.38 ± 1.60, P = 0.011). Conclusion: Myocardial perfusion images from D.SPECT are enhanced for patients positioned in a forward-leaning bikerlike position comparatively to sitting or supine positions, with a notably lower rate of attenuation artifacts.","Results: The biker position was associated with lower heart-to-detector distances than the supine or sitting positions (both P < 0.001); lower cardiac motion amplitudes, assessed on panograms, than the supine position (P < 0.001); and fewer segments with attenuation artifacts than the supine position (on average, 1.10 ± 1.01 vs. 1.90 ± 1.74, P = 0.010) or the sitting position (0.75 ± 0.93 vs. 1.38 ± 1.60, P = 0.011). ",5,4,RCT,3
30389818,"This prospective randomized study assessed myocardial perfusion imaging with the high-sensitivity D.SPECT cadmium-zinc-telluride camera in a forward-leaning bikerlike position, which may potentially lower diaphragmatic attenuation and reduce breathing-related cardiac motion, in a manner comparable to the prone position proposed with other SPECT cameras. Methods: Patients referred for a stress-rest 99mTc-sestamibi protocol and positioned in the biker position, with the chest leaning forward on the D.SPECT camera-head at 35° from vertical, had an additional resting D.SPECT recording in the supine position (n = 40) or in the sitting position with the back rearward at 30° from vertical (n = 40). Segments with attenuation artifacts were defined as those with less than 65% uptake but with strictly normal contractility at gated SPECT and no defect reversibility from stress images. Results: The biker position was associated with lower heart-to-detector distances than the supine or sitting positions (both P < 0.001); lower cardiac motion amplitudes, assessed on panograms, than the supine position (P < 0.001); and fewer segments with attenuation artifacts than the supine position (on average, 1.10 ± 1.01 vs. 1.90 ± 1.74, P = 0.010) or the sitting position (0.75 ± 0.93 vs. 1.38 ± 1.60, P = 0.011). Conclusion: Myocardial perfusion images from D.SPECT are enhanced for patients positioned in a forward-leaning bikerlike position comparatively to sitting or supine positions, with a notably lower rate of attenuation artifacts.","Conclusion: Myocardial perfusion images from D.SPECT are enhanced for patients positioned in a forward-leaning bikerlike position comparatively to sitting or supine positions, with a notably lower rate of attenuation artifacts.",5,5,RCT,3
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.","To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. ",8,1,RCT,0
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.","Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. ",8,2,RCT,0
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.","As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. ",8,3,RCT,0
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.",The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. ,8,4,RCT,0
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.",The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. ,8,5,RCT,0
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.","The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). ",8,6,RCT,3
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.","However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). ",8,7,RCT,3
8609071,"To clarify whether pre-operative transcatheter arterial embolization (TAE) improves survival after hepatectomy, a prospective randomized comparative study was done. Of a total of 115 registered patients having solitary hepatocellular carcinoma (HCC) 2 to 5 cm in diameter, 18 (15.7%) were excluded after randomization. As a result, 97 patients were chosen as subjects and divided into two groups: hepatectomy with (group A: n=50) and without (group B: n=47) pre-operative TAE. The period of observation of the patients who survived the surgery was between 4.0 and 6.6 years. The randomization appeared to have provided well-balanced groups of patients and the clinico-pathological characteristics of the two groups were quite similar. The necrotic part of the cancerous lesions, as confirmed by operative specimens, amounted to 74.8+/-33.4% (mean +/-SD) in group A and 6.8+/-7.2% in group B (P<0.01). However, the cancer-free survival rates after hepatectomy in both groups showed little difference (39.1+/-7.0 (%+/-SE) and 31.1+/-0.1, respectively). We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.",We speculate that TAE is not effective against such HCC accessory lesions as minute intrahepatic metastasis and tumor thrombus and that pre-operative TAE does not improve post-operative survival.,8,8,RCT,1
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.",How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. ,11,1,RCT,0
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.",We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. ,11,2,RCT,0
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.","To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. ",11,3,RCT,0
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.",Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ,11,4,RCT,0
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.",ACTH was measured every 10 min during the last 8 h of the infusions. ,11,5,RCT,0
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.","The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ",11,6,RCT,0
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.","ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). ",11,7,RCT,3
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.","In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). ",11,8,RCT,3
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.",E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). ,11,9,RCT,1
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.",T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. ,11,10,RCT,3
24573184,"How sex steroids modulate glucocorticoid feedback on the hypothalamic-pituitary-corticotrope (HPC) unit is controversial in humans. We postulated that testosterone (T) in men and estradiol (E2) in women govern unstressed cortisol-mediated negative feedback on ACTH secretion. To test this hypothesis, 24 men and 24 women age 58 ± 2.4 yr were pretreated with leuprolide and either sex steroid (E2 in women, T in men) or placebo addback. Placebo or ketoconazole (KTCZ) was administered overnight to inhibit adrenal steroidogenesis during overnight 14-h intravenous infusions of saline or cortisol in a continuous versus pulsatile manner to test for feedback differences. ACTH was measured every 10 min during the last 8 h of the infusions. The main outcome measures were mean ACTH concentrations, pulsatile ACTH secretion, and ACTH approximate entropy (ApEn). ACTH concentrations were lower in women than men (P < 0.01), and in women in the E2+ compared with E2- group under both continuous (P = 0.01) and pulsatile (P = 0.006) cortisol feedback, despite higher cortisol binding globulin and lower free cortisol levels in women than men (P < 0.01). In the combined groups, under both modes of cortisol addback, ACTH concentrations, pulsatile ACTH secretion, and ACTH secretory-burst mass correlated negatively and univariately with E2 levels (each P < 0.005). E2 also suppressed ACTH ApEn (process randomness) during continuous cortisol feedback (P = 0.004). T had no univariate effect but was a positive correlate of ACTH when assessed jointly with E2 (negative) under cortisol pulses. In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.","In conclusion, sex steroids modulate selective gender-related hypothalamic-pituitary adrenal-axis adaptations to cortisol feedback in unstressed humans.",11,11,RCT,1
23785038,"Using a longitudinal cluster-randomized controlled design, we examined whether students' reading outcomes differed when they received 1, 2, or 3 years of individualized reading instruction from first through third grade, compared with a treated control group. More than 45% of students came from families living in poverty. Following students, we randomly assigned their teachers each year to deliver individualized reading instruction or a treated control condition intervention focused on mathematics. Students who received individualized reading instruction in all three grades showed the strongest reading skills by the end of third grade compared with those who received fewer years of such instruction. There was inconsistent evidence supporting a sustained first-grade treatment effect: Individualized instruction in first grade was necessary but not sufficient for stronger third-grade reading outcomes. These effects were achieved by regular classroom teachers who received professional development, which indicates that policies that support the use of evidence-based reading instruction and teacher training can yield increased student achievement.","Using a longitudinal cluster-randomized controlled design, we examined whether students' reading outcomes differed when they received 1, 2, or 3 years of individualized reading instruction from first through third grade, compared with a treated control group.",6,1,RCT,0
23785038,"Using a longitudinal cluster-randomized controlled design, we examined whether students' reading outcomes differed when they received 1, 2, or 3 years of individualized reading instruction from first through third grade, compared with a treated control group. More than 45% of students came from families living in poverty. Following students, we randomly assigned their teachers each year to deliver individualized reading instruction or a treated control condition intervention focused on mathematics. Students who received individualized reading instruction in all three grades showed the strongest reading skills by the end of third grade compared with those who received fewer years of such instruction. There was inconsistent evidence supporting a sustained first-grade treatment effect: Individualized instruction in first grade was necessary but not sufficient for stronger third-grade reading outcomes. These effects were achieved by regular classroom teachers who received professional development, which indicates that policies that support the use of evidence-based reading instruction and teacher training can yield increased student achievement.",More than 45% of students came from families living in poverty. ,6,2,RCT,0
23785038,"Using a longitudinal cluster-randomized controlled design, we examined whether students' reading outcomes differed when they received 1, 2, or 3 years of individualized reading instruction from first through third grade, compared with a treated control group. More than 45% of students came from families living in poverty. Following students, we randomly assigned their teachers each year to deliver individualized reading instruction or a treated control condition intervention focused on mathematics. Students who received individualized reading instruction in all three grades showed the strongest reading skills by the end of third grade compared with those who received fewer years of such instruction. There was inconsistent evidence supporting a sustained first-grade treatment effect: Individualized instruction in first grade was necessary but not sufficient for stronger third-grade reading outcomes. These effects were achieved by regular classroom teachers who received professional development, which indicates that policies that support the use of evidence-based reading instruction and teacher training can yield increased student achievement.","Following students, we randomly assigned their teachers each year to deliver individualized reading instruction or a treated control condition intervention focused on mathematics. ",6,3,RCT,0
23785038,"Using a longitudinal cluster-randomized controlled design, we examined whether students' reading outcomes differed when they received 1, 2, or 3 years of individualized reading instruction from first through third grade, compared with a treated control group. More than 45% of students came from families living in poverty. Following students, we randomly assigned their teachers each year to deliver individualized reading instruction or a treated control condition intervention focused on mathematics. Students who received individualized reading instruction in all three grades showed the strongest reading skills by the end of third grade compared with those who received fewer years of such instruction. There was inconsistent evidence supporting a sustained first-grade treatment effect: Individualized instruction in first grade was necessary but not sufficient for stronger third-grade reading outcomes. These effects were achieved by regular classroom teachers who received professional development, which indicates that policies that support the use of evidence-based reading instruction and teacher training can yield increased student achievement.",Students who received individualized reading instruction in all three grades showed the strongest reading skills by the end of third grade compared with those who received fewer years of such instruction. ,6,4,RCT,3
23785038,"Using a longitudinal cluster-randomized controlled design, we examined whether students' reading outcomes differed when they received 1, 2, or 3 years of individualized reading instruction from first through third grade, compared with a treated control group. More than 45% of students came from families living in poverty. Following students, we randomly assigned their teachers each year to deliver individualized reading instruction or a treated control condition intervention focused on mathematics. Students who received individualized reading instruction in all three grades showed the strongest reading skills by the end of third grade compared with those who received fewer years of such instruction. There was inconsistent evidence supporting a sustained first-grade treatment effect: Individualized instruction in first grade was necessary but not sufficient for stronger third-grade reading outcomes. These effects were achieved by regular classroom teachers who received professional development, which indicates that policies that support the use of evidence-based reading instruction and teacher training can yield increased student achievement.",There was inconsistent evidence supporting a sustained first-grade treatment effect: Individualized instruction in first grade was necessary but not sufficient for stronger third-grade reading outcomes. ,6,5,RCT,0
23785038,"Using a longitudinal cluster-randomized controlled design, we examined whether students' reading outcomes differed when they received 1, 2, or 3 years of individualized reading instruction from first through third grade, compared with a treated control group. More than 45% of students came from families living in poverty. Following students, we randomly assigned their teachers each year to deliver individualized reading instruction or a treated control condition intervention focused on mathematics. Students who received individualized reading instruction in all three grades showed the strongest reading skills by the end of third grade compared with those who received fewer years of such instruction. There was inconsistent evidence supporting a sustained first-grade treatment effect: Individualized instruction in first grade was necessary but not sufficient for stronger third-grade reading outcomes. These effects were achieved by regular classroom teachers who received professional development, which indicates that policies that support the use of evidence-based reading instruction and teacher training can yield increased student achievement.","These effects were achieved by regular classroom teachers who received professional development, which indicates that policies that support the use of evidence-based reading instruction and teacher training can yield increased student achievement.",6,6,RCT,1
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.",Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. ,12,1,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.",Recovery from an acute episode is followed by 4-6 months of relapse prevention. ,12,2,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.","After that, long-term maintenance treatment is administered to avoid recurrence. ",12,3,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.","We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. ",12,4,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.","An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. ",12,5,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.",After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. ,12,6,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.",Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. ,12,7,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.","Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. ",12,8,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.","The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). ",12,9,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.","Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. ",12,10,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.",The trial was designed according to current consensus and guidance. ,12,11,RCT,0
15297901,"Unipolar major depression is often a chronic disease that may require lifelong prophylaxis. Recovery from an acute episode is followed by 4-6 months of relapse prevention. After that, long-term maintenance treatment is administered to avoid recurrence. We present the rationale and design of an ongoing double-blind, randomized, placebo-controlled trial investigating the efficacy of Hypericum extract WS 5570 in relapse prevention in recurrent unipolar depression. An estimated sample of 425 adults with recurrent, mild to moderate major depression (ICD-10 and DSM-IV criteria), > or = 3 previous episodes (last 5 years) and a total score > or = 20 points on the 17-item Hamilton Rating Scale for Depression (HAMD) will be included. After a one-week wash out patients receive 3 x 300 mg/day WS 5570 single-blind for 6 weeks. Responders are randomized to 26 weeks of double-blind continuation treatment with 3 x 300 mg/day WS 5570 or placebo. Patients completing continuation treatment without relapse enter 52 weeks of doubleblind maintenance treatment, where those treated with WS 5570 are re-randomized to 3 x 300 mg/day WS 5570 or placebo. The primary outcome measure is the time to relapse during continuation treatment (HAMD > or = 16, clinical diagnosis of depression, or premature treatment termination for inefficacy). Hypericum extract, with its favourable tolerability profile, could be an interesting option for long-term prophylaxis. The trial was designed according to current consensus and guidance. Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.","Notably, it includes long-term prophylactic treatment with the same drug and the same therapeutic dose applied during acute treatment, uses well-defined outcome measures and provides a clear distinction between relapse and recurrence.",12,12,RCT,0
27720142,"This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. The end point was BP < 140/90 mm Hg at 6-month follow-up. Fourteen baseline patient characteristics were selected a priori as subgroup variables. Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9 years, and mean BP was 149/86 mm Hg. The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P < .001). The effect of the intervention was significantly larger in patients who were younger (interaction P = .02), did not have diabetes (P = .005), had high baseline diastolic BP (P = .02), added salt less than daily in food preparation (P = .007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P = .02). These findings may help prioritize patients for whom the intervention is most effective.",This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. ,7,1,RCT,0
27720142,"This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. The end point was BP < 140/90 mm Hg at 6-month follow-up. Fourteen baseline patient characteristics were selected a priori as subgroup variables. Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9 years, and mean BP was 149/86 mm Hg. The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P < .001). The effect of the intervention was significantly larger in patients who were younger (interaction P = .02), did not have diabetes (P = .005), had high baseline diastolic BP (P = .02), added salt less than daily in food preparation (P = .007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P = .02). These findings may help prioritize patients for whom the intervention is most effective.",The end point was BP\xa0<\xa0140/90\xa0mm Hg at 6-month follow-up. ,7,2,RCT,0
27720142,"This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. The end point was BP < 140/90 mm Hg at 6-month follow-up. Fourteen baseline patient characteristics were selected a priori as subgroup variables. Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9 years, and mean BP was 149/86 mm Hg. The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P < .001). The effect of the intervention was significantly larger in patients who were younger (interaction P = .02), did not have diabetes (P = .005), had high baseline diastolic BP (P = .02), added salt less than daily in food preparation (P = .007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P = .02). These findings may help prioritize patients for whom the intervention is most effective.",Fourteen baseline patient characteristics were selected a priori as subgroup variables. ,7,3,RCT,0
27720142,"This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. The end point was BP < 140/90 mm Hg at 6-month follow-up. Fourteen baseline patient characteristics were selected a priori as subgroup variables. Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9 years, and mean BP was 149/86 mm Hg. The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P < .001). The effect of the intervention was significantly larger in patients who were younger (interaction P = .02), did not have diabetes (P = .005), had high baseline diastolic BP (P = .02), added salt less than daily in food preparation (P = .007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P = .02). These findings may help prioritize patients for whom the intervention is most effective.","Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9\xa0years, and mean BP was 149/86\xa0mm Hg. ",7,4,RCT,0
27720142,"This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. The end point was BP < 140/90 mm Hg at 6-month follow-up. Fourteen baseline patient characteristics were selected a priori as subgroup variables. Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9 years, and mean BP was 149/86 mm Hg. The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P < .001). The effect of the intervention was significantly larger in patients who were younger (interaction P = .02), did not have diabetes (P = .005), had high baseline diastolic BP (P = .02), added salt less than daily in food preparation (P = .007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P = .02). These findings may help prioritize patients for whom the intervention is most effective.",The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P\xa0<\xa0.001). ,7,5,RCT,3
27720142,"This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. The end point was BP < 140/90 mm Hg at 6-month follow-up. Fourteen baseline patient characteristics were selected a priori as subgroup variables. Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9 years, and mean BP was 149/86 mm Hg. The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P < .001). The effect of the intervention was significantly larger in patients who were younger (interaction P = .02), did not have diabetes (P = .005), had high baseline diastolic BP (P = .02), added salt less than daily in food preparation (P = .007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P = .02). These findings may help prioritize patients for whom the intervention is most effective.","The effect of the intervention was significantly larger in patients who were younger (interaction P\xa0=\xa0.02), did not have diabetes (P\xa0=\xa0.005), had high baseline diastolic BP (P\xa0=\xa0.02), added salt less than daily in food preparation (P\xa0=\xa0.007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P\xa0=\xa0.02). ",7,6,RCT,3
27720142,"This paper reports subgroup analysis of a successful cluster-randomized trial to identify attributes of hypertensive patients who benefited more or less from an intervention combining blood pressure (BP) telemonitoring and pharmacist management. The end point was BP < 140/90 mm Hg at 6-month follow-up. Fourteen baseline patient characteristics were selected a priori as subgroup variables. Among the 351 trial participants, 44% were female, 84% non-Hispanic white, mean age was 60.9 years, and mean BP was 149/86 mm Hg. The overall adjusted odds ratio for BP control in the intervention versus usual care group was 3.64 (P < .001). The effect of the intervention was significantly larger in patients who were younger (interaction P = .02), did not have diabetes (P = .005), had high baseline diastolic BP (P = .02), added salt less than daily in food preparation (P = .007), and took 0-2 (rather than 3-6) antihypertensive medication classes at baseline (P = .02). These findings may help prioritize patients for whom the intervention is most effective.",These findings may help prioritize patients for whom the intervention is most effective.,7,7,RCT,0
24215228,"We conducted a pilot randomized controlled trial to determine the feasibility and acceptability of a patient navigation intervention. Forty-seven smokers from one safety-net hospital were randomized to either a control group, in which they received a smoking cessation brochure and a list of smoking cessation resources, or a navigation group, in which they received the smoking cessation brochure, a list of smoking cessation resources, and patient navigation. Follow-up data were obtained for 33 participants. Nine (47.4%) of 19 of navigation group participants had engaged in smoking cessation treatment by 3 months versus 6 (42.9%) of 14 control group participants (chi-square p = ns). Patient navigation to promote engagement in smoking cessation treatment was feasible and acceptable to participants.",We conducted a pilot randomized controlled trial to determine the feasibility and acceptability of a patient navigation intervention. ,5,1,RCT,0
24215228,"We conducted a pilot randomized controlled trial to determine the feasibility and acceptability of a patient navigation intervention. Forty-seven smokers from one safety-net hospital were randomized to either a control group, in which they received a smoking cessation brochure and a list of smoking cessation resources, or a navigation group, in which they received the smoking cessation brochure, a list of smoking cessation resources, and patient navigation. Follow-up data were obtained for 33 participants. Nine (47.4%) of 19 of navigation group participants had engaged in smoking cessation treatment by 3 months versus 6 (42.9%) of 14 control group participants (chi-square p = ns). Patient navigation to promote engagement in smoking cessation treatment was feasible and acceptable to participants.","Forty-seven smokers from one safety-net hospital were randomized to either a control group, in which they received a smoking cessation brochure and a list of smoking cessation resources, or a navigation group, in which they received the smoking cessation brochure, a list of smoking cessation resources, and patient navigation. ",5,2,RCT,0
24215228,"We conducted a pilot randomized controlled trial to determine the feasibility and acceptability of a patient navigation intervention. Forty-seven smokers from one safety-net hospital were randomized to either a control group, in which they received a smoking cessation brochure and a list of smoking cessation resources, or a navigation group, in which they received the smoking cessation brochure, a list of smoking cessation resources, and patient navigation. Follow-up data were obtained for 33 participants. Nine (47.4%) of 19 of navigation group participants had engaged in smoking cessation treatment by 3 months versus 6 (42.9%) of 14 control group participants (chi-square p = ns). Patient navigation to promote engagement in smoking cessation treatment was feasible and acceptable to participants.",Follow-up data were obtained for 33 participants. ,5,3,RCT,0
24215228,"We conducted a pilot randomized controlled trial to determine the feasibility and acceptability of a patient navigation intervention. Forty-seven smokers from one safety-net hospital were randomized to either a control group, in which they received a smoking cessation brochure and a list of smoking cessation resources, or a navigation group, in which they received the smoking cessation brochure, a list of smoking cessation resources, and patient navigation. Follow-up data were obtained for 33 participants. Nine (47.4%) of 19 of navigation group participants had engaged in smoking cessation treatment by 3 months versus 6 (42.9%) of 14 control group participants (chi-square p = ns). Patient navigation to promote engagement in smoking cessation treatment was feasible and acceptable to participants.",Nine (47.4%) of 19 of navigation group participants had engaged in smoking cessation treatment by 3 months versus 6 (42.9%) of 14 control group participants (chi-square p = ns). ,5,4,RCT,0
24215228,"We conducted a pilot randomized controlled trial to determine the feasibility and acceptability of a patient navigation intervention. Forty-seven smokers from one safety-net hospital were randomized to either a control group, in which they received a smoking cessation brochure and a list of smoking cessation resources, or a navigation group, in which they received the smoking cessation brochure, a list of smoking cessation resources, and patient navigation. Follow-up data were obtained for 33 participants. Nine (47.4%) of 19 of navigation group participants had engaged in smoking cessation treatment by 3 months versus 6 (42.9%) of 14 control group participants (chi-square p = ns). Patient navigation to promote engagement in smoking cessation treatment was feasible and acceptable to participants.",Patient navigation to promote engagement in smoking cessation treatment was feasible and acceptable to participants.,5,5,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.","This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. ",11,1,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.",Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. ,11,2,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.","Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. ",11,3,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.",The patients prospectively underwent clinical and radiological studies. ,11,4,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.","In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). ",11,5,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.","The postoperative follow-ups were at 6 months, 2 years and 5.4 years. ",11,6,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.",It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. ,11,7,RCT,3
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.",Over a number of years a stable fusion can be achieved through either operation. ,11,8,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.",Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. ,11,9,RCT,0
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.",These advantages result in a clinically smoother course after the operation. ,11,10,RCT,1
16708233,"This study examines prospectively the randomised, long-term, clinical and radiological results of the treatment of spondylitis patients by ventro-dorsal or ventral spine fusion. Group 1 consisted of 12 patients who (after ventral removal of the focus of infection and autologous bone grafting) were treated by dorsal instrumentation. Group 2 consisted of ten patients who, after similar ventral removal and bone interposition, were stabilised by ventral instrumentation. The patients prospectively underwent clinical and radiological studies. In addition, they were asked to fill in self-assessment questionnaires such as the short-form (SF)-36 health survey, the Oswestry questionnaire, and the visual analog scales (VAS). The postoperative follow-ups were at 6 months, 2 years and 5.4 years. It proved possible to demonstrate clinically that patients with an isolated ventral spondylodesis feel significantly better and experience significantly less pain in the area of spinal fusion than patients with ventro-dorsal fusion 2 and 5.4 years after the operation. Over a number of years a stable fusion can be achieved through either operation. Ventral stabilisation yields more advantages than dorsal instrumentation in the long term. These advantages result in a clinically smoother course after the operation. If, in the individual case, ventral instrumentation is feasible, this method should be used.","If, in the individual case, ventral instrumentation is feasible, this method should be used.",11,11,RCT,0
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.","In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN).",8,1,RCT,0
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.","It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.",8,2,RCT,0
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.","Serum creatinine was determined immediately before, 24 and 48\u200ah after CM exposure. ",8,3,RCT,0
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.",As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).,8,4,RCT,0
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.",Both groups were comparable regarding baseline characteristics. ,8,5,RCT,0
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.",Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P\u200a=\u200a0.035). ,8,6,RCT,3
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.",Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.,8,7,RCT,3
27227933,"In this study, we investigated whether hydration with sodium bicarbonate is superior to hydration with saline in addition to theophylline (both groups) in the prophylaxis of contrast-induced nephropathy (CIN). It was a prospective, randomized, double-blinded study in a university hospital on 2 general intensive care units (63% of investigations) and normal wards.After approval of the local ethics committee and informed consent 152 patients with screening serum creatinine ≥1.1 mg/dL and/or at least 1 additional risk factor for CIN undergoing intravascular contrast media (CM) exposure were randomized to receive a total of 9 mL/kg bicarbonate 154 mmol/L (group B; n = 74) or saline 0.9% (group S; n = 78) hydration within 7 h in addition to intravenous application of 200 mg theophylline. Serum creatinine was determined immediately before, 24 and 48 h after CM exposure. As primary endpoint we investigated the incidence of CIN (increase of serum creatinine ≥0.5 mg/dL and/or ≥25% within 48 h of CM).Both groups were comparable regarding baseline characteristics. Incidence of CIN was significantly less frequent with bicarbonate compared to sodium hydration (1/74 [1.4%] vs 7/78 [9.0%]; P = 0.035). Time course of serum creatinine was more favorable in group B with decreases in serum creatinine after 24 h (-0.084 mg/dL [95% confidence interval: -0.035 to -0.133 mg/dL]; P = 0.008) and 48 h (-0.093 mg/dL (-0.025 to -0.161 mg/dL); P = 0.007) compared to baseline which were not observed in group S.In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.","In patients at increased risk of CIN receiving prophylactic theophylline, hydration with sodium bicarbonate reduces contrast-induced renal impairment compared to hydration with saline.",8,8,RCT,1
19838787,"The authors examine differential changes in values of tolerance among 150 participants discharged from inpatient treatment centers, and randomly assigned to either a self-help-based, communal living setting (i.e., Oxford House), or usual aftercare. Participants were interviewed every 6 months for a 24-month period. Hierarchical linear modeling (HLM) was used to examine the effect of condition (therapeutic communal living versus usual aftercare) on wave trajectories of tolerance (i.e., universality/diversity scores). Over time, residents of the communal living model demonstrated significantly greater values of tolerance than usual aftercare participants. Communal living participants who resided in the house for over 6 months showed the most substantial increases in tolerance. Results support the notion that communal living residents may develop more tolerant attitudes by striving toward superordinate community goals (objectives held by (a) the whole group and (b) which individual members could not achieve alone).","The authors examine differential changes in values of tolerance among 150 participants discharged from inpatient treatment centers, and randomly assigned to either a self-help-based, communal living setting (i.e., Oxford House), or usual aftercare. ",6,1,RCT,0
19838787,"The authors examine differential changes in values of tolerance among 150 participants discharged from inpatient treatment centers, and randomly assigned to either a self-help-based, communal living setting (i.e., Oxford House), or usual aftercare. Participants were interviewed every 6 months for a 24-month period. Hierarchical linear modeling (HLM) was used to examine the effect of condition (therapeutic communal living versus usual aftercare) on wave trajectories of tolerance (i.e., universality/diversity scores). Over time, residents of the communal living model demonstrated significantly greater values of tolerance than usual aftercare participants. Communal living participants who resided in the house for over 6 months showed the most substantial increases in tolerance. Results support the notion that communal living residents may develop more tolerant attitudes by striving toward superordinate community goals (objectives held by (a) the whole group and (b) which individual members could not achieve alone).",Participants were interviewed every 6 months for a 24-month period. ,6,2,RCT,0
19838787,"The authors examine differential changes in values of tolerance among 150 participants discharged from inpatient treatment centers, and randomly assigned to either a self-help-based, communal living setting (i.e., Oxford House), or usual aftercare. Participants were interviewed every 6 months for a 24-month period. Hierarchical linear modeling (HLM) was used to examine the effect of condition (therapeutic communal living versus usual aftercare) on wave trajectories of tolerance (i.e., universality/diversity scores). Over time, residents of the communal living model demonstrated significantly greater values of tolerance than usual aftercare participants. Communal living participants who resided in the house for over 6 months showed the most substantial increases in tolerance. Results support the notion that communal living residents may develop more tolerant attitudes by striving toward superordinate community goals (objectives held by (a) the whole group and (b) which individual members could not achieve alone).","Hierarchical linear modeling (HLM) was used to examine the effect of condition (therapeutic communal living versus usual aftercare) on wave trajectories of tolerance (i.e., universality/diversity scores). ",6,3,RCT,0
19838787,"The authors examine differential changes in values of tolerance among 150 participants discharged from inpatient treatment centers, and randomly assigned to either a self-help-based, communal living setting (i.e., Oxford House), or usual aftercare. Participants were interviewed every 6 months for a 24-month period. Hierarchical linear modeling (HLM) was used to examine the effect of condition (therapeutic communal living versus usual aftercare) on wave trajectories of tolerance (i.e., universality/diversity scores). Over time, residents of the communal living model demonstrated significantly greater values of tolerance than usual aftercare participants. Communal living participants who resided in the house for over 6 months showed the most substantial increases in tolerance. Results support the notion that communal living residents may develop more tolerant attitudes by striving toward superordinate community goals (objectives held by (a) the whole group and (b) which individual members could not achieve alone).","Over time, residents of the communal living model demonstrated significantly greater values of tolerance than usual aftercare participants. ",6,4,RCT,3
19838787,"The authors examine differential changes in values of tolerance among 150 participants discharged from inpatient treatment centers, and randomly assigned to either a self-help-based, communal living setting (i.e., Oxford House), or usual aftercare. Participants were interviewed every 6 months for a 24-month period. Hierarchical linear modeling (HLM) was used to examine the effect of condition (therapeutic communal living versus usual aftercare) on wave trajectories of tolerance (i.e., universality/diversity scores). Over time, residents of the communal living model demonstrated significantly greater values of tolerance than usual aftercare participants. Communal living participants who resided in the house for over 6 months showed the most substantial increases in tolerance. Results support the notion that communal living residents may develop more tolerant attitudes by striving toward superordinate community goals (objectives held by (a) the whole group and (b) which individual members could not achieve alone).",Communal living participants who resided in the house for over 6 months showed the most substantial increases in tolerance. ,6,5,RCT,3
19838787,"The authors examine differential changes in values of tolerance among 150 participants discharged from inpatient treatment centers, and randomly assigned to either a self-help-based, communal living setting (i.e., Oxford House), or usual aftercare. Participants were interviewed every 6 months for a 24-month period. Hierarchical linear modeling (HLM) was used to examine the effect of condition (therapeutic communal living versus usual aftercare) on wave trajectories of tolerance (i.e., universality/diversity scores). Over time, residents of the communal living model demonstrated significantly greater values of tolerance than usual aftercare participants. Communal living participants who resided in the house for over 6 months showed the most substantial increases in tolerance. Results support the notion that communal living residents may develop more tolerant attitudes by striving toward superordinate community goals (objectives held by (a) the whole group and (b) which individual members could not achieve alone).",Results support the notion that communal living residents may develop more tolerant attitudes by striving toward superordinate community goals (objectives held by (a) the whole group and (b) which individual members could not achieve alone).,6,6,RCT,0
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.",Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. ,10,1,RCT,3
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.","We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. ",10,2,RCT,0
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.",We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. ,10,3,RCT,0
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.","A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). ",10,4,RCT,0
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.","The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1\u2009mg/kg twice/day for 12 months, n = 19). ",10,5,RCT,0
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.","Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. ",10,6,RCT,0
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.",Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. ,10,7,RCT,0
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.","Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98\u2009±\u200953\u2009L/min vs 115\u2009±\u200956 with treatment, P = .03). ",10,8,RCT,3
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.","During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0\u2009±\u20098.7\u2009L/min vs 43.7\u2009±\u200913.6 with treatment, P = .02). ",10,9,RCT,3
27171844,"Pulmonary dysfunction is a significant contributor to morbidity and mortality in the pediatric burned population. We have previously reported that the administration of a synthetic testosterone derivative, oxandrolone, significantly reduced hypermetabolism, and significantly increased height percentile, bone mineral content, lean body mass, and strength in pediatric burned patients. We hypothesize that the administration of oxandrolone will improve pulmonary function in burned pediatric subjects. A subset of severely burned pediatric subjects from a prospective clinical trial (n = 222) were included in our study (n = 54, 7-18 years, ≥30% TBSA burn). The subjects were previously randomized to either the control arm (n = 35) or the oxandrolone arm (0.1 mg/kg twice/day for 12 months, n = 19). Maximum voluntary ventilation, the ratio between forced expiratory volume and forced vital capacity, and diffusion capacity were measured 6 months following burn injury, and results were compared between burned subjects with and without oxandrolone administration. Maximum expired ventilation (VEmax) was also measured in a subset of burned subjects. Subjects treated with oxandrolone had a significantly higher maximum voluntary ventilation (98 ± 53 L/min vs 115 ± 56 with treatment, P = .03). During maximal exercise, subjects treated with oxandrolone had a significantly higher VEmax compared with untreated subjects (32.0 ± 8.7 L/min vs 43.7 ± 13.6 with treatment, P = .02). The administration of oxandrolone was associated with improved lung function in pediatric burned patients.",The administration of oxandrolone was associated with improved lung function in pediatric burned patients.,10,10,RCT,3
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.",Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). ,10,1,RCT,0
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.","However, there have been no controlled studies comparing patient-reported outcomes between them. ",10,2,RCT,0
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.","In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. ",10,3,RCT,0
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.",Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. ,10,4,RCT,0
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.","We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. ",10,5,RCT,0
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.","Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. ",10,6,RCT,0
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.",Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. ,10,7,RCT,3
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.","All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. ",10,8,RCT,3
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.","Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. ",10,9,RCT,3
28464754,"Background and purpose - Criticism of the lateral approach (LA) for hip arthroplasty is mainly based on the risk of poor patient-reported outcomes compared to the posterior approach (PA). However, there have been no controlled studies comparing patient-reported outcomes between them. In this randomized controlled trial, we tested the hypothesis that patient-reported outcomes are better in patients who have undergone total hip arthroplasty (THA) with PA than in those who have undergone THA with LA, 12 months postoperatively. Patients and methods - 80 patients with hip osteoarthritis (mean age 61 years) were randomized to THA using PA or the modified direct LA. We recorded outcome measures preoperatively and 3, 6, and 12 months postoperatively using the Hip Disability and Osteoarthritis Outcome Score-Physical Function Short Form (HOOS-PS) as the primary outcome. Secondary outcomes were HOOS-Pain, HOOS-Quality-Of-Life, EQ-5D, UCLA Activity Score, and limping. Results - We found no statistically significant difference in the improvements in HOOS-PS between the treatment groups at 12-month follow-up. All secondary outcomes showed similar results except for limping, where PA patients improved significantly more than LA patients. Interpretation - Contrary to our hypothesis, patients treated with PA did not improve more than patients treated with LA regarding physical function, pain, physical activity, and quality of life 12 months postoperatively. However, limping was more pronounced in the LA patients.","However, limping was more pronounced in the LA patients.",10,10,RCT,3
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.",The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.,10,1,RCT,0
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.",Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n\u200a=\u200a25 each). ,10,2,RCT,0
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.",We set the fraction of inspired oxygen (FiO2) at 0.30. ,10,3,RCT,0
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.","The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14\u200acm H2O. ",10,4,RCT,0
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.","Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. ",10,5,RCT,0
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.","The postoperative pulmonary complications within the first 5 days were also observed.We chose 10\u200acm H2O and 8\u200acm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. ",10,6,RCT,0
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.",EIT-guided PEEP titration led to a more dorsal shift of ventilation. ,10,7,RCT,1
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.","The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330\u200a±\u200a10 vs 305.56\u200a±\u200a4\u200amm Hg; P\u200a=\u200a0.09). ",10,8,RCT,0
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.","The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P\u200a=\u200a0.75).",10,9,RCT,3
27057904,"The aim of the study is to utilize electrical impedance tomography (EIT) to guide positive end-expiratory pressure (PEEP) and to optimize oxygenation in patients undergoing laparoscopic abdominal surgery.Fifty patients were randomly assigned to the control (C) group and the EIT (E) group (n = 25 each). We set the fraction of inspired oxygen (FiO2) at 0.30. The PEEP was titrated and increased in a 2-cm H2O stepwise manner, from 6 to 14 cm H2O. Hemodynamic variables, respiratory mechanics, EIT images, analysis of blood gas, and regional cerebral oxygen saturation were recorded. The postoperative pulmonary complications within the first 5 days were also observed.We chose 10 cm H2O and 8 cm H2O as the ""ideal"" PEEP for the C and the E groups, respectively. EIT-guided PEEP titration led to a more dorsal shift of ventilation. The PaO2/FiO2 ratio in the E group was superior to that in the C group in the pneumoperitoneum period, though the difference was not significant (330 ± 10 vs 305.56 ± 4 mm Hg; P = 0.09). The C group patients experienced 8.7% postoperative pulmonary complications versus 5.3% among the E group patients (relative risk 1.27, 95% confidence interval 0.31-5.3, P = 0.75).Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.",Electrical impedance tomography represents a new promising technique that could enable anesthesiologists to assess regional ventilation of the lungs and optimize global oxygenation for patients undergoing laparoscopic abdominal surgery.,10,10,RCT,0
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.","A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. ",9,1,RCT,0
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.",We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ,9,2,RCT,0
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.",ProTECT III was conducted at 49 level I trauma centers in the United States. ,9,3,RCT,0
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.",Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4\u2009h of injury for a total of 4 days. ,9,4,RCT,0
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.","At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. ",9,5,RCT,0
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.","Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. ",9,6,RCT,3
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.","Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p\u2009=\u20090.53; delayed recall, p\u2009=\u20090.94), attention (Trails A speed, p\u2009=\u20090.81 and errors, p\u2009=\u20090.22; Digit Span Forward length, p\u2009=\u20090.66), executive functioning (Trails B speed, p\u2009=\u20090.97 and errors, p\u2009=\u20090.93; Digit Span Backward length, p\u2009=\u20090.60), language (timed phonemic fluency, p\u2009=\u20090.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p\u2009=\u20090.75 and peg drops, p\u2009=\u20090.59; nondominant hand time, p\u2009=\u20090.74 and peg drops, p\u2009=\u20090.61). ",9,7,RCT,3
26973025,"A Phase III, double-blind, placebo-controlled trial (ProTECT III) found that administration of progesterone did not reduce mortality or improve functional outcome as measured by the Glasgow Outcome Scale Extended (GOSE) in subjects with moderate to severe traumatic brain injury. We conducted a secondary analysis of neuropsychological outcomes to evaluate whether progesterone is associated with improved recovery of cognitive and motor functioning. ProTECT III was conducted at 49 level I trauma centers in the United States. Adults with moderate to severe TBI were randomized to receive intravenous progesterone or placebo within 4 h of injury for a total of 4 days. At 6 months, subjects underwent evaluation of memory, attention, executive functioning, language, and fine motor coordination/dexterity. Chi-square analysis revealed no significant difference in the proportion of subjects (263/280 progesterone, 283/295 placebo) with Galveston Orientation and Amnesia Test scores ≥75. Analyses of covariance did not reveal significant treatment effects for memory (Buschke immediate recall, p = 0.53; delayed recall, p = 0.94), attention (Trails A speed, p = 0.81 and errors, p = 0.22; Digit Span Forward length, p = 0.66), executive functioning (Trails B speed, p = 0.97 and errors, p = 0.93; Digit Span Backward length, p = 0.60), language (timed phonemic fluency, p = 0.05), and fine motor coordination/dexterity (Grooved Pegboard dominant hand time, p = 0.75 and peg drops, p = 0.59; nondominant hand time, p = 0.74 and peg drops, p = 0.61). Pearson Product Moment Correlations demonstrated significant (p < 0.001) associations between better neuropsychological performance and higher GOSE scores. Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.","Pearson Product Moment Correlations demonstrated significant (p\u2009<\u20090.001) associations between better neuropsychological performance and higher GOSE scores. ', ""Similar to the ProTECT III trial's results of the primary outcome, the secondary outcomes do not provide evidence of a neuroprotective effect of progesterone.""]",9,8,RCT,3
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.",It is important to identify the patients at highest risk of fractures. ,13,1,RCT,0
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. ",13,2,RCT,3
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. ",13,3,RCT,0
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. ",13,4,RCT,0
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). ",13,5,RCT,0
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.",GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. ,13,6,RCT,3
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.",Both GRS63 and GRS16 were associated with fractures. ,13,7,RCT,3
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","After BMD adjustment, the effect sizes for these associations were substantially reduced. ",13,8,RCT,3
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.",Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. ,13,9,RCT,0
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. ",13,10,RCT,3
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.",Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. ,13,11,RCT,0
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. ",13,12,RCT,3
25043339,"It is important to identify the patients at highest risk of fractures. A recent large-scale meta-analysis identified 63 autosomal single-nucleotide polymorphisms (SNPs) associated with bone mineral density (BMD), of which 16 were also associated with fracture risk. Based on these findings, two genetic risk scores (GRS63 and GRS16) were developed. Our aim was to determine the clinical usefulness of these GRSs for the prediction of BMD, BMD change, and fracture risk in elderly subjects. We studied two male (Osteoporotic Fractures in Men Study [MrOS] US, MrOS Sweden) and one female (Study of Osteoporotic Fractures [SOF]) large prospective cohorts of older subjects, looking at BMD, BMD change, and radiographically and/or medically confirmed incident fractures (8067 subjects, 2185 incident nonvertebral or vertebral fractures). GRS63 was associated with BMD (≅3% of the variation explained) but not with BMD change. Both GRS63 and GRS16 were associated with fractures. After BMD adjustment, the effect sizes for these associations were substantially reduced. Similar results were found using an unweighted GRS63 and an unweighted GRS16 compared with those found using the corresponding weighted risk scores. Only minor improvements in C-statistics (AUC) for fractures were found when the GRSs were added to a base model (age, weight, and height), and no significant improvements in C-statistics were found when they were added to a model further adjusted for BMD. Net reclassification improvements with the addition of the GRSs to a base model were modest and substantially attenuated in BMD-adjusted models. GRS63 is associated with BMD, but not BMD change, suggesting that the genetic determinants of BMD differ from those of BMD change. When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.","When BMD is known, the clinical utility of the two GRSs for fracture prediction is limited in elderly subjects.",13,13,RCT,0
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","[""The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. "", 'Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. ",11,1,RCT,1
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.",Recent studies have reported multiple GSV loci associated with risk of obesity. ,11,2,RCT,0
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. ",11,3,RCT,0
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. ",11,4,RCT,0
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","However, only one obesity association was replicated. ",11,5,RCT,0
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.",Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. ,11,6,RCT,1
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. ",11,7,RCT,3
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. ",11,8,RCT,0
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. ",11,9,RCT,0
23554873,"The limited ability of common variants to account for the genetic contribution to complex disease has prompted searches for rare variants of large effect, to partly explain the 'missing heritability'. Analyses of genome-wide genotyping data have identified genomic structural variants (GSVs) as a source of such rare causal variants. Recent studies have reported multiple GSV loci associated with risk of obesity. We attempted to replicate these associations by similar analysis of two familial-obesity case-control cohorts and a population cohort, and detected GSVs at 11 out of 18 loci, at frequencies similar to those previously reported. Based on their reported frequencies and effect sizes (OR≥25), we had sufficient statistical power to detect the large majority (80%) of genuine associations at these loci. However, only one obesity association was replicated. Deletion of a 220 kb region on chromosome 16p11.2 has a carrier population frequency of 2×10(-4) (95% confidence interval [9.6×10(-5)-3.1×10(-4)]); accounts overall for 0.5% [0.19%-0.82%] of severe childhood obesity cases (P = 3.8×10(-10); odds ratio = 25.0 [9.9-60.6]); and results in a mean body mass index (BMI) increase of 5.8 kg.m(-2) [1.8-10.3] in adults from the general population. We also attempted replication using BMI as a quantitative trait in our population cohort; associations with BMI at or near nominal significance were detected at two further loci near KIF2B and within FOXP2, but these did not survive correction for multiple testing. These findings emphasise several issues of importance when conducting rare GSV association, including the need for careful cohort selection and replication strategy, accurate GSV identification, and appropriate correction for multiple testing and/or control of false discovery rate. Moreover, they highlight the potential difficulty in replicating rare CNV associations across different populations. Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.","Nevertheless, we show that such studies are potentially valuable for the identification of variants making an appreciable contribution to complex disease.",11,10,RCT,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.",BACKGROUND Surgery remains the mainstay of gastric cancer treatment. ,14,1,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.","It is, however, associated with a relatively high risk of perioperative complications. ",14,2,Prospective Studies,3
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.","The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. ",14,3,Prospective Studies,1
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.",The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. ,14,4,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.",MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. ,14,5,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.","Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. ",14,6,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.","RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. ",14,7,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.","In 3 (5.7%) cases, conversion was required. ",14,8,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.",Median length of hospital stay was 5 days. ,14,9,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.",Compliance with ERAS protocol was 79.6±14.5%. ,14,10,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.","Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. ",14,11,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.","In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). ",14,12,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.",The 30-day readmission rate was 9.4%. ,14,13,Prospective Studies,0
28331173,"BACKGROUND Surgery remains the mainstay of gastric cancer treatment. It is, however, associated with a relatively high risk of perioperative complications. The use of laparoscopy and the Enhanced Recovery After Surgery (ERAS) protocol allows clinicians to limit surgically induced trauma, thus improving recovery and reducing the number of complications. The aim of the study is to present clinical outcomes of patients with gastric cancer undergoing laparoscopic gastrectomy combined with the ERAS protocol. MATERIAL AND METHODS Fifty-three (21 female/32 male) patients who underwent elective laparoscopic total gastrectomy due to cancer were prospectively analyzed. Demographic and surgical parameters were assessed, as well as the compliance with ERAS protocol elements, length of hospital stay, number of complications, and readmissions. RESULTS Mean operative time was 296.4±98.9 min, and mean blood loss was 293.3±213.8 mL. In 3 (5.7%) cases, conversion was required. Median length of hospital stay was 5 days. Compliance with ERAS protocol was 79.6±14.5%. Thirty (56.6%) patients tolerated an early oral diet well within 24 h postoperatively; in 48 (90.6%) patients, mobilization in the first 24 hours was successful. In 17 (32.1%) patients, postoperative complications occurred, with 7 of them (13.2%) being serious (Clavien-Dindo 3-5). The 30-day readmission rate was 9.4%. CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.",CONCLUSIONS The combination of laparoscopy and the ERAS protocol in patients with gastric cancer is feasible and allows achieving good clinical outcomes.,14,14,Prospective Studies,0
24478506,"The Xpert GBS real-time PCR assay was applied to gastric fluid samples from 143 newborns, and it detected group B streptococcus (GBS) within 1 h for 16 (11.2%) cases, while microscopic examination detected only 2 cases. The sensitivity and specificity of the Xpert GBS were 80% and 100%, respectively, with regard to 20 cases of GBS colonization or infection. Concordance of Xpert GBS results versus culture was 92.3%. This test detects in a timely manner newborns at risk for invasive GBS disease.","The Xpert GBS real-time PCR assay was applied to gastric fluid samples from 143 newborns, and it detected group B streptococcus (GBS) within 1 h for 16 (11.2%) cases, while microscopic examination detected only 2 cases. ",4,1,Prospective Studies,0
24478506,"The Xpert GBS real-time PCR assay was applied to gastric fluid samples from 143 newborns, and it detected group B streptococcus (GBS) within 1 h for 16 (11.2%) cases, while microscopic examination detected only 2 cases. The sensitivity and specificity of the Xpert GBS were 80% and 100%, respectively, with regard to 20 cases of GBS colonization or infection. Concordance of Xpert GBS results versus culture was 92.3%. This test detects in a timely manner newborns at risk for invasive GBS disease.","The sensitivity and specificity of the Xpert GBS were 80% and 100%, respectively, with regard to 20 cases of GBS colonization or infection. ",4,2,Prospective Studies,0
24478506,"The Xpert GBS real-time PCR assay was applied to gastric fluid samples from 143 newborns, and it detected group B streptococcus (GBS) within 1 h for 16 (11.2%) cases, while microscopic examination detected only 2 cases. The sensitivity and specificity of the Xpert GBS were 80% and 100%, respectively, with regard to 20 cases of GBS colonization or infection. Concordance of Xpert GBS results versus culture was 92.3%. This test detects in a timely manner newborns at risk for invasive GBS disease.",Concordance of Xpert GBS results versus culture was 92.3%. ,4,3,Prospective Studies,0
24478506,"The Xpert GBS real-time PCR assay was applied to gastric fluid samples from 143 newborns, and it detected group B streptococcus (GBS) within 1 h for 16 (11.2%) cases, while microscopic examination detected only 2 cases. The sensitivity and specificity of the Xpert GBS were 80% and 100%, respectively, with regard to 20 cases of GBS colonization or infection. Concordance of Xpert GBS results versus culture was 92.3%. This test detects in a timely manner newborns at risk for invasive GBS disease.",This test detects in a timely manner newborns at risk for invasive GBS disease.,4,4,Prospective Studies,0
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.","In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). ",8,1,Prospective Studies,0
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.",This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. ,8,2,Prospective Studies,0
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.","Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. ",8,3,Prospective Studies,0
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.",The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. ,8,4,Prospective Studies,0
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.",CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P\xa0<\xa00.001). ,8,5,Prospective Studies,3
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.","The liver metastatic CRC patients with high expression of CD133+ CD54+ (P\xa0<\xa00.001), CD133- CD54+ (P\xa0=\xa00.004), and CD133+ CD44+ CD54+ (P\xa0=\xa00.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. ",8,6,Prospective Studies,3
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.","Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR]\xa0=\xa03.056; 95% confidence interval [CI]\xa0=\xa01.354-6.897; P\xa0=\xa00.007), treatment strategy (HR\xa0=\xa00.212; 95% CI\xa0=\xa00.056-0.808; P\xa0=\xa00.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR\xa0=\xa06.459; 95% CI\xa0=\xa01.461-28.558; P\xa0=\xa00.014) as independent prognostic factors for CRC patients with liver metastasis. ",8,7,Prospective Studies,3
29105339,"In the previous study, we had showed the expression of CD133+ CD54+ CD44+ cellular subpopulation of circulating tumor cells (CTCs) was significantly associated with liver metastasis of colorectal cancer (CRC). This study aimed to explore whether this subpopulation of CTCs have a prognostic value in CRC patients. Flow cytometry was used to detect the expression of cellular subpopulations of CTCs with CD133, CD54, and CD44 in 152 CRC patients, between December 2013 and October 2014. The impact of clinicopathological factors and the expression of cellular subpopulations of CTCs on overall survival were then analyzed. CRC patients with liver metastases who underwent resection of the primary tumor accompanied by surgical treatment for metastasis had a better survival than other patients (P < 0.001). The liver metastatic CRC patients with high expression of CD133+ CD54+ (P < 0.001), CD133- CD54+ (P = 0.004), and CD133+ CD44+ CD54+ (P = 0.003) cellular subpopulations of CTCs had a worse survival than those patients with low expression. Multivariable survival analyses identified carcinoembryonic antigen levels (hazard ratio [HR] = 3.056; 95% confidence interval [CI] = 1.354-6.897; P = 0.007), treatment strategy (HR = 0.212; 95% CI = 0.056-0.808; P = 0.023), and CD133+ CD44+ CD54+ cellular subpopulation of CTCs (HR = 6.459; 95% CI = 1.461-28.558; P = 0.014) as independent prognostic factors for CRC patients with liver metastasis. CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.","CD133+ CD44+ CD54+ cellular subpopulation of CTCs has a prognostic value in CRC patients with liver metastasis, especially in the survival of CRC patients with liver metastasis who did not undergo surgical treatment for metastasis.",8,8,Prospective Studies,3
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. ,14,1,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. ,14,2,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.","This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. ",14,3,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). ,14,4,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.","In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. ",14,5,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. ,14,6,Prospective Studies,1
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. ,14,7,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. ,14,8,Prospective Studies,1
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.","Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. ",14,9,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",KIR genotyping did not delay transplantation. ,14,10,Prospective Studies,1
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. ,14,11,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.","Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. ",14,12,Prospective Studies,2
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.","Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. ",14,13,Prospective Studies,0
30149149,"We previously reported that acute myelogenous leukemia (AML) transplants using killer cell immunoglobulin-type receptor (KIR) B haplotype better or best (≥2 B activating gene loci ± Cen B/B) unrelated donors (URDs) yield less relapse and better survival. In this prospective trial we evaluated 535 AML searches from 14 participating centers with centralized donor KIR genotyping for donor selection. This represented 3% to 48% of all AML searches (median 20%) per center, totaling 3 to 172 patients (median 22) per center. Donor KIR genotype was reported at a median of 14 days after request (≤26 days for 76% of searches). In 535 searches, 2080 donors were requested for KIR genotyping (mean 4.3 per search); and a median of 1.8 (range, 0 to 4.5) per search were KIR typed. Choosing more donors for confirmatory HLA and KIR haplotype identification enriched the likelihood of finding KIR better or best donors. The search process identified a mean of 30% KIR better or best donors; the success ranged from 24% to 38% in the 11 centers enrolling ≥8 patients. More donors requested for KIR genotyping increased the likelihood of identifying KIR better or best haplotype donors. Of the 247 transplants, 9.3% used KIR best, 19% used KIR better, and 48% used KIR neutral donors while 24% used a non-KIR-tested donor. KIR genotyping did not delay transplantation. The time from search to transplant was identical for transplants using a KIR-genotyped versus a non-KIR-genotyped donor. Prospective evaluation can rapidly identify KIR favorable genotype donors, but choosing more donors per search would substantially increase the likelihood of having a KIR best or better donor available for transplantation. Transplant centers and donor registries must both commit extra effort to incorporate new characteristics (beyond HLA, age, and parity) into improved donor selection. Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.",Deliberate efforts to present additional genetic factors for donor selection will require novel procedures.,14,14,Prospective Studies,0
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. ",12,1,Prospective Studies,0
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","However, few studies have investigated the link between reproductive factors and DNA methylation in humans. ",12,2,Prospective Studies,0
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.",Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). ,12,3,Prospective Studies,0
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. ",12,4,Prospective Studies,0
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. ",12,5,Prospective Studies,0
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.",Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. ,12,6,Prospective Studies,3
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). ",12,7,Prospective Studies,3
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). ",12,8,Prospective Studies,3
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. ",12,9,Prospective Studies,3
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. ",12,10,Prospective Studies,3
24278132,"Reproductive factors have been linked to both breast cancer and DNA methylation, suggesting methylation as an important mechanism by which reproductive factors impact on disease risk. However, few studies have investigated the link between reproductive factors and DNA methylation in humans. Genome-wide methylation in peripheral blood lymphocytes of 376 healthy women from the prospective EPIC study was investigated using LUminometric Methylation Assay (LUMA). Also, methylation of 458877 CpG sites was additionally investigated in an independent group of 332 participants of the EPIC-Italy sub-cohort, using the Infinium HumanMethylation 450 BeadChip. Multivariate logistic regression and linear models were used to investigate the association between reproductive risk factors and genome wide and CpG-specific DNA methylation, respectively. Menarcheal age was inversely associated with global DNA methylation as measured with LUMA. For each yearly increase in age at menarche, the risk of having genome wide methylation below median level was increased by 32% (OR:1.32, 95%CI:1.14-1.53). When age at menarche was treated as a categorical variable, there was an inverse dose-response relationship with LUMA methylation levels (OR(12-14 vs. ≤11 yrs):1.78, 95%CI:1.01-3.17 and OR(≥15 vs. ≤11 yrs):4.59, 95%CI:2.04-10.33; P for trend<0.0001). However, average levels of global methylation as measured by the Illumina technology were not significantly associated with menarcheal age. In locus by locus comparative analyses, only one CpG site had significantly different methylation depending on the menarcheal age category examined, but this finding was not replicated by pyrosequencing in an independent data set. This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. Epigenetic changes may be modulated by menarcheal age, or the association may be a mirror of other important changes in early life that have a detectable effect on both methylation levels and menarcheal age.","This study suggests a link between age at menarche and genome wide DNA methylation, and the difference in results between the two arrays suggests that repetitive element methylation has a role in the association. ",12,11,Prospective Studies,3
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. ",17,1,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","In the present study, we investigated national epidemiology of ST410 E.\xa0coli isolates from Danish patients. ",17,2,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","Furthermore, E.\xa0coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. ",17,3,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E.\xa0coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. ",17,4,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E.\xa0coli ST131. ",17,5,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. ",17,6,Prospective Studies,2
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. ",17,7,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. ",17,8,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","From an epidemiological investigation of 49 E.\xa0coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. ",17,9,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","The accumulated multidrug resistance in E.\xa0coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E.\xa0coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. ",17,10,Prospective Studies,1
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. ",17,11,Prospective Studies,0
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. ",17,12,Prospective Studies,3
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.",The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. ,17,13,Prospective Studies,1
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","We show that clones of the E.\xa0coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. ",17,14,Prospective Studies,1
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. ",17,15,Prospective Studies,1
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.",Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. ,17,16,Prospective Studies,2
30021879,"Escherichia coli sequence type 410 (ST410) has been reported worldwide as an extraintestinal pathogen associated with resistance to fluoroquinolones, third-generation cephalosporins, and carbapenems. In the present study, we investigated national epidemiology of ST410 E. coli isolates from Danish patients. Furthermore, E. coli ST410 was investigated in a global context to provide further insight into the acquisition of the carbapenemase genes blaOXA-181 and blaNDM-5 of this successful lineage. From 127 whole-genome-sequenced isolates, we reconstructed an evolutionary framework of E. coli ST410 which portrays the antimicrobial-resistant clades B2/H24R, B3/H24Rx, and B4/H24RxC. The B2/H24R and B3/H24Rx clades emerged around 1987, concurrently with the C1/H30R and C2/H30Rx clades in E. coli ST131. B3/H24Rx appears to have evolved by the acquisition of the extended-spectrum β-lactamase (ESBL)-encoding gene blaCTX-M-15 and an IncFII plasmid, encoding IncFIA and IncFIB. Around 2003, the carbapenem-resistant clade B4/H24RxC emerged when ST410 acquired an IncX3 plasmid carrying a blaOXA-181 carbapenemase gene. Around 2014, the clade B4/H24RxC acquired a second carbapenemase gene, blaNDM-5, on a conserved IncFII plasmid. From an epidemiological investigation of 49 E. coli ST410 isolates from Danish patients, we identified five possible regional outbreaks, of which one outbreak involved nine patients with blaOXA-181- and blaNDM-5-carrying B4/H24RxC isolates. The accumulated multidrug resistance in E. coli ST410 over the past two decades, together with its proven potential of transmission between patients, poses a high risk in clinical settings, and thus, E. coli ST410 should be considered a lineage with emerging ""high-risk"" clones, which should be monitored closely in the future.IMPORTANCE Extraintestinal pathogenic Escherichia coli (ExPEC) is the main cause of urinary tract infections and septicemia. Significant attention has been given to the ExPEC sequence type ST131, which has been categorized as a ""high-risk"" clone. High-risk clones are globally distributed clones associated with various antimicrobial resistance determinants, ease of transmission, persistence in hosts, and effective transmission between hosts. The high-risk clones have enhanced pathogenicity and cause severe and/or recurrent infections. We show that clones of the E. coli ST410 lineage persist and/or cause recurrent infections in humans, including bloodstream infections. We found evidence of ST410 being a highly resistant globally distributed lineage, capable of patient-to-patient transmission causing hospital outbreaks. Our analysis suggests that the ST410 lineage should be classified with the potential to cause new high-risk clones. Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.","Thus, with the clonal expansion over the past decades and increased antimicrobial resistance to last-resort treatment options, ST410 needs to be monitored prospectively.",17,17,Prospective Studies,0
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.",Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. ,11,1,Prospective Studies,3
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.",We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. ,11,2,Prospective Studies,0
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.","This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. ",11,3,Prospective Studies,0
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.",HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. ,11,4,Prospective Studies,0
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.",Forward stepwise regression was performed to construct the final multivariate model. ,11,5,Prospective Studies,0
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.","The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. ",11,6,Prospective Studies,0
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.","In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. ",11,7,Prospective Studies,3
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.","After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. ",11,8,Prospective Studies,3
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.","In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. ",11,9,Prospective Studies,3
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.","The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. ",11,10,Prospective Studies,3
25209674,"Seminal plasma HIV-1 RNA level is an important determinant of the risk of HIV-1 sexual transmission. We investigated potential associations between seminal plasma cytokine levels and viral concentration in the seminal plasma of HIV-1-infected men. This was a prospective, observational study of paired blood and semen samples from 18 HIV-1 chronically infected men off antiretroviral therapy. HIV-1 RNA levels and cytokine levels in seminal plasma and blood plasma were measured and analyzed using simple linear regressions to screen for associations between cytokines and seminal plasma HIV-1 levels. Forward stepwise regression was performed to construct the final multivariate model. The median HIV-1 RNA concentrations were 4.42 log10 copies/ml (IQR 2.98, 4.70) and 2.96 log10 copies/ml (IQR 2, 4.18) in blood and seminal plasma, respectively. In stepwise multivariate linear regression analysis, blood HIV-1 RNA level (p<0.0001) was most strongly associated with seminal plasma HIV-1 RNA level. After controlling for blood HIV-1 RNA level, seminal plasma HIV-1 RNA level was positively associated with interferon (IFN)-γ (p=0.03) and interleukin (IL)-17 (p=0.03) and negatively associated with IL-5 (p=0.0007) in seminal plasma. In addition to blood HIV-1 RNA level, cytokine profiles in the male genital tract are associated with HIV-1 RNA levels in semen. The Th1 and Th17 cytokines IFN-γ and IL-17 are associated with increased seminal plasma HIV-1 RNA, while the Th2 cytokine IL-5 is associated with decreased seminal plasma HIV-1 RNA. These results support the importance of genital tract immunomodulation in HIV-1 transmission.",These results support the importance of genital tract immunomodulation in HIV-1 transmission.,11,11,Prospective Studies,1
26828641,"We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. Replication tests in more generalizable samples and of longer duration are indicated. Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. (PsycINFO Database Record","We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. ",7,1,Prospective Studies,0
26828641,"We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. Replication tests in more generalizable samples and of longer duration are indicated. Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. (PsycINFO Database Record","Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. ",7,2,Prospective Studies,0
26828641,"We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. Replication tests in more generalizable samples and of longer duration are indicated. Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. (PsycINFO Database Record","This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. ",7,3,Prospective Studies,0
26828641,"We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. Replication tests in more generalizable samples and of longer duration are indicated. Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. (PsycINFO Database Record","Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. ",7,4,Prospective Studies,3
26828641,"We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. Replication tests in more generalizable samples and of longer duration are indicated. Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. (PsycINFO Database Record",Replication tests in more generalizable samples and of longer duration are indicated. ,7,5,Prospective Studies,0
26828641,"We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. Replication tests in more generalizable samples and of longer duration are indicated. Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. (PsycINFO Database Record",Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. ,7,6,Prospective Studies,0
26828641,"We attempted to replicate and add to our prior study of attempts to stop or reduce cannabis use among daily cannabis users trying to change on their own, by observing a larger sample and adding further clinically relevant outcomes. Daily users (n = 193) who intended to stop or reduce sometime in the next 3 months called an Interactive Voice Response system each morning for 3 months to report on cannabis use, attempts to stop or reduce, withdrawal symptoms, and so forth, on the prior day. This study replicated our prior findings that (a) cannabis users trying to change make many, and often rapid, transitions among use as usual, reduction, and abstinence; (b) reduction attempts are more common than abstinence attempts; (c) quit and reduction attempts are short-lived and few participants achieve long-term abstinence; (d) alcohol and drug use are not greater on abstinence days; and (e) few users seek treatment. Novel findings included (f) a greater number of days of abstinence or intentional reduction predicted a greater decline in cannabis dependence, (g) most users do not prepare before their quit attempt, (h) coping outcomes during abstinence predict increased duration of abstinence, (i) tobacco use is less common on days of abstinence, and (j) withdrawal symptoms occur even with short quit attempts. Replication tests in more generalizable samples and of longer duration are indicated. Further natural history studies are likely to provide information to help improve the content of psychological treatments for cannabis use. (PsycINFO Database Record",(PsycINFO Database Record,7,7,Prospective Studies,0
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. ,9,1,Prospective Studies,1
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. ,9,2,Prospective Studies,0
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. ,9,3,Prospective Studies,0
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",The tolerance was assessed by recording all side effects related with immunotherapy. ,9,4,Prospective Studies,0
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. ,9,5,Prospective Studies,0
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. ,9,6,Prospective Studies,3
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.","After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. ",9,7,Prospective Studies,0
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. ,9,8,Prospective Studies,3
17302898,"The physicochemical modification of allergen vaccines provides a chance for administering higher doses in a shorter period of time. We sought to assess the safety and immunological changes of using a biologically standardized and modified Parietaria judaica pollen extract in accelerated schedules. Two accelerated schedules were tested in 45 P. judaica-allergic patients: 20 patients reached the maximum dose after two visits using two different concentrations and 25 patients reached the maximum dose after only one visit with two injections of the maximum concentration vial. The tolerance was assessed by recording all side effects related with immunotherapy. Specific antibody levels against native extract and rPar j 2 allergen were evaluated at the beginning and the end of the study. Allergenic potency determined by enzyme allergosorbent test (EAST) inhibition and skin prick test showed that modified P. judaica pollen had a 99.9% less allergenicity than native extract. After 650 doses administered, two clinically irrelevant local reactions (diameter<0 x 5 cm) and no systemic reactions were registered. Significant increases in allergen-specific IgG4 and IgG against P. judaica extract and rPar j 2 and significant decrease of specific IgE against Par j 2 were observed. The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.","The modified extract of P. judaica is safe to treat sensitive patients, even at accelerated regimens, and induces significant immunological changes.",9,9,Prospective Studies,1
23042942,"Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. No other associations with HLA genotype were identified. There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.",Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. ,9,1,Prospective Studies,1
23042942,"Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. No other associations with HLA genotype were identified. There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.","Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. ",9,2,Prospective Studies,0
23042942,"Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. No other associations with HLA genotype were identified. There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.","In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. ",9,3,Prospective Studies,0
23042942,"Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. No other associations with HLA genotype were identified. There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.","SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. ', ""Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. "", 'Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. ",9,4,Prospective Studies,0
23042942,"Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. No other associations with HLA genotype were identified. There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.","In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. ",9,5,Prospective Studies,3
23042942,"Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. No other associations with HLA genotype were identified. There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.",No other associations with HLA genotype were identified. ,9,6,Prospective Studies,0
23042942,"Patients with irritable bowel syndrome (IBS) with diarrhea (IBS-D) carrying human leukocyte antigen (HLA)-DQ2/8 genotypes benefit from gluten withdrawal. Our objective was to compare gastrointestinal barrier function, mucosal inflammation, and transit in nonceliac IBS-D patients and assess association with HLA-DQ2/8 status. In 45 IBS-D patients who were naive to prior exclusion of dietary gluten, we measured small bowel (SB) and colonic mucosal permeability by cumulative urinary lactulose and mannitol excretion (0-2 h for SB and 8-24 h for colon), inflammation on duodenal and rectosigmoid mucosal biopsies (obtained in 28 of 45 patients), tight junction (TJ) protein mRNA and protein expression in SB and rectosigmoid mucosa, and gastrointestinal and colonic transit by validated scintigraphy. SB mucosal biopsies were stained with hematoxylin-eosin to assess villi and intraepithelial lymphocytes, and immunohistochemistry was used to assess CD3, CD8, tryptase, and zonula occludens 1 (ZO-1); colonic biopsy intraepithelial lymphocytes were quantitated. Associations of HLA-DQ were assessed using Wilcoxon's rank-sum test. Relative to healthy control data, we observed a significant increase in SB permeability (P < 0.001), a borderline increase in colonic permeability (P = 0.10), and a decrease in TJ mRNA expression in rectosigmoid mucosa in IBS-D. In HLA-DQ2/8-positive patients, ZO-1 protein expression in the rectosigmoid mucosa was reduced compared with that in HLA-DQ2/8-negative patients and colonic transit was slower than in HLA-DQ2/8-negative patients. No other associations with HLA genotype were identified. There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.","There is abnormal barrier function (increased SB permeability and reduced mRNA expression of TJ proteins) in IBS-D relative to health that may be, in part, related to immunogenotype, given reduced ZO-1 protein expression in rectosigmoid mucosa in HLA-DQ2/8-positive relative to HLA-DQ2/8-negative patients.",9,7,Prospective Studies,3
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.","The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. ",8,1,Prospective Studies,0
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.","Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. ",8,2,Prospective Studies,0
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.",Women using exogenous hormones at the time of blood collection were excluded from the study. ,8,3,Prospective Studies,0
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.",Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). ,8,4,Prospective Studies,3
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.","The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). ",8,5,Prospective Studies,3
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.",Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. ,8,6,Prospective Studies,1
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.","Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. ",8,7,Prospective Studies,3
9252211,"The associations between serum concentrations of oestradiol, testosterone and sex hormone-binding globulin (SHBG) and risk of breast cancer in post-menopausal women were investigated in a prospective study on the island of Guernsey. Sixty-one women who developed breast cancer an average of 7.8 years after blood collection were matched for age, year of blood collection and number of years post-menopausal with 179 control subjects. Women using exogenous hormones at the time of blood collection were excluded from the study. Women who subsequently developed breast cancer had a 29% higher geometric mean oestradiol concentration than control women (P = 0.004). The odds ratio for breast cancer in the top third compared with the lowest third of the oestradiol concentration distribution was 5.03 (95% confidence interval 2.02-12.49, P for trend < 0.001). Adjusting for testosterone and SHBG concentrations did not substantially alter the odds ratio for oestradiol. Although testosterone and SHBG concentrations were associated with breast cancer risk, the concentrations of these hormones were correlated with those of oestradiol; the associations were not statistically significant after adjusting for oestradiol concentration. These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.",These data provide evidence that serum oestradiol concentrations in post-menopausal women may have a substantial effect on breast cancer risk.,8,8,Prospective Studies,2
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.","[""Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. "", 'Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. ",10,1,Prospective Studies,0
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.","Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). ",10,2,Prospective Studies,0
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.","General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. ",10,3,Prospective Studies,0
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.","Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. ",10,4,Prospective Studies,3
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.",The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. ,10,5,Prospective Studies,3
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.",For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. ,10,6,Prospective Studies,3
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.",An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. ,10,7,Prospective Studies,3
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.","Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. ",10,8,Prospective Studies,2
27781389,"Alzheimer's disease (AD) is the most common form of dementia and is characterized by impairment in memory, behavioral changes, and gradual loss of autonomy. Since there is a long latent period prior to diagnosis, the aim of this study was to determine whether twenty single nucleotide polymorphisms identified in genome-wide association analyses of AD are associated with cognitive change in 8,320 white and 2,039 African-American middle-aged adults enrolled in the prospective Atherosclerosis Risk in Communities (ARIC) study. Cognition was evaluated using the Delayed Word Recall Test (DWRT; verbal memory), Digit Symbol Substitution Test (DSST; processing speed), and Word Fluency Test (WFT; executive function). General linear models were used to assess mean differences in 6-year change in test scores among individuals categorized by genotype after adjusting for age, gender, and years of education. Addition of the minor allele for rs670139 (MS4A4E), rs9331896 (CLU), and rs12155159 (NME8) was nominally associated with change on the DWRT, DSST, and WFT, respectively, in whites. The ZCWPW1 (rs1476679) and CDS33 (rs3865444) variants were nominally associated with change on the DWRT and WFT in African-Americans. For rs670139 and rs9331896 the association was only significant in individuals bearing at least one APOE ϵ4 allele in stratified analyses. An unweighted genetic risk score aggregating the risk alleles for 15 polymorphisms was not associated with change in cognitive function. Although the AD-associated genetic variants appear to have small effects on early cognitive change, replication will be required to establish whether there is a discernible influence on cognitive status in midlife. © 2016 Wiley Periodicals, Inc.","© 2016 Wiley Periodicals, Inc.",10,9,Prospective Studies,0
9549574,"One hundred and twenty-nine consecutive cementless primary total hip arthroplasties with Zweymüller-Alloclassic grit-blasted titanium components were evaluated prospectively at an average follow-up of 5.9 years. The clinical results were graded as excellent or good in 116 hips (90%), fair in 12, and poor in one due to a fracture of the femoral shaft at operation leading to early stem loosening and delayed ring loosening. Failure was defined as definite aseptic loosening, and the final survivorship at 8 years was 99.3% for the SL-stem, and 99.1% for the CSF-threaded cup. These early results compare favourably with those of total hip arthroplasties using new cementing techniques.",One hundred and twenty-nine consecutive cementless primary total hip arthroplasties with Zweymüller-Alloclassic grit-blasted titanium components were evaluated prospectively at an average follow-up of 5.9 years. ,4,1,Prospective Studies,0
9549574,"One hundred and twenty-nine consecutive cementless primary total hip arthroplasties with Zweymüller-Alloclassic grit-blasted titanium components were evaluated prospectively at an average follow-up of 5.9 years. The clinical results were graded as excellent or good in 116 hips (90%), fair in 12, and poor in one due to a fracture of the femoral shaft at operation leading to early stem loosening and delayed ring loosening. Failure was defined as definite aseptic loosening, and the final survivorship at 8 years was 99.3% for the SL-stem, and 99.1% for the CSF-threaded cup. These early results compare favourably with those of total hip arthroplasties using new cementing techniques.","The clinical results were graded as excellent or good in 116 hips (90%), fair in 12, and poor in one due to a fracture of the femoral shaft at operation leading to early stem loosening and delayed ring loosening. ",4,2,Prospective Studies,0
9549574,"One hundred and twenty-nine consecutive cementless primary total hip arthroplasties with Zweymüller-Alloclassic grit-blasted titanium components were evaluated prospectively at an average follow-up of 5.9 years. The clinical results were graded as excellent or good in 116 hips (90%), fair in 12, and poor in one due to a fracture of the femoral shaft at operation leading to early stem loosening and delayed ring loosening. Failure was defined as definite aseptic loosening, and the final survivorship at 8 years was 99.3% for the SL-stem, and 99.1% for the CSF-threaded cup. These early results compare favourably with those of total hip arthroplasties using new cementing techniques.","Failure was defined as definite aseptic loosening, and the final survivorship at 8 years was 99.3% for the SL-stem, and 99.1% for the CSF-threaded cup. ",4,3,Prospective Studies,0
9549574,"One hundred and twenty-nine consecutive cementless primary total hip arthroplasties with Zweymüller-Alloclassic grit-blasted titanium components were evaluated prospectively at an average follow-up of 5.9 years. The clinical results were graded as excellent or good in 116 hips (90%), fair in 12, and poor in one due to a fracture of the femoral shaft at operation leading to early stem loosening and delayed ring loosening. Failure was defined as definite aseptic loosening, and the final survivorship at 8 years was 99.3% for the SL-stem, and 99.1% for the CSF-threaded cup. These early results compare favourably with those of total hip arthroplasties using new cementing techniques.",These early results compare favourably with those of total hip arthroplasties using new cementing techniques.,4,4,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.",Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. ,13,1,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.","Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. ",13,2,Prospective Studies,3
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.",Failure to name objects emerged as the most consistent and severe deficit. ,13,3,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.","Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. ",13,4,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.","Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. ",13,5,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.",The remaining set of errors had associative components. ,13,6,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.",These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. ,13,7,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.","In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. ",13,8,Prospective Studies,3
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.","Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. ",13,9,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.",This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. ,13,10,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.","These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. ",13,11,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.","A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. ",13,12,Prospective Studies,0
23361063,"Eleven of 69 prospectively enrolled primary progressive aphasics were selected for this study because of peak atrophy sites located predominantly or exclusively within the anterior left temporal lobe. Cortical volumes in these areas were reduced to less than half of control values, whereas average volume elsewhere in the left hemisphere deviated from control values by only 8%. Failure to name objects emerged as the most consistent and severe deficit. Naming errors were attributed to pure retrieval failure if the object could not be named even when the denoting word was understood, the object recognized and the two accurately matched. Surprisingly many of the naming errors reflected pure retrieval failures, without discernible semantic or associative component. The remaining set of errors had associative components. These errors reflected the inability to define the word denoting the object more often than the inability to define the nature of the pictured object. In a separate task where the same object had to be linked to verbal or non-verbal associations, performance was abnormal only in the verbal format. Excessive taxonomic interference was observed for picture-word, but not picture-picture, matching tasks. This excessive interference reflected a blurring of intra- rather than inter-category distinctions as if the acuity of word-object associations had been diminished so that correspondences were easier to recognize at generic than specific levels. These dissociations between verbal and non-verbal markers of object knowledge indicate that the reduced neural mass at peak atrophy sites of the left temporal tip, accounting for half or more of the presumed premorbid volume, was unlikely to have contained domain-independent semantic representations of the type that would be expected in a strictly amodal hub. A more likely arrangement entails two highly interactive routes--a strongly left lateralized temporosylvian language network for verbal concepts, and a presumably more bilateral or right-sided inferotemporal/fusiform object recognition network, which remained relatively spared because peak atrophy sites were concentrated on the left. The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.",The current results also suggest that the left anterior temporal neocortex should be inserted into the language network where it is likely to play a major role in selecting verbal labels for objects and mediating the progression of word comprehension from generic to specific levels of precision.,13,13,Prospective Studies,2
28827610,"We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. Four trajectories were identified: 73.2%(n = 857) of the children showed a normal BMIz trajectory, 13.2%(n = 155) a stable low-BMIz trajectory, 8.6%(n = 100) a stable high-BMIz trajectory and 5.0%(n = 58) a rapid BMIz gain after 3 months trajectory. Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.","We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. ",7,1,Prospective Studies,0
28827610,"We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. Four trajectories were identified: 73.2%(n = 857) of the children showed a normal BMIz trajectory, 13.2%(n = 155) a stable low-BMIz trajectory, 8.6%(n = 100) a stable high-BMIz trajectory and 5.0%(n = 58) a rapid BMIz gain after 3 months trajectory. Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.",Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. ,7,2,Prospective Studies,0
28827610,"We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. Four trajectories were identified: 73.2%(n = 857) of the children showed a normal BMIz trajectory, 13.2%(n = 155) a stable low-BMIz trajectory, 8.6%(n = 100) a stable high-BMIz trajectory and 5.0%(n = 58) a rapid BMIz gain after 3 months trajectory. Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.","Four trajectories were identified: 73.2%(n\u2009=\u2009857) of the children showed a normal BMIz trajectory, 13.2%(n\u2009=\u2009155) a stable low-BMIz trajectory, 8.6%(n\u2009=\u2009100) a stable high-BMIz trajectory and 5.0%(n\u2009=\u200958) a rapid BMIz gain after 3 months trajectory. ",7,3,Prospective Studies,0
28827610,"We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. Four trajectories were identified: 73.2%(n = 857) of the children showed a normal BMIz trajectory, 13.2%(n = 155) a stable low-BMIz trajectory, 8.6%(n = 100) a stable high-BMIz trajectory and 5.0%(n = 58) a rapid BMIz gain after 3 months trajectory. Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.","Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. ",7,4,Prospective Studies,3
28827610,"We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. Four trajectories were identified: 73.2%(n = 857) of the children showed a normal BMIz trajectory, 13.2%(n = 155) a stable low-BMIz trajectory, 8.6%(n = 100) a stable high-BMIz trajectory and 5.0%(n = 58) a rapid BMIz gain after 3 months trajectory. Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.","At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. ",7,5,Prospective Studies,3
28827610,"We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. Four trajectories were identified: 73.2%(n = 857) of the children showed a normal BMIz trajectory, 13.2%(n = 155) a stable low-BMIz trajectory, 8.6%(n = 100) a stable high-BMIz trajectory and 5.0%(n = 58) a rapid BMIz gain after 3 months trajectory. Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.",BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. ,7,6,Prospective Studies,3
28827610,"We investigated body mass index (BMI) trajectories in the first 2 years of life in 1170 children from an Asian mother-offspring cohort in Singapore, and examined their predictors and associations with childhood cardio-metabolic risk measures at 5 years. Latent class growth mixture modelling analyses were performed to identify distinct BMI z-score (BMIz) trajectories. Four trajectories were identified: 73.2%(n = 857) of the children showed a normal BMIz trajectory, 13.2%(n = 155) a stable low-BMIz trajectory, 8.6%(n = 100) a stable high-BMIz trajectory and 5.0%(n = 58) a rapid BMIz gain after 3 months trajectory. Predictors of the stable high-BMIz and rapid BMIz gain trajectories were pre-pregnancy BMI, gestational weight gain, Malay and Indian ethnicity, while predictors of stable low-BMIz trajectory were preterm delivery and Indian ethnicity. At 5 years, children with stable high-BMIz or rapid BMIz gain trajectories had increased waist-to-height ratios [B(95%CI) 0.02(0.01,0.03) and 0.03(0.02,0.04)], sum of skinfolds [0.42(0.19,0.65) and 0.70(0.36,1.03)SD units], fat-mass index [0.97(0.32,1.63)SD units] and risk of obesity [relative risk 3.22(1.73,6.05) and 2.56 (1.19,5.53)], but not higher blood pressure. BMIz trajectories were more predictive of adiposity at 5 years than was BMIz at 2 years. Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.",Our findings on BMIz trajectories in the first 2 years suggest important ethnic-specific differences and impacts on later metabolic outcomes.,7,7,Prospective Studies,1
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.",Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. ,8,1,Prospective Studies,0
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.","Expanding deep trees, however, is computationally taxing. ",8,2,Prospective Studies,0
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.","Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. ",8,3,Prospective Studies,0
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.","Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"".",8,4,Prospective Studies,0
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.","Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. ",8,5,Prospective Studies,0
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.",Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. ,8,6,Prospective Studies,0
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.","We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. ",8,7,Prospective Studies,1
30861001,"Evaluating the future consequences of actions is achievable by simulating a mental search tree into the future. Expanding deep trees, however, is computationally taxing. Therefore, machines and humans use a plan-until-habit scheme that simulates the environment up to a limited depth and then exploits habitual values as proxies for consequences that may arise in the future. Two outstanding questions in this scheme are ""in which directions the search tree should be expanded?"", and ""when should the expansion stop?"". Here we propose a principled solution to these questions based on a speed/accuracy tradeoff: deeper expansion in the appropriate directions leads to more accurate planning, but at the cost of slower decision-making. Our simulation results show how this algorithm expands the search tree effectively and efficiently in a grid-world environment. We further show that our algorithm can explain several behavioral patterns in animals and humans, namely the effect of time-pressure on the depth of planning, the effect of reward magnitudes on the direction of planning, and the gradual shift from goal-directed to habitual behavior over the course of training. The algorithm also provides several predictions testable in animal/human experiments.",The algorithm also provides several predictions testable in animal/human experiments.,8,8,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.",Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. ,13,1,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.",Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. ,13,2,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.",Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. ,13,3,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.","Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. ",13,4,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.",These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). ,13,5,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.","Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. ",13,6,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.","Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). ",13,7,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.","TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. ",13,8,Prospective Studies,3
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.",These correlations were largely independent of several demographic parameters as well as family environment. ,13,9,Prospective Studies,3
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.","Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. ",13,10,Prospective Studies,3
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.",Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. ,13,11,Prospective Studies,3
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.","© RSNA, 2019 Online supplemental material is available for this article. ",13,12,Prospective Studies,0
31161971,"Background It is well known that white matter injuries observed at birth are associated with adverse neurodevelopmental outcomes later in life. Whether white matter developmental variations in healthy newborns are also associated with changes in later neurodevelopment remains to be established. Purpose To evaluate whether developmental variations of white matter microstructures identified by MRI correlate with neurodevelopmental outcomes in healthy full-term infants. Materials and Methods In this prospective study, pregnant women were recruited and their healthy full-term newborns underwent a brain MRI including diffusion tensor imaging at approximately 2 weeks of age. These infants were tested at approximately 2 years of age with the Bayley Scales of Infant Development (BSID). Voxel-wise correlation analyses of fractional anisotropy (FA), measured with diffusion tensor MRI, and neurodevelopmental test scores, measured by using BSID, were performed by using tract-based spatial statistics (TBSS), followed by region-of-interest (ROI) analyses of correlations between mean FA in selected white matter ROIs and each BSID subscale score. Results Thirty-eight full-term infants (20 boys, 18 girls) underwent MRI examination at 2 weeks of age (14.3 days ± 1.6) and BSID measurement at 2 years of age (732 days ± 6). TBSS analyses showed widespread clusters in major white matter tracts, with positive correlations (P ≤ .05, corrected for the voxel-wise multiple comparisons) between FA values and multiple BSID subscale scores. These correlations were largely independent of several demographic parameters as well as family environment. Gestational age at birth appeared to be a confounding factor as TBSS-observed correlations weakened when it was included as a covariate; however, after controlling for gestational age at birth, ROI analyses still showed positive correlations (P ≤ .05, R = 0.35 to 0.48) between mean FA in many white matter ROIs and BSID cognitive, language, and motor scores. Conclusion There were significant associations between white matter microstructure developmental variations in healthy full-term newborns and their neurodevelopmental outcomes. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Hu and McAllister in this issue.",See also the editorial by Hu and McAllister in this issue.,13,13,Prospective Studies,0
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.","Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. ",9,1,Prospective Studies,0
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.",This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. ,9,2,Prospective Studies,0
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.",We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. ,9,3,Prospective Studies,0
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.",Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). ,9,4,Prospective Studies,0
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.","Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). ",9,5,Prospective Studies,3
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.",Ankle-brachial indices were abnormal in 78% of patients. ,9,6,Prospective Studies,0
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.",Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. ,9,7,Prospective Studies,2
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.","The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. ",9,8,Prospective Studies,0
22083160,"Hutchinson-Gilford progeria syndrome is a rare, segmental premature aging syndrome of accelerated atherosclerosis and early death from myocardial infarction or stroke. This study sought to establish comprehensive characterization of the fatal vasculopathy in Hutchinson-Gilford progeria syndrome and its relevance to normal aging. We performed cardiovascular assessments at a single clinical site on the largest prospectively studied cohort to date. Carotid-femoral pulse wave velocity was dramatically elevated (mean: 13.00±3.83 m/s). Carotid duplex ultrasound echobrightness, assessed in predefined tissue sites as a measure of arterial wall density, was significantly greater than age- and sex-matched controls in the intima-media (P<0.02), near adventitia (P<0.003), and deep adventitia (P<0.01), as was internal carotid artery mean flow velocity (P<0.0001). Ankle-brachial indices were abnormal in 78% of patients. Effective disease treatments may be heralded by normalizing trends of these noninvasive cardiovascular measures. The data demonstrate that, along with peripheral vascular occlusive disease, accelerated vascular stiffening is an early and pervasive mechanism of vascular disease in Hutchinson-Gilford progeria syndrome. There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.","There is considerable overlap with cardiovascular changes of normal aging, which reinforces the view that defining mechanisms of cardiovascular disease in Hutchinson-Gilford progeria syndrome provides a unique opportunity to isolate a subset of factors influencing cardiovascular disease in the general aging population.",9,9,Prospective Studies,0
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.",Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. ,9,1,Prospective Studies,2
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.","Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. ",9,2,Prospective Studies,2
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.","The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. ",9,3,Prospective Studies,0
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.","Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). ",9,4,Prospective Studies,0
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.","This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. ",9,5,Prospective Studies,0
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.","The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. ",9,7,Prospective Studies,3
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.",These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. ,9,8,Prospective Studies,3
29632170,"Immune function is an energetically costly physiological activity that potentially diverts calories away from less immediately essential life tasks. Among developing organisms, the allocation of energy toward immune function may lead to tradeoffs with physical growth, particularly in high-pathogen, low-resource environments. The present study tests this hypothesis across diverse timeframes, branches of immunity, and conditions of energy availability among humans. Using a prospective mixed-longitudinal design, we collected anthropometric and blood immune biomarker data from 261 Amazonian forager-horticulturalist Shuar children (age 4-11 y old). This strategy provided baseline measures of participant stature, s.c. body fat, and humoral and cell-mediated immune activity as well as subsample longitudinal measures of linear growth (1 wk, 3 mo, 20 mo) and acute inflammation. Multilevel analyses demonstrate consistent negative effects of immune function on growth, with children experiencing up to 49% growth reduction during periods of mildly elevated immune activity. The direct energetic nature of these relationships is indicated by (i) the manifestation of biomarker-specific negative immune effects only when examining growth over timeframes capturing active competition for energetic resources, (ii) the exaggerated impact of particularly costly inflammation on growth, and (iii) the ability of children with greater levels of body fat (i.e., energy reserves) to completely avoid the growth-inhibiting effects of acute inflammation. These findings provide evidence for immunologically and temporally diverse body fat-dependent tradeoffs between immune function and growth during childhood. We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.","We discuss the implications of this work for understanding human developmental energetics and the biological mechanisms regulating variation in human ontogeny, life history, and health.",9,9,Prospective Studies,0
30458915,"Since 7 June 2018, an enterovirus D-68 (EV-D68) season (the third since 2015) is ongoing in Wales, with 114 confirmed cases thus far. Median age of the 220 cases since 2015 is 2.5 years (2.5 years in intensive care cases), 94% were hospitalised, 17% (n = 38) in intensive care. All had respiratory symptoms; bronchiolitis symptoms were reported in 60 cases, severe respiratory symptoms in 23 and acute flaccid myelitis in two cases.","Since 7 June 2018, an enterovirus D-68 (EV-D68) season (the third since 2015) is ongoing in Wales, with 114 confirmed cases thus far. ",3,1,Prospective Studies,0
30458915,"Since 7 June 2018, an enterovirus D-68 (EV-D68) season (the third since 2015) is ongoing in Wales, with 114 confirmed cases thus far. Median age of the 220 cases since 2015 is 2.5 years (2.5 years in intensive care cases), 94% were hospitalised, 17% (n = 38) in intensive care. All had respiratory symptoms; bronchiolitis symptoms were reported in 60 cases, severe respiratory symptoms in 23 and acute flaccid myelitis in two cases.","Median age of the 220 cases since 2015 is 2.5 years (2.5 years in intensive care cases), 94% were hospitalised, 17% (n\xa0=\xa038) in intensive care. ",3,2,Prospective Studies,0
30458915,"Since 7 June 2018, an enterovirus D-68 (EV-D68) season (the third since 2015) is ongoing in Wales, with 114 confirmed cases thus far. Median age of the 220 cases since 2015 is 2.5 years (2.5 years in intensive care cases), 94% were hospitalised, 17% (n = 38) in intensive care. All had respiratory symptoms; bronchiolitis symptoms were reported in 60 cases, severe respiratory symptoms in 23 and acute flaccid myelitis in two cases.","All had respiratory symptoms; bronchiolitis symptoms were reported in 60 cases, severe respiratory symptoms in 23 and acute flaccid myelitis in two cases.",3,3,Prospective Studies,0
26538132,"Recent genome-wide associated studies (GWASs) have revealed several common loci associated with the risk of hepatitis B virus (HBV)- or hepatitis C virus (HCV)-related hepatocellular carcinoma (HCC). We selected 15 single nucleotide polymorphisms (SNPs) identified through GWASs on HBV- or HCV-related HCC, and genotyped them in two independent Chinese cohorts of chronic HBV carriers, including 712 LC cases and 2601 controls. The association of each SNP with the risk of HBV-related LC was assessed by meta-analysis of the two cohorts. Of the 12 SNPs reported in HBV-related HCC GWASs, five SNPs (rs7574865 in STAT4, rs9267673 near C2, rs2647073 and rs3997872 near HLA-DRB1 and rs9275319 near HLA-DQ), were found to be significantly associated with the risk of HBV-related LC (rs7574865: P = 1.79 × 10(-2), OR = 1.17, 95% CI = 1.03-1.34; rs9267673: P = 4.91 × 10(-4), OR = 1.37, 95% CI = 1.15-1.63; rs2647073: P = 3.53 × 10(-5), OR = 1.63, 95% CI = 1.29-2.06; rs3997872: P = 4.22 × 10(-4), OR = 1.86, 95% CI = 1.32-2.62; rs9275319: P = 1.30 × 10(-2), OR = 1.32, 95% CI = 1.06-1.64). However, among the three SNPs associated with the risk of HCV-related HCC in previous GWASs, none of them showed significant association with the risk of HBV-related LC. Our results suggested that genetic variants associated with HBV-related hepatocarcinogenesis may already play an important role in the progression from CHB to LC.",Recent genome-wide associated studies (GWASs) have revealed several common loci associated with the risk of hepatitis B virus (HBV)- or hepatitis C virus (HCV)-related hepatocellular carcinoma (HCC). ,6,1,Case-Control Studies,0
26538132,"Recent genome-wide associated studies (GWASs) have revealed several common loci associated with the risk of hepatitis B virus (HBV)- or hepatitis C virus (HCV)-related hepatocellular carcinoma (HCC). We selected 15 single nucleotide polymorphisms (SNPs) identified through GWASs on HBV- or HCV-related HCC, and genotyped them in two independent Chinese cohorts of chronic HBV carriers, including 712 LC cases and 2601 controls. The association of each SNP with the risk of HBV-related LC was assessed by meta-analysis of the two cohorts. Of the 12 SNPs reported in HBV-related HCC GWASs, five SNPs (rs7574865 in STAT4, rs9267673 near C2, rs2647073 and rs3997872 near HLA-DRB1 and rs9275319 near HLA-DQ), were found to be significantly associated with the risk of HBV-related LC (rs7574865: P = 1.79 × 10(-2), OR = 1.17, 95% CI = 1.03-1.34; rs9267673: P = 4.91 × 10(-4), OR = 1.37, 95% CI = 1.15-1.63; rs2647073: P = 3.53 × 10(-5), OR = 1.63, 95% CI = 1.29-2.06; rs3997872: P = 4.22 × 10(-4), OR = 1.86, 95% CI = 1.32-2.62; rs9275319: P = 1.30 × 10(-2), OR = 1.32, 95% CI = 1.06-1.64). However, among the three SNPs associated with the risk of HCV-related HCC in previous GWASs, none of them showed significant association with the risk of HBV-related LC. Our results suggested that genetic variants associated with HBV-related hepatocarcinogenesis may already play an important role in the progression from CHB to LC.","We selected 15 single nucleotide polymorphisms (SNPs) identified through GWASs on HBV- or HCV-related HCC, and genotyped them in two independent Chinese cohorts of chronic HBV carriers, including 712 LC cases and 2601 controls. ",6,2,Case-Control Studies,0
26538132,"Recent genome-wide associated studies (GWASs) have revealed several common loci associated with the risk of hepatitis B virus (HBV)- or hepatitis C virus (HCV)-related hepatocellular carcinoma (HCC). We selected 15 single nucleotide polymorphisms (SNPs) identified through GWASs on HBV- or HCV-related HCC, and genotyped them in two independent Chinese cohorts of chronic HBV carriers, including 712 LC cases and 2601 controls. The association of each SNP with the risk of HBV-related LC was assessed by meta-analysis of the two cohorts. Of the 12 SNPs reported in HBV-related HCC GWASs, five SNPs (rs7574865 in STAT4, rs9267673 near C2, rs2647073 and rs3997872 near HLA-DRB1 and rs9275319 near HLA-DQ), were found to be significantly associated with the risk of HBV-related LC (rs7574865: P = 1.79 × 10(-2), OR = 1.17, 95% CI = 1.03-1.34; rs9267673: P = 4.91 × 10(-4), OR = 1.37, 95% CI = 1.15-1.63; rs2647073: P = 3.53 × 10(-5), OR = 1.63, 95% CI = 1.29-2.06; rs3997872: P = 4.22 × 10(-4), OR = 1.86, 95% CI = 1.32-2.62; rs9275319: P = 1.30 × 10(-2), OR = 1.32, 95% CI = 1.06-1.64). However, among the three SNPs associated with the risk of HCV-related HCC in previous GWASs, none of them showed significant association with the risk of HBV-related LC. Our results suggested that genetic variants associated with HBV-related hepatocarcinogenesis may already play an important role in the progression from CHB to LC.",The association of each SNP with the risk of HBV-related LC was assessed by meta-analysis of the two cohorts. ,6,3,Case-Control Studies,0
26538132,"Recent genome-wide associated studies (GWASs) have revealed several common loci associated with the risk of hepatitis B virus (HBV)- or hepatitis C virus (HCV)-related hepatocellular carcinoma (HCC). We selected 15 single nucleotide polymorphisms (SNPs) identified through GWASs on HBV- or HCV-related HCC, and genotyped them in two independent Chinese cohorts of chronic HBV carriers, including 712 LC cases and 2601 controls. The association of each SNP with the risk of HBV-related LC was assessed by meta-analysis of the two cohorts. Of the 12 SNPs reported in HBV-related HCC GWASs, five SNPs (rs7574865 in STAT4, rs9267673 near C2, rs2647073 and rs3997872 near HLA-DRB1 and rs9275319 near HLA-DQ), were found to be significantly associated with the risk of HBV-related LC (rs7574865: P = 1.79 × 10(-2), OR = 1.17, 95% CI = 1.03-1.34; rs9267673: P = 4.91 × 10(-4), OR = 1.37, 95% CI = 1.15-1.63; rs2647073: P = 3.53 × 10(-5), OR = 1.63, 95% CI = 1.29-2.06; rs3997872: P = 4.22 × 10(-4), OR = 1.86, 95% CI = 1.32-2.62; rs9275319: P = 1.30 × 10(-2), OR = 1.32, 95% CI = 1.06-1.64). However, among the three SNPs associated with the risk of HCV-related HCC in previous GWASs, none of them showed significant association with the risk of HBV-related LC. Our results suggested that genetic variants associated with HBV-related hepatocarcinogenesis may already play an important role in the progression from CHB to LC.","Of the 12 SNPs reported in HBV-related HCC GWASs, five SNPs (rs7574865 in STAT4, rs9267673 near C2, rs2647073 and rs3997872 near HLA-DRB1 and rs9275319 near HLA-DQ), were found to be significantly associated with the risk of HBV-related LC (rs7574865: P\u2009=\u20091.79\u2009×\u200910(-2), OR\u2009=\u20091.17, 95% CI\u2009=\u20091.03-1.34; rs9267673: P\u2009=\u20094.91\u2009×\u200910(-4), OR\u2009=\u20091.37, 95% CI\u2009=\u20091.15-1.63; rs2647073: P\u2009=\u20093.53\u2009×\u200910(-5), OR\u2009=\u20091.63, 95% CI\u2009=\u20091.29-2.06; rs3997872: P\u2009=\u20094.22\u2009×\u200910(-4), OR\u2009=\u20091.86, 95% CI\u2009=\u20091.32-2.62; rs9275319: P\u2009=\u20091.30\u2009×\u200910(-2), OR\u2009=\u20091.32, 95% CI\u2009=\u20091.06-1.64). ",6,4,Case-Control Studies,3
26538132,"Recent genome-wide associated studies (GWASs) have revealed several common loci associated with the risk of hepatitis B virus (HBV)- or hepatitis C virus (HCV)-related hepatocellular carcinoma (HCC). We selected 15 single nucleotide polymorphisms (SNPs) identified through GWASs on HBV- or HCV-related HCC, and genotyped them in two independent Chinese cohorts of chronic HBV carriers, including 712 LC cases and 2601 controls. The association of each SNP with the risk of HBV-related LC was assessed by meta-analysis of the two cohorts. Of the 12 SNPs reported in HBV-related HCC GWASs, five SNPs (rs7574865 in STAT4, rs9267673 near C2, rs2647073 and rs3997872 near HLA-DRB1 and rs9275319 near HLA-DQ), were found to be significantly associated with the risk of HBV-related LC (rs7574865: P = 1.79 × 10(-2), OR = 1.17, 95% CI = 1.03-1.34; rs9267673: P = 4.91 × 10(-4), OR = 1.37, 95% CI = 1.15-1.63; rs2647073: P = 3.53 × 10(-5), OR = 1.63, 95% CI = 1.29-2.06; rs3997872: P = 4.22 × 10(-4), OR = 1.86, 95% CI = 1.32-2.62; rs9275319: P = 1.30 × 10(-2), OR = 1.32, 95% CI = 1.06-1.64). However, among the three SNPs associated with the risk of HCV-related HCC in previous GWASs, none of them showed significant association with the risk of HBV-related LC. Our results suggested that genetic variants associated with HBV-related hepatocarcinogenesis may already play an important role in the progression from CHB to LC.","However, among the three SNPs associated with the risk of HCV-related HCC in previous GWASs, none of them showed significant association with the risk of HBV-related LC. ",6,5,Case-Control Studies,3
26538132,"Recent genome-wide associated studies (GWASs) have revealed several common loci associated with the risk of hepatitis B virus (HBV)- or hepatitis C virus (HCV)-related hepatocellular carcinoma (HCC). We selected 15 single nucleotide polymorphisms (SNPs) identified through GWASs on HBV- or HCV-related HCC, and genotyped them in two independent Chinese cohorts of chronic HBV carriers, including 712 LC cases and 2601 controls. The association of each SNP with the risk of HBV-related LC was assessed by meta-analysis of the two cohorts. Of the 12 SNPs reported in HBV-related HCC GWASs, five SNPs (rs7574865 in STAT4, rs9267673 near C2, rs2647073 and rs3997872 near HLA-DRB1 and rs9275319 near HLA-DQ), were found to be significantly associated with the risk of HBV-related LC (rs7574865: P = 1.79 × 10(-2), OR = 1.17, 95% CI = 1.03-1.34; rs9267673: P = 4.91 × 10(-4), OR = 1.37, 95% CI = 1.15-1.63; rs2647073: P = 3.53 × 10(-5), OR = 1.63, 95% CI = 1.29-2.06; rs3997872: P = 4.22 × 10(-4), OR = 1.86, 95% CI = 1.32-2.62; rs9275319: P = 1.30 × 10(-2), OR = 1.32, 95% CI = 1.06-1.64). However, among the three SNPs associated with the risk of HCV-related HCC in previous GWASs, none of them showed significant association with the risk of HBV-related LC. Our results suggested that genetic variants associated with HBV-related hepatocarcinogenesis may already play an important role in the progression from CHB to LC.",Our results suggested that genetic variants associated with HBV-related hepatocarcinogenesis may already play an important role in the progression from CHB to LC.,6,6,Case-Control Studies,2
12766226,"In a myasthenic syndrome associated with fatigable generalized weakness and recurrent attacks of respiratory and bulbar paralysis since birth, nerve stimulation at physiologic rates rapidly decremented the compound muscle action potential. Intercostal muscle studies revealed no abnormality of the resting membrane potential, evoked quantal release, synaptic potentials, acetylcholine receptor channel kinetics, or endplate ultrastructure, but endplate potentials depolarizing the resting potential to -40 mV failed to excite action potentials. Pursuing this clue, we sequenced SCN4A encoding the skeletal muscle sodium channel (Nav1.4) and detected two heteroallelic mutations involving conserved residues not present in 400 normal alleles: S246L in the S4/S5 cytoplasmic linker in domain I, and V1442E in the S3/S4 extracellular linker in domain IV. The genetically engineered V1442E-Na channel expressed in HEK cells shows marked enhancement of fast inactivation close to the resting potential, and enhanced use-dependent inactivation on high-frequency stimulation; S246L is likely a benign polymorphism. The V1442E mutation in SCN4A defines a novel disease mechanism and a novel phenotype with myasthenic features.","In a myasthenic syndrome associated with fatigable generalized weakness and recurrent attacks of respiratory and bulbar paralysis since birth, nerve stimulation at physiologic rates rapidly decremented the compound muscle action potential. ",5,1,Case-Control Studies,1
12766226,"In a myasthenic syndrome associated with fatigable generalized weakness and recurrent attacks of respiratory and bulbar paralysis since birth, nerve stimulation at physiologic rates rapidly decremented the compound muscle action potential. Intercostal muscle studies revealed no abnormality of the resting membrane potential, evoked quantal release, synaptic potentials, acetylcholine receptor channel kinetics, or endplate ultrastructure, but endplate potentials depolarizing the resting potential to -40 mV failed to excite action potentials. Pursuing this clue, we sequenced SCN4A encoding the skeletal muscle sodium channel (Nav1.4) and detected two heteroallelic mutations involving conserved residues not present in 400 normal alleles: S246L in the S4/S5 cytoplasmic linker in domain I, and V1442E in the S3/S4 extracellular linker in domain IV. The genetically engineered V1442E-Na channel expressed in HEK cells shows marked enhancement of fast inactivation close to the resting potential, and enhanced use-dependent inactivation on high-frequency stimulation; S246L is likely a benign polymorphism. The V1442E mutation in SCN4A defines a novel disease mechanism and a novel phenotype with myasthenic features.","Intercostal muscle studies revealed no abnormality of the resting membrane potential, evoked quantal release, synaptic potentials, acetylcholine receptor channel kinetics, or endplate ultrastructure, but endplate potentials depolarizing the resting potential to -40 mV failed to excite action potentials. ",5,2,Case-Control Studies,1
12766226,"In a myasthenic syndrome associated with fatigable generalized weakness and recurrent attacks of respiratory and bulbar paralysis since birth, nerve stimulation at physiologic rates rapidly decremented the compound muscle action potential. Intercostal muscle studies revealed no abnormality of the resting membrane potential, evoked quantal release, synaptic potentials, acetylcholine receptor channel kinetics, or endplate ultrastructure, but endplate potentials depolarizing the resting potential to -40 mV failed to excite action potentials. Pursuing this clue, we sequenced SCN4A encoding the skeletal muscle sodium channel (Nav1.4) and detected two heteroallelic mutations involving conserved residues not present in 400 normal alleles: S246L in the S4/S5 cytoplasmic linker in domain I, and V1442E in the S3/S4 extracellular linker in domain IV. The genetically engineered V1442E-Na channel expressed in HEK cells shows marked enhancement of fast inactivation close to the resting potential, and enhanced use-dependent inactivation on high-frequency stimulation; S246L is likely a benign polymorphism. The V1442E mutation in SCN4A defines a novel disease mechanism and a novel phenotype with myasthenic features.","Pursuing this clue, we sequenced SCN4A encoding the skeletal muscle sodium channel (Nav1.4) and detected two heteroallelic mutations involving conserved residues not present in 400 normal alleles: S246L in the S4/S5 cytoplasmic linker in domain I, and V1442E in the S3/S4 extracellular linker in domain IV. ",5,3,Case-Control Studies,0
12766226,"In a myasthenic syndrome associated with fatigable generalized weakness and recurrent attacks of respiratory and bulbar paralysis since birth, nerve stimulation at physiologic rates rapidly decremented the compound muscle action potential. Intercostal muscle studies revealed no abnormality of the resting membrane potential, evoked quantal release, synaptic potentials, acetylcholine receptor channel kinetics, or endplate ultrastructure, but endplate potentials depolarizing the resting potential to -40 mV failed to excite action potentials. Pursuing this clue, we sequenced SCN4A encoding the skeletal muscle sodium channel (Nav1.4) and detected two heteroallelic mutations involving conserved residues not present in 400 normal alleles: S246L in the S4/S5 cytoplasmic linker in domain I, and V1442E in the S3/S4 extracellular linker in domain IV. The genetically engineered V1442E-Na channel expressed in HEK cells shows marked enhancement of fast inactivation close to the resting potential, and enhanced use-dependent inactivation on high-frequency stimulation; S246L is likely a benign polymorphism. The V1442E mutation in SCN4A defines a novel disease mechanism and a novel phenotype with myasthenic features.","The genetically engineered V1442E-Na channel expressed in HEK cells shows marked enhancement of fast inactivation close to the resting potential, and enhanced use-dependent inactivation on high-frequency stimulation; S246L is likely a benign polymorphism. ",5,4,Case-Control Studies,3
12766226,"In a myasthenic syndrome associated with fatigable generalized weakness and recurrent attacks of respiratory and bulbar paralysis since birth, nerve stimulation at physiologic rates rapidly decremented the compound muscle action potential. Intercostal muscle studies revealed no abnormality of the resting membrane potential, evoked quantal release, synaptic potentials, acetylcholine receptor channel kinetics, or endplate ultrastructure, but endplate potentials depolarizing the resting potential to -40 mV failed to excite action potentials. Pursuing this clue, we sequenced SCN4A encoding the skeletal muscle sodium channel (Nav1.4) and detected two heteroallelic mutations involving conserved residues not present in 400 normal alleles: S246L in the S4/S5 cytoplasmic linker in domain I, and V1442E in the S3/S4 extracellular linker in domain IV. The genetically engineered V1442E-Na channel expressed in HEK cells shows marked enhancement of fast inactivation close to the resting potential, and enhanced use-dependent inactivation on high-frequency stimulation; S246L is likely a benign polymorphism. The V1442E mutation in SCN4A defines a novel disease mechanism and a novel phenotype with myasthenic features.",The V1442E mutation in SCN4A defines a novel disease mechanism and a novel phenotype with myasthenic features.,5,5,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. ,14,1,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",Gene-expression profiling is widely used to identify genes associated with such progression. ,14,2,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",Usually candidate genes are identified according to a gene-by-gene comparison of expression. ,14,3,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. ,14,4,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. ,14,5,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.","We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. ",14,6,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). ,14,7,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",These two gene pairs were validated in three independent data sets. ,14,8,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.","In addition, combining two pairs of genes improved sensitivity without compromising specificity. ",14,9,Case-Control Studies,3
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.","Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. ",14,10,Case-Control Studies,3
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.","In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. ",14,11,Case-Control Studies,3
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.","In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. ",14,12,Case-Control Studies,0
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",Two gene pairs can predict which men would experience progression to the metastatic form of the disease. ,14,13,Case-Control Studies,3
20386565,"Prediction of cancer progression after radical prostatectomy is one of the most challenging problems in the management of prostate cancer. Gene-expression profiling is widely used to identify genes associated with such progression. Usually candidate genes are identified according to a gene-by-gene comparison of expression. Recent reports suggested that relative expression of a gene pair more efficiently predicts cancer progression than single-gene analysis does. The top-scoring pair (TSP) algorithm classifies phenotypes according to the relative expression of a pair of genes. We applied the TSP approach to predict, which patients would experience systemic tumor progression after radical prostatectomy. Relative expression of TPD52L2/SQLE and CEACAM1/BRCA1 gene pairs identified those patients with more than 99% specificity but relatively low sensitivity (approximately 10%). These two gene pairs were validated in three independent data sets. In addition, combining two pairs of genes improved sensitivity without compromising specificity. Functional annotation of the TSP genes showed that they cluster by a limited number of biological functions and pathways, suggesting that relatively lower expression of genes from specific pathways can predict cancer progression. In conclusion, comparative analysis of the expression of two genes may be a simple and effective classifier for prediction of prostate cancer progression. In summary, the TSP approach can be used to identify patients whose prostate cancer will progress after they undergo radical prostatectomy. Two gene pairs can predict which men would experience progression to the metastatic form of the disease. However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.","However, because our analysis was based on a relatively small number of genes, a larger study will be needed to identify the best predictors of disease outcome overall.",14,14,Case-Control Studies,0
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.","Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. ",10,1,Case-Control Studies,3
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.","Moreover, the differences among stroke subtypes remain unclear. ",10,2,Case-Control Studies,0
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.","This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. ",10,3,Case-Control Studies,0
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.",We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. ,10,4,Case-Control Studies,0
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.","A total of 333 patients (n\u2009=\u2009223, 67.0%, with ischemic stroke; n\u2009=\u2009110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. ",10,5,Case-Control Studies,0
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.",Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. ,10,6,Case-Control Studies,0
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.",Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. ,10,7,Case-Control Studies,0
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.","Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. ",10,8,Case-Control Studies,3
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.","Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β\u2009=\u20090.84), and the result was proven to be robust. ",10,9,Case-Control Studies,3
27698374,"Sleep habits are associated with stroke in western populations, but this relation has been rarely investigated in China. Moreover, the differences among stroke subtypes remain unclear. This study aimed to explore the associations of total stroke, including ischemic and hemorrhagic type, with sleep habits of a population in southern China. We performed a case-control study in patients admitted to the hospital with first stroke and community control subjects. A total of 333 patients (n = 223, 67.0%, with ischemic stroke; n = 110, 23.0%, with hemorrhagic stroke) and 547 controls were enrolled in the study. Participants completed a structured questionnaire to identify sleep habits and other stroke risk factors. Least absolute shrinkage and selection operator (Lasso) and multiple logistic regression were performed to identify risk factors of disease. Incidence of stroke, and its subtypes, was significantly associated with snorting/gasping, snoring, sleep duration, and daytime napping. Snorting/gasping was identified as an important risk factor in the Lasso logistic regression model (Lasso' β = 0.84), and the result was proven to be robust. This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.",This study showed the association between stroke and sleep habits in the southern Chinese population and might help in better detecting important sleep-related factors for stroke risk.,10,10,Case-Control Studies,3
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. ",10,1,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. ",10,2,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. ",10,3,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. ",10,4,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. ",10,5,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. ",10,6,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. ",10,7,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. ",10,8,Case-Control Studies,0
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). ",10,9,Case-Control Studies,3
31630191,"Genome-wide association study (GWAS)-identified single-nucleotide polymorphisms (SNPs) are tag SNPs located in both transcribed and non-coding regulatory DNA regions, rather than representing causal or functional variants for disease. To identify functional variants or genes for melanoma susceptibility, we used functional mapping and annotation (FUMA) to perform functional annotation of the summary statistics of 2541 significant melanoma risk SNPs (P < 5 × 10-8) identified by GWAS. The original GWAS melanoma study included 15 990 cases and 26 409 controls, representing the largest international meta-analysis of melanoma susceptibility. We prioritized 330 unique genes, including those in immune cytokine signaling pathways, from 19 loci through positional, expression quantitative trait locus, and chromatin interaction mapping. In comparison, only 38 melanoma-related genes were identified in the original meta-analysis. In addition to the well-known melanoma susceptibility genes confirmed in the meta-analysis (MC1R, CDKN2A, TERT, OCA2 and ARNT/SETDB1), we also identified additional novel genes using FUMA to map SNPs to genes. Through chromatin interaction mapping, we prioritized IFNA7, IFNA10, IFNA16, IFNA17, IFNA14, IFNA6, IFNA21, IFNA4, IFNE and IFNA5; these 10 most significant genes are all involved in immune system and cytokine signaling pathways. In the gene analysis, we identified 72 genes with a P < 2.5 × 10-6. The genes associated with melanoma risk were DEF8 (P = 1.09 × 10-57), DBNDD1 (P = 2.19 × 10-42), SPATA33 (P = 3.54 × 10-38) and MC1R (P = 1.04 × 10-36). In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.","In summary, this study identifies novel putative melanoma susceptibility genes and provides a guide for further experimental validation of functional variants and disease-related genes.",10,10,Case-Control Studies,0
23633923,"BIM is a proapoptotic member of the Bcl-2 family. Here, we investigated the epigenetic status of the BIM locus in NPM/ALK+ anaplastic large cell lymphoma (ALCL) cell lines and in lymph node biopsies from NPM/ALK+ ALCL patients. We show that BIM is epigenetically silenced in cell lines and lymph node specimens and that treatment with the deacetylase inhibitor trichostatin A restores the histone acetylation, strongly upregulates BIM expression, and induces cell death. BIM silencing occurs through recruitment of MeCP2 and the SIN3a/histone deacetylase 1/2 (HDAC1/2) corepressor complex. This event requires BIM CpG methylation/demethylation with 5-azacytidine that leads to detachment of the MeCP2 corepressor complex and reacetylation of the histone tails. Treatment with the ALK inhibitor PF2341066 or with an inducible shRNA targeting NPM/ALK does not restore BIM locus reacetylation; however, enforced expression of NPM/ALK in an NPM/ALK-negative cell line significantly increases the methylation at the BIM locus. This study demonstrates that BIM is epigenetically silenced in NPM/ALK-positive cells through recruitment of the SIN3a/HDAC1/2 corepressor complex and that NPM/ALK is dispensable to maintain BIM epigenetic silencing but is able to act as an inducer of BIM methylation.",BIM is a proapoptotic member of the Bcl-2 family. ,7,1,Case-Control Studies,0
23633923,"BIM is a proapoptotic member of the Bcl-2 family. Here, we investigated the epigenetic status of the BIM locus in NPM/ALK+ anaplastic large cell lymphoma (ALCL) cell lines and in lymph node biopsies from NPM/ALK+ ALCL patients. We show that BIM is epigenetically silenced in cell lines and lymph node specimens and that treatment with the deacetylase inhibitor trichostatin A restores the histone acetylation, strongly upregulates BIM expression, and induces cell death. BIM silencing occurs through recruitment of MeCP2 and the SIN3a/histone deacetylase 1/2 (HDAC1/2) corepressor complex. This event requires BIM CpG methylation/demethylation with 5-azacytidine that leads to detachment of the MeCP2 corepressor complex and reacetylation of the histone tails. Treatment with the ALK inhibitor PF2341066 or with an inducible shRNA targeting NPM/ALK does not restore BIM locus reacetylation; however, enforced expression of NPM/ALK in an NPM/ALK-negative cell line significantly increases the methylation at the BIM locus. This study demonstrates that BIM is epigenetically silenced in NPM/ALK-positive cells through recruitment of the SIN3a/HDAC1/2 corepressor complex and that NPM/ALK is dispensable to maintain BIM epigenetic silencing but is able to act as an inducer of BIM methylation.","Here, we investigated the epigenetic status of the BIM locus in NPM/ALK+ anaplastic large cell lymphoma (ALCL) cell lines and in lymph node biopsies from NPM/ALK+ ALCL patients. ",7,2,Case-Control Studies,0
23633923,"BIM is a proapoptotic member of the Bcl-2 family. Here, we investigated the epigenetic status of the BIM locus in NPM/ALK+ anaplastic large cell lymphoma (ALCL) cell lines and in lymph node biopsies from NPM/ALK+ ALCL patients. We show that BIM is epigenetically silenced in cell lines and lymph node specimens and that treatment with the deacetylase inhibitor trichostatin A restores the histone acetylation, strongly upregulates BIM expression, and induces cell death. BIM silencing occurs through recruitment of MeCP2 and the SIN3a/histone deacetylase 1/2 (HDAC1/2) corepressor complex. This event requires BIM CpG methylation/demethylation with 5-azacytidine that leads to detachment of the MeCP2 corepressor complex and reacetylation of the histone tails. Treatment with the ALK inhibitor PF2341066 or with an inducible shRNA targeting NPM/ALK does not restore BIM locus reacetylation; however, enforced expression of NPM/ALK in an NPM/ALK-negative cell line significantly increases the methylation at the BIM locus. This study demonstrates that BIM is epigenetically silenced in NPM/ALK-positive cells through recruitment of the SIN3a/HDAC1/2 corepressor complex and that NPM/ALK is dispensable to maintain BIM epigenetic silencing but is able to act as an inducer of BIM methylation.","We show that BIM is epigenetically silenced in cell lines and lymph node specimens and that treatment with the deacetylase inhibitor trichostatin A restores the histone acetylation, strongly upregulates BIM expression, and induces cell death. ",7,3,Case-Control Studies,1
23633923,"BIM is a proapoptotic member of the Bcl-2 family. Here, we investigated the epigenetic status of the BIM locus in NPM/ALK+ anaplastic large cell lymphoma (ALCL) cell lines and in lymph node biopsies from NPM/ALK+ ALCL patients. We show that BIM is epigenetically silenced in cell lines and lymph node specimens and that treatment with the deacetylase inhibitor trichostatin A restores the histone acetylation, strongly upregulates BIM expression, and induces cell death. BIM silencing occurs through recruitment of MeCP2 and the SIN3a/histone deacetylase 1/2 (HDAC1/2) corepressor complex. This event requires BIM CpG methylation/demethylation with 5-azacytidine that leads to detachment of the MeCP2 corepressor complex and reacetylation of the histone tails. Treatment with the ALK inhibitor PF2341066 or with an inducible shRNA targeting NPM/ALK does not restore BIM locus reacetylation; however, enforced expression of NPM/ALK in an NPM/ALK-negative cell line significantly increases the methylation at the BIM locus. This study demonstrates that BIM is epigenetically silenced in NPM/ALK-positive cells through recruitment of the SIN3a/HDAC1/2 corepressor complex and that NPM/ALK is dispensable to maintain BIM epigenetic silencing but is able to act as an inducer of BIM methylation.",BIM silencing occurs through recruitment of MeCP2 and the SIN3a/histone deacetylase 1/2 (HDAC1/2) corepressor complex. ,7,4,Case-Control Studies,1
23633923,"BIM is a proapoptotic member of the Bcl-2 family. Here, we investigated the epigenetic status of the BIM locus in NPM/ALK+ anaplastic large cell lymphoma (ALCL) cell lines and in lymph node biopsies from NPM/ALK+ ALCL patients. We show that BIM is epigenetically silenced in cell lines and lymph node specimens and that treatment with the deacetylase inhibitor trichostatin A restores the histone acetylation, strongly upregulates BIM expression, and induces cell death. BIM silencing occurs through recruitment of MeCP2 and the SIN3a/histone deacetylase 1/2 (HDAC1/2) corepressor complex. This event requires BIM CpG methylation/demethylation with 5-azacytidine that leads to detachment of the MeCP2 corepressor complex and reacetylation of the histone tails. Treatment with the ALK inhibitor PF2341066 or with an inducible shRNA targeting NPM/ALK does not restore BIM locus reacetylation; however, enforced expression of NPM/ALK in an NPM/ALK-negative cell line significantly increases the methylation at the BIM locus. This study demonstrates that BIM is epigenetically silenced in NPM/ALK-positive cells through recruitment of the SIN3a/HDAC1/2 corepressor complex and that NPM/ALK is dispensable to maintain BIM epigenetic silencing but is able to act as an inducer of BIM methylation.",This event requires BIM CpG methylation/demethylation with 5-azacytidine that leads to detachment of the MeCP2 corepressor complex and reacetylation of the histone tails. ,7,5,Case-Control Studies,1
23633923,"BIM is a proapoptotic member of the Bcl-2 family. Here, we investigated the epigenetic status of the BIM locus in NPM/ALK+ anaplastic large cell lymphoma (ALCL) cell lines and in lymph node biopsies from NPM/ALK+ ALCL patients. We show that BIM is epigenetically silenced in cell lines and lymph node specimens and that treatment with the deacetylase inhibitor trichostatin A restores the histone acetylation, strongly upregulates BIM expression, and induces cell death. BIM silencing occurs through recruitment of MeCP2 and the SIN3a/histone deacetylase 1/2 (HDAC1/2) corepressor complex. This event requires BIM CpG methylation/demethylation with 5-azacytidine that leads to detachment of the MeCP2 corepressor complex and reacetylation of the histone tails. Treatment with the ALK inhibitor PF2341066 or with an inducible shRNA targeting NPM/ALK does not restore BIM locus reacetylation; however, enforced expression of NPM/ALK in an NPM/ALK-negative cell line significantly increases the methylation at the BIM locus. This study demonstrates that BIM is epigenetically silenced in NPM/ALK-positive cells through recruitment of the SIN3a/HDAC1/2 corepressor complex and that NPM/ALK is dispensable to maintain BIM epigenetic silencing but is able to act as an inducer of BIM methylation.",This study demonstrates that BIM is epigenetically silenced in NPM/ALK-positive cells through recruitment of the SIN3a/HDAC1/2 corepressor complex and that NPM/ALK is dispensable to maintain BIM epigenetic silencing but is able to act as an inducer of BIM methylation.,7,7,Case-Control Studies,1
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.","Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. ",10,1,Case-Control Studies,1
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.","Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. ",10,3,Case-Control Studies,3
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.","However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. ",10,4,Case-Control Studies,0
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.","We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. ",10,5,Case-Control Studies,0
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.",Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P\xa0<\xa00.05) with risk of breast cancer in a log-additive model. ,10,6,Case-Control Studies,3
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.","In addition, 60 SNPs were associated (P\xa0<\xa00.05) with risk of high grade breast cancer. ",10,7,Case-Control Studies,3
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.","However, none of these associations were significant after Bonferroni correction for multiple testing. ",10,8,Case-Control Studies,3
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.","In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P\xa0<\xa00.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. ",10,9,Case-Control Studies,3
21607584,"Mitotic regulatory pathways insure proper timing of mitotic entry, sister chromatid cohesion and separation, and cytokinesis. Disruption of this process results in inappropriate chromosome segregation and aneuploidy, and appears to contribute to cancer. Specifically, disregulation and somatic mutation of mitotic regulators has been observed in human cancers, and overexpression of mitotic regulators is common in aggressive and late stage tumors. However, the role of germline variation in mitotic pathways and risk of cancer is not well understood. We tested 1,084 haplotype-tagging and functional variants from 164 genes in mitotic regulatory pathways in 791 Caucasian women with breast cancer and 843 healthy controls for association with risk of overall and high grade breast cancer. Sixty-one single nucleotide polymorphisms (SNPs) from 40 genes were associated (P < 0.05) with risk of breast cancer in a log-additive model. In addition, 60 SNPs were associated (P < 0.05) with risk of high grade breast cancer. However, none of these associations were significant after Bonferroni correction for multiple testing. In gene-level analyses, CDC25C, SCC1/RAD21, TLK2, and SMC6L1 were associated (P < 0.05) with overall breast cancer risk, CDC6, CDC27, SUMO3, RASSF1, KIF2, and CDC14A were associated with high grade breast cancer risk, and EIF3S10 and CDC25A were associated with both. Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.",Further investigation in breast and other cancers are needed to understand the influence of inherited variation in mitotic genes on tumor grade and cancer risk.,10,10,Case-Control Studies,0
32498097,"The novel coronavirus, SARS-CoV-2, is causing a global pandemic of life-threatening multiorgan disease, called COVID-19. Accumulating evidence indicates that patients with COVID-19 are at significant risk of thromboembolic complications, mainly affecting the venous, but also the arterial vascular system. While the risk of venous thromboembolism (VTE) appears to be higher in patients requiring intensive care unit support compared to those admitted to general wards, recent autopsy findings and data on the timing of VTE diagnosis relative to hospitalization clearly suggest that thromboembolic events also contribute to morbidity and mortality in the ambulatory setting. In addition to a severe hypercoagulable state caused by systemic inflammation and viral endotheliitis, some patients with advanced COVID-19 may develop a coagulopathy, which meets established laboratory criteria for disseminated intravascular coagulation, but is not typically associated with relevant bleeding. Similar to other medical societies, the Society of Thrombosis and Haemostasis Research has issued empirical recommendations on initiation, dosing, and duration of pharmacological VTE prophylaxis in COVID-19 patients.","The novel coronavirus, SARS-CoV-2, is causing a global pandemic of life-threatening multiorgan disease, called COVID-19. ",5,1,Case-Control Studies,0
32498097,"The novel coronavirus, SARS-CoV-2, is causing a global pandemic of life-threatening multiorgan disease, called COVID-19. Accumulating evidence indicates that patients with COVID-19 are at significant risk of thromboembolic complications, mainly affecting the venous, but also the arterial vascular system. While the risk of venous thromboembolism (VTE) appears to be higher in patients requiring intensive care unit support compared to those admitted to general wards, recent autopsy findings and data on the timing of VTE diagnosis relative to hospitalization clearly suggest that thromboembolic events also contribute to morbidity and mortality in the ambulatory setting. In addition to a severe hypercoagulable state caused by systemic inflammation and viral endotheliitis, some patients with advanced COVID-19 may develop a coagulopathy, which meets established laboratory criteria for disseminated intravascular coagulation, but is not typically associated with relevant bleeding. Similar to other medical societies, the Society of Thrombosis and Haemostasis Research has issued empirical recommendations on initiation, dosing, and duration of pharmacological VTE prophylaxis in COVID-19 patients.","Similar to other medical societies, the Society of Thrombosis and Haemostasis Research has issued empirical recommendations on initiation, dosing, and duration of pharmacological VTE prophylaxis in COVID-19 patients.",5,5,Case-Control Studies,0
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.","This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. ",8,1,Case-Control Studies,0
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.","The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. ",8,2,Case-Control Studies,0
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.",The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).,8,3,Case-Control Studies,0
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.","In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P\u200a=\u200a.04) and was 1.71 times risk developing CHD, compared with AA (OR\u200a=\u200a1.71, 95%CI\u200a=\u200a1.02-2.86), and T allele had 1.63 times risk for carriers (OR\u200a=\u200a1.63, 95%CI\u200a=\u200a1.05-2.54). ",8,4,Case-Control Studies,3
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.","Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P\u200a<\u200a.05) and it was closely associated with CHD susceptibility. ",8,5,Case-Control Studies,3
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.","At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. ",8,6,Case-Control Studies,3
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.",Environmental factors are found the interaction with AKT1 polymorphisms. ,8,7,Case-Control Studies,3
32590727,"This study aimed to explore the combined association between AKT serine/threonine kinase 1 (AKT1) polymorphisms and congenital heart disease (CHD) risk, meanwhile, the role of AKT1 single polymorphism on CHD was also analyzed.In the first, AKT1 polymorphisms were genotyped in 130 CHD patients and 145 healthy people with the way of polymerase chain reaction-direct sequencing. The clinical data and genotypes, alleles between 2 groups were compared by χ test and the genotype distributions in the control group were checked by Hardy-Weinberg equilibrium. The relative risk strength of disease based on genetic variant was revealed using odds ratio (OR) with 95% confidence interval (95%CI).In 3 polymorphisms of AKT1 (rs1130214, rs2494732, rs3803300), the GT/TT genotype of rs1130214 in cases and controls had a significant frequency difference (P = .04) and was 1.71 times risk developing CHD, compared with AA (OR = 1.71, 95%CI = 1.02-2.86), and T allele had 1.63 times risk for carriers (OR = 1.63, 95%CI = 1.05-2.54). Similarly, both rs3803300 GG genotype and G allele had obvious differences between case and control groups (P < .05) and it was closely associated with CHD susceptibility. At the same time, the combined effects of rs1130214, rs3803300 and family history, smoking were found in our study.AKT1 rs1130214, rs3803300 polymorphisms are associated with the increased susceptibility to CHD. Environmental factors are found the interaction with AKT1 polymorphisms. Further study is needed to verify this conclusion.",Further study is needed to verify this conclusion.,8,8,Case-Control Studies,0
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.","Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. ",8,1,Case-Control Studies,1
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.","In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. ",8,2,Case-Control Studies,0
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.",The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. ,8,3,Case-Control Studies,0
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.",Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ,8,4,Case-Control Studies,0
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.",ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. ,8,5,Case-Control Studies,0
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.",The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. ,8,6,Case-Control Studies,0
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.","We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). ",8,7,Case-Control Studies,3
23089924,"Osteoarthritis is a degenerative joint disorder resulting in destruction of articular cartilage, osteophyte formation, and subchondral bone sclerosis. In recent years, numerous genetic factors have been identified and implicated in osteoarthritis. The aim of the current study was to examine the influence of methylenetetrahydrofolate reductase (MTHFR) gene C677T mutation and angiotensin converting enzyme (ACE) gene insertion/deletion (I/D) variations on the risk of osteoarthritis. Genomic DNA is obtained from 421 persons (221 patients with osteoarthritis and 200 healthy controls). ACE gene I/D polymorphism genotypes were determined using polymerase chain reaction using I and D allele-specific primers. The MTHFR C677T mutation was analyzed by polymerase chain reaction (PCR) based restriction fragment length polymorphism (RFLP) methods. We found significant difference between the groups with respect to both ACE and MTHFR genotype distributions (p< 0.001, p< 0.001 respectively). Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.",Our study suggests that ACE gene DD genotype and MTHFR gene CC genotype could be used as genetic markers in osteoarthritis in Turkish study populations.,8,8,Case-Control Studies,3
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.",The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. ,11,1,Case-Control Studies,0
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.","Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. ",11,2,Case-Control Studies,0
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.",Twelve young participants were included as controls. ,11,3,Case-Control Studies,0
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.",Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. ,11,4,Case-Control Studies,0
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.","During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. ",11,5,Case-Control Studies,0
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.","The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. ",11,6,Case-Control Studies,3
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.",Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. ,11,7,Case-Control Studies,3
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.","Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. ",11,8,Case-Control Studies,3
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.",Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. ,11,9,Case-Control Studies,3
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.",This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. ,11,10,Case-Control Studies,1
19632845,"The purpose of this study was to investigate the effect of subsensory vibratory noise applied to the soles of the feet on gait variability in a population of elderly recurrent fallers compared to non-fallers and young controls. Eighteen elderly recurrent fallers and 18 elderly non-fallers were recruited from the MOBILIZE Boston Study (MBS), a population-based cohort study investigating novel risk factors for falls. Twelve young participants were included as controls. Participants performed three 6-min walking trials while wearing a pair of insoles containing vibrating actuators. During each trial, the noise stimulus was applied for 3 of the 6min, and differences in stride, stance, and swing time variability were analyzed between noise and no-noise conditions. The use of vibrating insoles significantly reduced stride, stance, and swing time variability measures for elderly recurrent fallers. Elderly non-fallers also demonstrated significant reductions in stride and stance time variability. Although young participants showed decreases in all variability measures, the results did not achieve statistical significance. Gait variability reductions with noise were similar between the elderly recurrent fallers and elderly non-fallers. This study supports the hypothesis that subsensory vibratory noise applied to the soles of the feet can reduce gait variability in elderly participants. Future studies are needed to determine if this intervention reduces falls risk.",Future studies are needed to determine if this intervention reduces falls risk.,11,11,Case-Control Studies,0
22763806,"Dysphagia is commonly associated with aging and Parkinson disease and can have a significant impact on a person's quality of life. In some cases, dysphagia may be life-threatening. Animal models may be used to study underlying mechanisms of dysphagia, but paradigms that allow adequate imaging of the swallow in combination with measurement of physiological variables have not been forthcoming. To begin development of methods that allow this, we used videofluorography to record the deglutition behaviors of 22 Fisher 344/Brown Norway rats in young adult (9 months old), old (32 months old), and parkinsonian (unilateral lesion to the medial forebrain bundle) groups. We hypothesized that the old and parkinsonian rats would manifest deficits in deglutition behaviors analogous to those found in human clinical populations. Our results supported our hypothesis in that the old group demonstrated reductions in bolus transport speeds and mastication rate while the parkinsonian rats showed impairments in oral processing. Interpretation of these results should consider the particular animal model, lesion type, and videofluorographic protocol used in this work. Future studies will link swallow imaging data of this kind with physiological and anatomical data in a manner not possible with human participants.","Animal models may be used to study underlying mechanisms of dysphagia, but paradigms that allow adequate imaging of the swallow in combination with measurement of physiological variables have not been forthcoming. ",8,2,Case-Control Studies,0
22763806,"Dysphagia is commonly associated with aging and Parkinson disease and can have a significant impact on a person's quality of life. In some cases, dysphagia may be life-threatening. Animal models may be used to study underlying mechanisms of dysphagia, but paradigms that allow adequate imaging of the swallow in combination with measurement of physiological variables have not been forthcoming. To begin development of methods that allow this, we used videofluorography to record the deglutition behaviors of 22 Fisher 344/Brown Norway rats in young adult (9 months old), old (32 months old), and parkinsonian (unilateral lesion to the medial forebrain bundle) groups. We hypothesized that the old and parkinsonian rats would manifest deficits in deglutition behaviors analogous to those found in human clinical populations. Our results supported our hypothesis in that the old group demonstrated reductions in bolus transport speeds and mastication rate while the parkinsonian rats showed impairments in oral processing. Interpretation of these results should consider the particular animal model, lesion type, and videofluorographic protocol used in this work. Future studies will link swallow imaging data of this kind with physiological and anatomical data in a manner not possible with human participants.","To begin development of methods that allow this, we used videofluorography to record the deglutition behaviors of 22 Fisher 344/Brown Norway rats in young adult (9 months old), old (32 months old), and parkinsonian (unilateral lesion to the medial forebrain bundle) groups. ",8,3,Case-Control Studies,0
22763806,"Dysphagia is commonly associated with aging and Parkinson disease and can have a significant impact on a person's quality of life. In some cases, dysphagia may be life-threatening. Animal models may be used to study underlying mechanisms of dysphagia, but paradigms that allow adequate imaging of the swallow in combination with measurement of physiological variables have not been forthcoming. To begin development of methods that allow this, we used videofluorography to record the deglutition behaviors of 22 Fisher 344/Brown Norway rats in young adult (9 months old), old (32 months old), and parkinsonian (unilateral lesion to the medial forebrain bundle) groups. We hypothesized that the old and parkinsonian rats would manifest deficits in deglutition behaviors analogous to those found in human clinical populations. Our results supported our hypothesis in that the old group demonstrated reductions in bolus transport speeds and mastication rate while the parkinsonian rats showed impairments in oral processing. Interpretation of these results should consider the particular animal model, lesion type, and videofluorographic protocol used in this work. Future studies will link swallow imaging data of this kind with physiological and anatomical data in a manner not possible with human participants.",We hypothesized that the old and parkinsonian rats would manifest deficits in deglutition behaviors analogous to those found in human clinical populations. ,8,4,Case-Control Studies,0
22763806,"Dysphagia is commonly associated with aging and Parkinson disease and can have a significant impact on a person's quality of life. In some cases, dysphagia may be life-threatening. Animal models may be used to study underlying mechanisms of dysphagia, but paradigms that allow adequate imaging of the swallow in combination with measurement of physiological variables have not been forthcoming. To begin development of methods that allow this, we used videofluorography to record the deglutition behaviors of 22 Fisher 344/Brown Norway rats in young adult (9 months old), old (32 months old), and parkinsonian (unilateral lesion to the medial forebrain bundle) groups. We hypothesized that the old and parkinsonian rats would manifest deficits in deglutition behaviors analogous to those found in human clinical populations. Our results supported our hypothesis in that the old group demonstrated reductions in bolus transport speeds and mastication rate while the parkinsonian rats showed impairments in oral processing. Interpretation of these results should consider the particular animal model, lesion type, and videofluorographic protocol used in this work. Future studies will link swallow imaging data of this kind with physiological and anatomical data in a manner not possible with human participants.",Our results supported our hypothesis in that the old group demonstrated reductions in bolus transport speeds and mastication rate while the parkinsonian rats showed impairments in oral processing. ,8,5,Case-Control Studies,3
22763806,"Dysphagia is commonly associated with aging and Parkinson disease and can have a significant impact on a person's quality of life. In some cases, dysphagia may be life-threatening. Animal models may be used to study underlying mechanisms of dysphagia, but paradigms that allow adequate imaging of the swallow in combination with measurement of physiological variables have not been forthcoming. To begin development of methods that allow this, we used videofluorography to record the deglutition behaviors of 22 Fisher 344/Brown Norway rats in young adult (9 months old), old (32 months old), and parkinsonian (unilateral lesion to the medial forebrain bundle) groups. We hypothesized that the old and parkinsonian rats would manifest deficits in deglutition behaviors analogous to those found in human clinical populations. Our results supported our hypothesis in that the old group demonstrated reductions in bolus transport speeds and mastication rate while the parkinsonian rats showed impairments in oral processing. Interpretation of these results should consider the particular animal model, lesion type, and videofluorographic protocol used in this work. Future studies will link swallow imaging data of this kind with physiological and anatomical data in a manner not possible with human participants.","Interpretation of these results should consider the particular animal model, lesion type, and videofluorographic protocol used in this work. ",8,6,Case-Control Studies,0
22763806,"Dysphagia is commonly associated with aging and Parkinson disease and can have a significant impact on a person's quality of life. In some cases, dysphagia may be life-threatening. Animal models may be used to study underlying mechanisms of dysphagia, but paradigms that allow adequate imaging of the swallow in combination with measurement of physiological variables have not been forthcoming. To begin development of methods that allow this, we used videofluorography to record the deglutition behaviors of 22 Fisher 344/Brown Norway rats in young adult (9 months old), old (32 months old), and parkinsonian (unilateral lesion to the medial forebrain bundle) groups. We hypothesized that the old and parkinsonian rats would manifest deficits in deglutition behaviors analogous to those found in human clinical populations. Our results supported our hypothesis in that the old group demonstrated reductions in bolus transport speeds and mastication rate while the parkinsonian rats showed impairments in oral processing. Interpretation of these results should consider the particular animal model, lesion type, and videofluorographic protocol used in this work. Future studies will link swallow imaging data of this kind with physiological and anatomical data in a manner not possible with human participants.",Future studies will link swallow imaging data of this kind with physiological and anatomical data in a manner not possible with human participants.,8,7,Case-Control Studies,0
18528529,"Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.",Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. ,7,1,Case-Control Studies,0
18528529,"Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.","Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. ",7,2,Case-Control Studies,0
18528529,"Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.","MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. ",7,3,Case-Control Studies,0
18528529,"Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.",The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. ,7,4,Case-Control Studies,0
18528529,"Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.",There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). ,7,5,Case-Control Studies,3
18528529,"Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.","Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). ",7,6,Case-Control Studies,3
18528529,"Recent study in a group of German patients with SSc has implicated the SNP in the MCP-1 gene (-2518 A to G) as a factor of susceptibility to SSc. Reflecting the need for replication of genetic association studies, we investigated if this SNP is associated with SSc in another Caucasian population. MCP-1 -2518 A/G genotypes were determined using PCR-SSP in 46 SSc patients and in 449 healthy subjects, all unrelated and of Slovak (Slavonic) origin. The distribution of MCP-1 -2518 A/G genotypes complied with the Hardy-Weinberg equilibrium both in patient and healthy control groups. There was no difference in MCP-1 -2518*G allele frequency between SSc patients and healthy subjects (patients: 0.23; controls: 0.24; P > .05). Furthermore, MCP-1 -2518 GG homozygotes were similarly represented among SSc patients and healthy subjects (P > .05). The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.",The association of MCP-1 -2518 A/G SNP with SSc observed originally in German population was not replicated in the Slovak population.,7,7,Case-Control Studies,3
27482889,"High levels of circulating TNF and its receptors, TNFR1 and TNFR2, predict the progression of diabetic kidney disease (DKD), but their contribution to organ damage in DKD remains largely unknown. Here, we investigated the function of local and systemic TNF in podocyte injury. We cultured human podocytes with sera collected from DKD patients, who displayed elevated TNF levels, and focal segmental glomerulosclerosis (FSGS) patients, whose TNF levels resembled those of healthy patients. Exogenous TNF administration or local TNF expression was equally sufficient to cause free cholesterol-dependent apoptosis in podocytes by acting through a dual mechanism that required a reduction in ATP-binding cassette transporter A1-mediated (ABCA1-mediated) cholesterol efflux and reduced cholesterol esterification by sterol-O-acyltransferase 1 (SOAT1). TNF-induced albuminuria was aggravated in mice with podocyte-specific ABCA1 deficiency and was partially prevented by cholesterol depletion with cyclodextrin. TNF-stimulated free cholesterol-dependent apoptosis in podocytes was mediated by nuclear factor of activated T cells 1 (NFATc1). ABCA1 overexpression or cholesterol depletion was sufficient to reduce albuminuria in mice with podocyte-specific NFATc1 activation. Our data implicate an NFATc1/ABCA1-dependent mechanism in which local TNF is sufficient to cause free cholesterol-dependent podocyte injury irrespective of TNF, TNFR1, or TNFR2 serum levels.","Here, we investigated the function of local and systemic TNF in podocyte injury. ",8,2,Case-Control Studies,0
27482889,"High levels of circulating TNF and its receptors, TNFR1 and TNFR2, predict the progression of diabetic kidney disease (DKD), but their contribution to organ damage in DKD remains largely unknown. Here, we investigated the function of local and systemic TNF in podocyte injury. We cultured human podocytes with sera collected from DKD patients, who displayed elevated TNF levels, and focal segmental glomerulosclerosis (FSGS) patients, whose TNF levels resembled those of healthy patients. Exogenous TNF administration or local TNF expression was equally sufficient to cause free cholesterol-dependent apoptosis in podocytes by acting through a dual mechanism that required a reduction in ATP-binding cassette transporter A1-mediated (ABCA1-mediated) cholesterol efflux and reduced cholesterol esterification by sterol-O-acyltransferase 1 (SOAT1). TNF-induced albuminuria was aggravated in mice with podocyte-specific ABCA1 deficiency and was partially prevented by cholesterol depletion with cyclodextrin. TNF-stimulated free cholesterol-dependent apoptosis in podocytes was mediated by nuclear factor of activated T cells 1 (NFATc1). ABCA1 overexpression or cholesterol depletion was sufficient to reduce albuminuria in mice with podocyte-specific NFATc1 activation. Our data implicate an NFATc1/ABCA1-dependent mechanism in which local TNF is sufficient to cause free cholesterol-dependent podocyte injury irrespective of TNF, TNFR1, or TNFR2 serum levels.","We cultured human podocytes with sera collected from DKD patients, who displayed elevated TNF levels, and focal segmental glomerulosclerosis (FSGS) patients, whose TNF levels resembled those of healthy patients. ",8,3,Case-Control Studies,0
27482889,"High levels of circulating TNF and its receptors, TNFR1 and TNFR2, predict the progression of diabetic kidney disease (DKD), but their contribution to organ damage in DKD remains largely unknown. Here, we investigated the function of local and systemic TNF in podocyte injury. We cultured human podocytes with sera collected from DKD patients, who displayed elevated TNF levels, and focal segmental glomerulosclerosis (FSGS) patients, whose TNF levels resembled those of healthy patients. Exogenous TNF administration or local TNF expression was equally sufficient to cause free cholesterol-dependent apoptosis in podocytes by acting through a dual mechanism that required a reduction in ATP-binding cassette transporter A1-mediated (ABCA1-mediated) cholesterol efflux and reduced cholesterol esterification by sterol-O-acyltransferase 1 (SOAT1). TNF-induced albuminuria was aggravated in mice with podocyte-specific ABCA1 deficiency and was partially prevented by cholesterol depletion with cyclodextrin. TNF-stimulated free cholesterol-dependent apoptosis in podocytes was mediated by nuclear factor of activated T cells 1 (NFATc1). ABCA1 overexpression or cholesterol depletion was sufficient to reduce albuminuria in mice with podocyte-specific NFATc1 activation. Our data implicate an NFATc1/ABCA1-dependent mechanism in which local TNF is sufficient to cause free cholesterol-dependent podocyte injury irrespective of TNF, TNFR1, or TNFR2 serum levels.",Exogenous TNF administration or local TNF expression was equally sufficient to cause free cholesterol-dependent apoptosis in podocytes by acting through a dual mechanism that required a reduction in ATP-binding cassette transporter A1-mediated (ABCA1-mediated) cholesterol efflux and reduced cholesterol esterification by sterol-O-acyltransferase 1 (SOAT1). ,8,4,Case-Control Studies,1
27482889,"High levels of circulating TNF and its receptors, TNFR1 and TNFR2, predict the progression of diabetic kidney disease (DKD), but their contribution to organ damage in DKD remains largely unknown. Here, we investigated the function of local and systemic TNF in podocyte injury. We cultured human podocytes with sera collected from DKD patients, who displayed elevated TNF levels, and focal segmental glomerulosclerosis (FSGS) patients, whose TNF levels resembled those of healthy patients. Exogenous TNF administration or local TNF expression was equally sufficient to cause free cholesterol-dependent apoptosis in podocytes by acting through a dual mechanism that required a reduction in ATP-binding cassette transporter A1-mediated (ABCA1-mediated) cholesterol efflux and reduced cholesterol esterification by sterol-O-acyltransferase 1 (SOAT1). TNF-induced albuminuria was aggravated in mice with podocyte-specific ABCA1 deficiency and was partially prevented by cholesterol depletion with cyclodextrin. TNF-stimulated free cholesterol-dependent apoptosis in podocytes was mediated by nuclear factor of activated T cells 1 (NFATc1). ABCA1 overexpression or cholesterol depletion was sufficient to reduce albuminuria in mice with podocyte-specific NFATc1 activation. Our data implicate an NFATc1/ABCA1-dependent mechanism in which local TNF is sufficient to cause free cholesterol-dependent podocyte injury irrespective of TNF, TNFR1, or TNFR2 serum levels.",TNF-stimulated free cholesterol-dependent apoptosis in podocytes was mediated by nuclear factor of activated T cells 1 (NFATc1). ,8,6,Case-Control Studies,1
27482889,"High levels of circulating TNF and its receptors, TNFR1 and TNFR2, predict the progression of diabetic kidney disease (DKD), but their contribution to organ damage in DKD remains largely unknown. Here, we investigated the function of local and systemic TNF in podocyte injury. We cultured human podocytes with sera collected from DKD patients, who displayed elevated TNF levels, and focal segmental glomerulosclerosis (FSGS) patients, whose TNF levels resembled those of healthy patients. Exogenous TNF administration or local TNF expression was equally sufficient to cause free cholesterol-dependent apoptosis in podocytes by acting through a dual mechanism that required a reduction in ATP-binding cassette transporter A1-mediated (ABCA1-mediated) cholesterol efflux and reduced cholesterol esterification by sterol-O-acyltransferase 1 (SOAT1). TNF-induced albuminuria was aggravated in mice with podocyte-specific ABCA1 deficiency and was partially prevented by cholesterol depletion with cyclodextrin. TNF-stimulated free cholesterol-dependent apoptosis in podocytes was mediated by nuclear factor of activated T cells 1 (NFATc1). ABCA1 overexpression or cholesterol depletion was sufficient to reduce albuminuria in mice with podocyte-specific NFATc1 activation. Our data implicate an NFATc1/ABCA1-dependent mechanism in which local TNF is sufficient to cause free cholesterol-dependent podocyte injury irrespective of TNF, TNFR1, or TNFR2 serum levels.",ABCA1 overexpression or cholesterol depletion was sufficient to reduce albuminuria in mice with podocyte-specific NFATc1 activation. ,8,7,Case-Control Studies,1
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.",The etiology of primary antibody deficiencies is largely unknown. ,12,1,Case-Control Studies,0
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. ",12,2,Case-Control Studies,0
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. ",12,3,Case-Control Studies,0
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. ",12,4,Case-Control Studies,0
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.",All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. ,12,5,Case-Control Studies,3
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. ",12,6,Case-Control Studies,3
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. ",12,7,Case-Control Studies,0
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.",About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. ,12,8,Case-Control Studies,0
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. ",12,9,Case-Control Studies,3
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.",Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. ,12,10,Case-Control Studies,3
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. ",12,11,Case-Control Studies,3
27634199,"The etiology of primary antibody deficiencies is largely unknown. Beside rare monogenic forms, the majority of cases seem to have a more complex genetic basis. Whereas common variable immunodeficiency has been investigated in depth, there are only a few reports on milder primary antibody deficiencies such as idiopathic primary hypogammaglobulinemia and IgG subclass deficiency. We performed flow cytometric immunophenotyping in 33 patients with common variable immunodeficiency, 23 with idiopathic primary hypogammaglobulinemia and 21 with IgG subclass deficiency, as well as in 47 asymptomatic first-degree family members of patients and 101 unrelated healthy controls. All three groups of patients showed decreased memory B- and naïve T-cell subsets and decreased B-cell activating factor receptor expression. In contrast, circulating follicular helper T-cell frequency and expression of inducible T-cell co-stimulator and chemokine receptors were only significantly altered in patients with common variable immunodeficiency. Asymptomatic first-degree family members of patients demonstrated similar, albeit intermediate, alterations in naïve and memory B- and T-cell subsets. About 13% of asymptomatic relatives had an abnormal peripheral B-cell composition. Furthermore, asymptomatic relatives showed decreased levels of CD4+ recent thymic emigrants and increased central memory T cells. Serum IgG and IgM levels were also significantly lower in asymptomatic relatives than in healthy controls. We conclude that, in our cohort, the immunophenotypic landscape of primary antibody deficiencies comprises a spectrum, in which some alterations are shared between all primary antibody deficiencies whereas others are only associated with common variable immunodeficiency. Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.","Importantly, asymptomatic first-degree family members of patients were found to have an intermediate phenotype for peripheral B- and T-cell subsets.",12,12,Case-Control Studies,0
15324550,"An outbreak of Q fever occurred in South Wales, United Kingdom, from July 15 through September 30, 2002. To investigate the outbreak a cohort and nested case-control study of persons who had worked at a cardboard manufacturing plant was conducted. The cohort included 282 employees and subcontractors, of whom 253 (90%) provided blood samples and 214 (76%) completed questionnaires. Ninety-five cases of acute Q fever were identified. The epidemic curve and other data suggested an outbreak source likely occurred August 5-9, 2002. Employees in the factory's offices were at greatest risk for infection (odds ratio 3.46; 95% confidence interval 1.38-9.06). The offices were undergoing renovation work around the time of likely exposure and contained straw board that had repeatedly been drilled. The outbreak may have been caused by aerosolization of Coxiella burnetii spore-like forms during drilling into contaminated straw board.","An outbreak of Q fever occurred in South Wales, United Kingdom, from July 15 through September 30, 2002. ",8,1,Case-Control Studies,0
15324550,"An outbreak of Q fever occurred in South Wales, United Kingdom, from July 15 through September 30, 2002. To investigate the outbreak a cohort and nested case-control study of persons who had worked at a cardboard manufacturing plant was conducted. The cohort included 282 employees and subcontractors, of whom 253 (90%) provided blood samples and 214 (76%) completed questionnaires. Ninety-five cases of acute Q fever were identified. The epidemic curve and other data suggested an outbreak source likely occurred August 5-9, 2002. Employees in the factory's offices were at greatest risk for infection (odds ratio 3.46; 95% confidence interval 1.38-9.06). The offices were undergoing renovation work around the time of likely exposure and contained straw board that had repeatedly been drilled. The outbreak may have been caused by aerosolization of Coxiella burnetii spore-like forms during drilling into contaminated straw board.",To investigate the outbreak a cohort and nested case-control study of persons who had worked at a cardboard manufacturing plant was conducted. ,8,2,Case-Control Studies,0
15324550,"An outbreak of Q fever occurred in South Wales, United Kingdom, from July 15 through September 30, 2002. To investigate the outbreak a cohort and nested case-control study of persons who had worked at a cardboard manufacturing plant was conducted. The cohort included 282 employees and subcontractors, of whom 253 (90%) provided blood samples and 214 (76%) completed questionnaires. Ninety-five cases of acute Q fever were identified. The epidemic curve and other data suggested an outbreak source likely occurred August 5-9, 2002. Employees in the factory's offices were at greatest risk for infection (odds ratio 3.46; 95% confidence interval 1.38-9.06). The offices were undergoing renovation work around the time of likely exposure and contained straw board that had repeatedly been drilled. The outbreak may have been caused by aerosolization of Coxiella burnetii spore-like forms during drilling into contaminated straw board.","The cohort included 282 employees and subcontractors, of whom 253 (90%) provided blood samples and 214 (76%) completed questionnaires. ",8,3,Case-Control Studies,0
15324550,"An outbreak of Q fever occurred in South Wales, United Kingdom, from July 15 through September 30, 2002. To investigate the outbreak a cohort and nested case-control study of persons who had worked at a cardboard manufacturing plant was conducted. The cohort included 282 employees and subcontractors, of whom 253 (90%) provided blood samples and 214 (76%) completed questionnaires. Ninety-five cases of acute Q fever were identified. The epidemic curve and other data suggested an outbreak source likely occurred August 5-9, 2002. Employees in the factory's offices were at greatest risk for infection (odds ratio 3.46; 95% confidence interval 1.38-9.06). The offices were undergoing renovation work around the time of likely exposure and contained straw board that had repeatedly been drilled. The outbreak may have been caused by aerosolization of Coxiella burnetii spore-like forms during drilling into contaminated straw board.",Ninety-five cases of acute Q fever were identified. ,8,4,Case-Control Studies,0
15324550,"An outbreak of Q fever occurred in South Wales, United Kingdom, from July 15 through September 30, 2002. To investigate the outbreak a cohort and nested case-control study of persons who had worked at a cardboard manufacturing plant was conducted. The cohort included 282 employees and subcontractors, of whom 253 (90%) provided blood samples and 214 (76%) completed questionnaires. Ninety-five cases of acute Q fever were identified. The epidemic curve and other data suggested an outbreak source likely occurred August 5-9, 2002. Employees in the factory's offices were at greatest risk for infection (odds ratio 3.46; 95% confidence interval 1.38-9.06). The offices were undergoing renovation work around the time of likely exposure and contained straw board that had repeatedly been drilled. The outbreak may have been caused by aerosolization of Coxiella burnetii spore-like forms during drilling into contaminated straw board.","The epidemic curve and other data suggested an outbreak source likely occurred August 5-9, 2002. ', ""Employees in the factory's offices were at greatest risk for infection (odds ratio 3.46; 95% confidence interval 1.38-9.06). "", 'The offices were undergoing renovation work around the time of likely exposure and contained straw board that had repeatedly been drilled. ",8,5,Case-Control Studies,3
15324550,"An outbreak of Q fever occurred in South Wales, United Kingdom, from July 15 through September 30, 2002. To investigate the outbreak a cohort and nested case-control study of persons who had worked at a cardboard manufacturing plant was conducted. The cohort included 282 employees and subcontractors, of whom 253 (90%) provided blood samples and 214 (76%) completed questionnaires. Ninety-five cases of acute Q fever were identified. The epidemic curve and other data suggested an outbreak source likely occurred August 5-9, 2002. Employees in the factory's offices were at greatest risk for infection (odds ratio 3.46; 95% confidence interval 1.38-9.06). The offices were undergoing renovation work around the time of likely exposure and contained straw board that had repeatedly been drilled. The outbreak may have been caused by aerosolization of Coxiella burnetii spore-like forms during drilling into contaminated straw board.",The outbreak may have been caused by aerosolization of Coxiella burnetii spore-like forms during drilling into contaminated straw board.,8,6,Case-Control Studies,2
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.",Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. ,9,1,Case-Control Studies,1
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.",Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. ,9,2,Case-Control Studies,1
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.","To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. ",9,3,Case-Control Studies,0
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.",Angiogenin levels were determined by radioimmunoassay and ELISA. ,9,4,Case-Control Studies,0
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.","Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. ",9,5,Case-Control Studies,3
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.","In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). ",9,6,Case-Control Studies,3
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.","Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. ",9,7,Case-Control Studies,3
12653852,"Angiogenesis is a key process in the pathogenesis of inflammatory arthritis. Angiogenin is one of the most potent inducers of neovascularization in experimental models in vivo. To look for evidence that angiogenin is involved in inflammatory joint disease, we examined plasma and synovial fluid (SF) samples from rheumatology patients and synovial fibroblast cell culture supernatants. Angiogenin levels were determined by radioimmunoassay and ELISA. Plasma angiogenin concentrations ranged from 96 to 478 ng/ml, with no significant difference between patients and normal controls. In SF, angiogenin concentrations were significantly higher in patients with acute or chronic synovitis (rheumatoid arthritis (RA): median, 104 ng/ml; range 13-748, n = 14; crystal-induced arthritis (CIA): median, 149 ng/ml; range, 37-616, n = 14, and other chronic inflammatory arthritis: median, 42 ng/ml; range, 15-205; n = 9) than in the 18 patients with osteoarthritis (OA) (median, 20 ng/ml; range 8-116) (P < 0.0001, anova). Angiogenin levels in SF from RA patients in remission with secondary OA were similar to those achieved in primary OA, and decreased in parallel with the resolution of acute gout. Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. These data suggest that angiogenin may mediate local inflammation in arthritis via effects on angiogenesis and leucocyte regulation.","Angiogenin protein was released by cultured synovial fibroblasts from OA and RA patients, and reached 1.18 ng/106 cells/day. ",9,8,Case-Control Studies,0
30833603,"It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI = 0.25, 0.18-0.36). Carrying G allele (AG + GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI = 0.71, 0.55-0.91). Those carrying the G allele (AG + GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI = 0.32, 0.22-0.46, P for interaction = 0.018), particularly for rectal cancer (OR, 95% CI = 0.24, 0.15-0.39, P for interaction = 0.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. In conclusion, colorectal cancer risk was related to an interactive effect between dietary lutein/zeaxanthin intake and the DICER1 rs3742330 polymorphism.",It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. ,8,1,Case-Control Studies,3
30833603,"It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI = 0.25, 0.18-0.36). Carrying G allele (AG + GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI = 0.71, 0.55-0.91). Those carrying the G allele (AG + GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI = 0.32, 0.22-0.46, P for interaction = 0.018), particularly for rectal cancer (OR, 95% CI = 0.24, 0.15-0.39, P for interaction = 0.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. In conclusion, colorectal cancer risk was related to an interactive effect between dietary lutein/zeaxanthin intake and the DICER1 rs3742330 polymorphism.",We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. ,8,2,Case-Control Studies,0
30833603,"It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI = 0.25, 0.18-0.36). Carrying G allele (AG + GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI = 0.71, 0.55-0.91). Those carrying the G allele (AG + GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI = 0.32, 0.22-0.46, P for interaction = 0.018), particularly for rectal cancer (OR, 95% CI = 0.24, 0.15-0.39, P for interaction = 0.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. In conclusion, colorectal cancer risk was related to an interactive effect between dietary lutein/zeaxanthin intake and the DICER1 rs3742330 polymorphism.","In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. ",8,3,Case-Control Studies,0
30833603,"It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI = 0.25, 0.18-0.36). Carrying G allele (AG + GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI = 0.71, 0.55-0.91). Those carrying the G allele (AG + GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI = 0.32, 0.22-0.46, P for interaction = 0.018), particularly for rectal cancer (OR, 95% CI = 0.24, 0.15-0.39, P for interaction = 0.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. In conclusion, colorectal cancer risk was related to an interactive effect between dietary lutein/zeaxanthin intake and the DICER1 rs3742330 polymorphism.",Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. ,8,4,Case-Control Studies,0
30833603,"It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI = 0.25, 0.18-0.36). Carrying G allele (AG + GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI = 0.71, 0.55-0.91). Those carrying the G allele (AG + GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI = 0.32, 0.22-0.46, P for interaction = 0.018), particularly for rectal cancer (OR, 95% CI = 0.24, 0.15-0.39, P for interaction = 0.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. In conclusion, colorectal cancer risk was related to an interactive effect between dietary lutein/zeaxanthin intake and the DICER1 rs3742330 polymorphism.","The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI\u2009=\u20090.25, 0.18-0.36). ",8,5,Case-Control Studies,3
30833603,"It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI = 0.25, 0.18-0.36). Carrying G allele (AG + GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI = 0.71, 0.55-0.91). Those carrying the G allele (AG + GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI = 0.32, 0.22-0.46, P for interaction = 0.018), particularly for rectal cancer (OR, 95% CI = 0.24, 0.15-0.39, P for interaction = 0.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. In conclusion, colorectal cancer risk was related to an interactive effect between dietary lutein/zeaxanthin intake and the DICER1 rs3742330 polymorphism.","Carrying G allele (AG\u2009+\u2009GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI\u2009=\u20090.71, 0.55-0.91). ",8,6,Case-Control Studies,3
30833603,"It is unclear whether dietary lutein/zeaxanthin intake in colorectal cancer is associated with microRNA processing involved in DICER1 cleavage for messenger RNA translation. We investigated whether dietary lutein/zeaxanthin intake affects colorectal cancer risk in patients with a DICER1 rs3742330 polymorphism. In this hospital-based case-control study, we recruited 923 colorectal cancer patients and 1,846 controls based on eligibility criteria, a semiquantitative food frequency questionnaire and the DICER1 rs3742330 genotype. Odds ratios (ORs) and 95% confidence intervals (CIs) were calculated by unconditional logistic regression adjusted for confounders. The highest quartile of lutein/zeaxanthin consumption was inversely associated with a reduced colorectal cancer risk (OR, 95% CI = 0.25, 0.18-0.36). Carrying G allele (AG + GG) showed a significantly reduced colorectal cancer incidence compared with that of AA carriers (OR, 95% CI = 0.71, 0.55-0.91). Those carrying the G allele (AG + GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI = 0.32, 0.22-0.46, P for interaction = 0.018), particularly for rectal cancer (OR, 95% CI = 0.24, 0.15-0.39, P for interaction = 0.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. In conclusion, colorectal cancer risk was related to an interactive effect between dietary lutein/zeaxanthin intake and the DICER1 rs3742330 polymorphism.","Those carrying the G allele (AG\u2009+\u2009GG) along with high lutein/zeaxanthin consumption were markedly associated with a decreased colorectal cancer risk (OR, 95% CI\u2009=\u20090.32, 0.22-0.46, P for interaction\u2009=\u20090.018), particularly for rectal cancer (OR, 95% CI\u2009=\u20090.24, 0.15-0.39, P for interaction\u2009=\u20090.004), compared with that of AA carriers with low lutein/zeaxanthin intakes. ",8,7,Case-Control Studies,3
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.",Early life experience can significantly determine later mental health status and cognitive function. ,8,1,Case-Control Studies,3
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.","Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. ",8,2,Case-Control Studies,3
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.","Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. ",8,3,Case-Control Studies,0
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.","Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. ",8,4,Case-Control Studies,0
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.","Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. ",8,5,Case-Control Studies,1
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.","Surprisingly, cortical size in LMC mice was also affected. ",8,6,Case-Control Studies,1
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.",These observations were compared to the effects of environmental enrichment in the same mouse strain. ,8,7,Case-Control Studies,0
22488100,"Early life experience can significantly determine later mental health status and cognitive function. Neonatal stress, in particular, has been linked to the etiology of mental health disorders as divergent as mood disorder, schizophrenia, and autism. Our study uses a Balb/CByJ mouse model to test the hypothesis, that neonatal stress will alter development and subsequent environmental modulation of neocortex. Using a split litter design, we generated stressed mice (STR) and within litter controls (LMC) along with age-matched, untreated animals (AMC), to serve as across litter controls. Short, daily exposure to a psychosocial/physical stressor, during the first week of life, resulted by adulthood in significant changes in neocortical thickness and architecture, which were further modulated by exposure to behavioral testing. Surprisingly, cortical size in LMC mice was also affected. These observations were compared to the effects of environmental enrichment in the same mouse strain. Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.","Our data indicate that LMC and STR males share with environmentally enriched males, an increase in thickness in infra-granular cortical layers, while STR also display a stress selective decrease in supragranular layers, in response to behavioral training as adults.",8,8,Case-Control Studies,1
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Background. ,14,1,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. ,14,2,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.","Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. ",14,3,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Material and Methods. ,14,4,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. ,14,5,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. ,14,6,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Plasma cytokines were detected. ,14,7,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Results. ,14,8,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. ,14,9,Case-Control Studies,3
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. ,14,10,Case-Control Studies,3
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.","Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. ",14,11,Case-Control Studies,3
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",Conclusions. ,14,12,Case-Control Studies,0
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.","Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. ",14,13,Case-Control Studies,1
26266085,"Background. Cardiorenal Syndrome Type 5 (CRS Type 5) reflects concomitant cardiac and renal dysfunctions in the setting of a wide spectrum of systemic disorders. Our aim was to study in vitro effects of CRS Type 5 plasma on renal tubular cells (RTCs), in terms of cellular death and the characterization of inflammatory plasma profile in these patients. Material and Methods. We enrolled 11 CRS Type 5 patients from ICU and 16 healthy controls. Plasma from patients and controls was incubated with renal tubular cells (RTCs) and cell death was evaluated. Plasma cytokines were detected. Results. RTCs incubated with CRS Type 5 plasma showed significantly higher apoptosis and necrosis with respect to controls. Plasma cytokine profile of CRS Type 5 patients was significantly different from controls: we observed the production of pro- and anti-inflammatory mediators in these patients. Caspase-3, caspase-8, and caspase-9 were activated in cells treated with CRS Type 5 plasma compared to controls. Conclusions. Our results underline the cytotoxic effect of CRS Type 5 mediators on RTC viability, probably due to the activation of both intrinsic and extrinsic pathways of apoptosis and to the deregulation of cytokine release. The consequence may be the damage of distant organs which lead to the worsening of condition of patients.",The consequence may be the damage of distant organs which lead to the worsening of condition of patients.,14,14,Case-Control Studies,1
26890183,"Although HCV is more prevalent among people with severe mental illness (SMI; e.g., schizophrenia, bipolar disorder) than in the general population (17% vs 1%), no large previous studies have examined HCV screening in this population. In this cross-sectional study, we examined administrative data for 57 170 California Medicaid enrollees with SMI to identify prevalence and predictors of HCV screening from October 2010 through September 2011. Only 4.7% (2674 of 57 170) received HCV screening, with strongest predictors being nonpsychiatric health care utilization and comorbid substance abuse.","Although HCV is more prevalent among people with severe mental illness (SMI; e.g., schizophrenia, bipolar disorder) than in the general population (17% vs 1%), no large previous studies have examined HCV screening in this population. ",3,1,Cross-Sectional Studies,3
26890183,"Although HCV is more prevalent among people with severe mental illness (SMI; e.g., schizophrenia, bipolar disorder) than in the general population (17% vs 1%), no large previous studies have examined HCV screening in this population. In this cross-sectional study, we examined administrative data for 57 170 California Medicaid enrollees with SMI to identify prevalence and predictors of HCV screening from October 2010 through September 2011. Only 4.7% (2674 of 57 170) received HCV screening, with strongest predictors being nonpsychiatric health care utilization and comorbid substance abuse.","In this cross-sectional study, we examined administrative data for 57\u2009170 California Medicaid enrollees with SMI to identify prevalence and predictors of HCV screening from October 2010 through September 2011. ",3,2,Cross-Sectional Studies,0
26890183,"Although HCV is more prevalent among people with severe mental illness (SMI; e.g., schizophrenia, bipolar disorder) than in the general population (17% vs 1%), no large previous studies have examined HCV screening in this population. In this cross-sectional study, we examined administrative data for 57 170 California Medicaid enrollees with SMI to identify prevalence and predictors of HCV screening from October 2010 through September 2011. Only 4.7% (2674 of 57 170) received HCV screening, with strongest predictors being nonpsychiatric health care utilization and comorbid substance abuse.","Only 4.7% (2674 of 57\u2009170) received HCV screening, with strongest predictors being nonpsychiatric health care utilization and comorbid substance abuse.",3,3,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.",This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. ,13,1,Cross-Sectional Studies,0
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.","Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. ",13,2,Cross-Sectional Studies,0
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.",Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. ,13,3,Cross-Sectional Studies,0
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.","A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. ",13,4,Cross-Sectional Studies,0
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.",Younger students were more likely to have atopic dermatitis. ,13,5,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.","Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. ",13,6,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.","In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR))\u200a=\u200a1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. ', ""Similar trend was shown among both boys and girls for their father's education level. "", 'The stress level was found to be significantly associated with the risk of atopic dermatitis. ",13,7,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.",Similar trend was shown among both boys and girls for their father's education level. ,13,8,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.",The stress level was found to be significantly associated with the risk of atopic dermatitis. ,13,9,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.","Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR\u200a=\u200a1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR\u200a=\u200a1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR\u200a=\u200a1.21, 95% CI, 1.00-1.45; P\u200a=\u200a0.05) with ""moderate"" stress. ",13,10,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.","In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls.",13,11,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.",This study suggests that stress and parents' education level were associated with atopic dermatitis. ,13,12,Cross-Sectional Studies,3
23940513,"This study investigated the relationship between level of stress in middle and high school students aged 12-18 and risk of atopic dermatitis. Data from the Sixth Korea Youth Risk Behavior Web-based Survey (KYRBWS-VI), a cross-sectional study among 74,980 students in 800 middle schools and high schools with a response rate of 97.7%, were analyzed. Ordinal logistic regression analyses were conducted to determine the relationship between stress and atopic dermatitis with severity. A total of 5,550 boys and 6,964 girls reported having been diagnosed with atopic dermatitis. Younger students were more likely to have atopic dermatitis. Interestingly, the educational level of parents was found to be associated with having atopic dermatitis and having more severe condition. In particular, girls with mothers with at least college education had a 41% higher risk of having atopic dermatitis and severe atopic condition (odds ratio (OR)) = 1.41, 95% CI, 1.22-1.63; P<0.0001) compared with those with mothers who had attended middle school at most. Similar trend was shown among both boys and girls for their father's education level. The stress level was found to be significantly associated with the risk of atopic dermatitis. Compared to boys with who reported ""no stress"", boys with ""very high"" stress had 46% higher the risk of having more severe atopic dermatitis (OR = 1.46, 95% CI, 1.20-1.78; P<0.0001), 44% higher (OR = 1.44, 95% CI, 1.19-1.73; P<0.0001) with ""high"" stress, and 21% higher (OR = 1.21, 95% CI, 1.00-1.45; P = 0.05) with ""moderate"" stress. In contrast, we found no statistically significant relationship between stress and atopic dermatitis in girls. This study suggests that stress and parents' education level were associated with atopic dermatitis. Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.","Specifically, degree of stress is positively correlated with likelihood of being diagnosed with this condition and increasing the severity.",13,13,Cross-Sectional Studies,3
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.",Balance problems are common after a traumatic brain injury (TBI). ,10,1,Cross-Sectional Studies,3
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.","Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. ",10,2,Cross-Sectional Studies,0
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.",These problems can be assessed with the Sensory Organization Test (SOT).,10,3,Cross-Sectional Studies,0
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.","However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability.",10,4,Cross-Sectional Studies,0
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.",The Limits of Stability (LOS) balance test has received little attention in TBI. ,10,5,Cross-Sectional Studies,0
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.","In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. ",10,6,Cross-Sectional Studies,0
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.","Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. ",10,7,Cross-Sectional Studies,0
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.","Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. ",10,8,Cross-Sectional Studies,0
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.","Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. ",10,9,Cross-Sectional Studies,3
30909842,"Balance problems are common after a traumatic brain injury (TBI). Symptoms of dizziness, unsteadiness, or imbalance have been most frequently attributed to sensory organization problems involving the use of visual, proprioceptive, and/or vestibular information for postural control. These problems can be assessed with the Sensory Organization Test (SOT). However, as head trauma can affect any brain region, areas responsible for voluntary control of movements involved in dynamic balance tasks, such as the motor cortex and its projections, could also be compromised, which would likely affect one's limits of stability. The Limits of Stability (LOS) balance test has received little attention in TBI. In the present study, we compared the prevalence of SOT versus LOS abnormalities in a cohort of 48 patients, the majority classified as having mild or moderate chronic TBI. Compared with a normative database provided by the balance testing manufacturer, a larger portion of our cohort presented abnormalities in the LOS test. Dizziness Handicap Inventory (DHI) results indicated mild disability, with the five activities most frequently endorsed as problematic being: looking up, performing quick head movements, performing ambitious such as sports or dancing activities, feeling frustrated, and performing strenuous house/yard work. Although regression analysis revealed that both tests significantly predicted subjective scores on the DHI, more LOS than SOT testing variables were important predictors of DHI results indicating disability. These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.",These results suggest that the LOS test is an informative tool that should be included in any objective balance evaluations that screen TBI patients with balance complaints.,10,10,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.","Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,\nand is ranked as the second-leading cause of death in women after lung cancer. ",12,1,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.","Early diagnosis of breast cancer is\npossible, and breast cancer is included in cancer-screening programs in Turkey. ",12,2,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.","The aim of this study was to evaluate\nthe knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and\nbreast cancer screening methods, and to determine the effect of sociodemographic characteristics. ",12,3,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.",Methods: A total\nof 489 patients were included in the study. ,12,4,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.",The mean age of participants was 36.53 ± 11.22 years. ,12,5,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.",346 (70.8%) of the\nparticipants were married. ,12,6,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.","The study was a cross-sectional, non-randomized study in public health. ",12,7,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.","The participants in\nthis study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and\nSisli Hamidiye Etfal Family Medicine policlinic. ",12,8,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.",Results: The rate and duration of breastfeeding were higher among\nparticipants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as\nthe level of education increased (p<0.001). ,12,9,Cross-Sectional Studies,3
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.",Having a family history of a high incidence of breast cancer significantly\nincreased the rate of breast self examination practice by 1.93 fold (p=0.016). ,12,10,Cross-Sectional Studies,3
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.",Conclusion: Breast Cancer is a disease\nthat can be treated 100 % with early diagnosis. ,12,11,Cross-Sectional Studies,0
30486608,"Background: Breast cancer is the most frequently diagnosed cancer among women in Turkey and worldwide,
and is ranked as the second-leading cause of death in women after lung cancer. Early diagnosis of breast cancer is
possible, and breast cancer is included in cancer-screening programs in Turkey. The aim of this study was to evaluate
the knowledge, attitude, and behaviour of young women (older than 20 years of age) on breast self-examination and
breast cancer screening methods, and to determine the effect of sociodemographic characteristics. Methods: A total
of 489 patients were included in the study. The mean age of participants was 36.53 ± 11.22 years. 346 (70.8%) of the
participants were married. The study was a cross-sectional, non-randomized study in public health. The participants in
this study were women over 20 years old, who presented to Arnavutkoy State Hospital General surgery policlinic and
Sisli Hamidiye Etfal Family Medicine policlinic. Results: The rate and duration of breastfeeding were higher among
participants living in rural areas than those living in urban areas (p<0.001) The awareness of breast cancer increased as
the level of education increased (p<0.001). Having a family history of a high incidence of breast cancer significantly
increased the rate of breast self examination practice by 1.93 fold (p=0.016). Conclusion: Breast Cancer is a disease
that can be treated 100 % with early diagnosis. Primary care physicians especially works in lower socioeconomic
conditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination
and other screening tools.","Primary care physicians especially works in lower socioeconomic\nconditions have to tell the importance of early diagnosis of breast cancer, and properly explain breast self examination\nand other screening tools.",12,12,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.",Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12\xa0years-old individuals in the 1991. ,10,1,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.",The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. ,10,2,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.","However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10\xa0mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services.",10,3,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.","This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination.",10,4,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.","During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. ",10,5,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.","Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10\xa0mIU/mL), were collected and analyzed. ",10,6,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.","Most of the HCSs (74.6%) included in the survey, mean age 24.8\xa0y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11\xa0months). ",10,7,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.","Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. ",10,8,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.","Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20\xa0y following the primary immunization, in HCSs who are exposed to occupational health risk. ",10,9,Cross-Sectional Studies,0
27925503,"Vaccination against Hepatitis B Virus (HBV) became mandatory in Italy for all newborns and 12 years-old individuals in the 1991. The immunogenicity of HBV vaccine and the effectiveness of the universal immunization strategy have been widely demonstrated. However the need to assess the antibody concentrations above the well known serological correlate of protection for HBV infection (≥10 mIU/mL), established in individuals immunized with a 3 doses vaccination course, is still recommended in subjects exposed to occupational risks in different settings, particularly the healthcare services. This practice has to be performed during the preventive medical examination, before the worker's exposure to biological hazards, as a fundamental part of Occupational Health Surveillance Programs in several Countries, including Italy: the goal is to assure individual protection, also providing booster doses when needed, after many years following the primary vaccination. During the 2011-2013 period, an observational study was performed in Healthcare students (HCSs) trained at a regional university acute-care hospital in North-Western Italy, properly immunized against HBV during infancy or adolescence, in order to evaluate the persistence of seroprotection and to assess the anamnestic response to booster vaccination. Data from 717 subjects undergoing HbsAg Ab and HBc Ab testing during the preventive medical examination, and receiving a booster dose of HBV vaccine when resulting with a non-protective titer (<10 mIU/mL), were collected and analyzed. Most of the HCSs (74.6%) included in the survey, mean age 24.8 y ( ± 4.6 SD), had received the primary vaccination course during the first year of life (3-5-11 months). Globally, 507 (70.7%) HCSs showed protective antibody titres, and an anamnestic response was observed in more than 95% subjects receiving the booster dose. Our study demonstrated the long-term persistence of protection of HBV vaccine, more than 20 y following the primary immunization, in HCSs who are exposed to occupational health risk. The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.",The anamnestic response observed in non-seroprotected subjects who received the booster further confirms the capability of the HBV vaccine to create a strong immunological memory.,10,10,Cross-Sectional Studies,1
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.",A healthy diet prevents type 2 diabetes but is often difficult to adhere to. ,10,1,Cross-Sectional Studies,1
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.",This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. ,10,2,Cross-Sectional Studies,0
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.","Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. ",10,3,Cross-Sectional Studies,3
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.","In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. ",10,4,Cross-Sectional Studies,0
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.","The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). ",10,5,Cross-Sectional Studies,0
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.","In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. ",10,6,Cross-Sectional Studies,0
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.","Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). ",10,7,Cross-Sectional Studies,3
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.","Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). ",10,8,Cross-Sectional Studies,3
31905938,"A healthy diet prevents type 2 diabetes but is often difficult to adhere to. This cross-sectional study aimed to investigate whether eating competence is associated with diet or risk factors and prevalence of type 2 diabetes in individuals screened for type 2 diabetes risk. Eating competence is an indicator of food acceptance, positive attitudes, internal regulation and contextual skills related to food and eating. In total, 3147 Finnish adults aged 18-74 at an increased risk for type 2 diabetes identified via online risk screening participated in the baseline examinations of the Stop Diabetes (StopDia) study. The participants filled out the digital questionnaire on food intake, physical activity and sleep, and the Satter Eating Competence Inventory 2.0TM (ecSI 2.0TM). In addition, anthropometric and laboratory measurements were performed at primary healthcare centres. Eating competent individuals (37%, classified by ecSI 2.0TM) had a better quality of diet (p < 0.05 for all). Additionally, eating competence was associated with a lower prevalence of previously undiagnosed type 2 diabetes, abdominal obesity, metabolic syndrome and hypertriglyceridaemia, and with better insulin sensitivity (p < 0.05 for all). However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. Eating competence is associated with a healthy diet and could, thus, in the long term, support the prevention of type 2 diabetes.","However, these associations, except for metabolic syndrome, were at least partly mediated by body mass index. ",10,9,Cross-Sectional Studies,2
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.",The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. ,10,1,Cross-Sectional Studies,0
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.","In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. ",10,2,Cross-Sectional Studies,1
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.",Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. ,10,3,Cross-Sectional Studies,0
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.",The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). ,10,4,Cross-Sectional Studies,3
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.","Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). ",10,5,Cross-Sectional Studies,3
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.",No individuals were homozygous for SLC4A1Delta27.,10,6,Cross-Sectional Studies,0
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.","In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040).",10,7,Cross-Sectional Studies,1
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.","While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. ",10,8,Cross-Sectional Studies,0
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.",Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. ,10,9,Cross-Sectional Studies,3
14695625,"The geographic overlap between the prevalence of erythrocyte polymorphisms and malaria endemicity is thought to be an example of natural selection on human populations. In Papua New Guinea (PNG), the Gerbich-negative phenotype is caused by an exon 3 deletion in the glycophorin C gene (GYPCDeltaex3) while heterozygosity for a 27-base pair deletion in the SLC4A1 gene (anion exchanger 1 or erythrocyte membrane protein, band 3), SLC4A1Delta27, results in Southeast Asian ovalocytosis. Two geographically and ethnically distinct malaria endemic regions of PNG (the Wosera [East Sepik Province] and Liksul [Madang Province]) were studied to illustrate the distribution of two prominent deletion polymorphisms (GYPCDeltaex3 and SLC4A1Delta27) and to determine if the genetic load associated with SLC4A1Delta27 would constrain independent assortment of GYPCDeltaex3 heterozygous and homozygous genotypes. The frequency of the GYPCDeltaex3 allele was higher in the Wosera (0.463) than Liksul (0.176) (chi(2); P < 0.0001). Conversely, the frequency of the SLC4A1Delta27 allele was higher in Liksul (0.0740) than the Wosera (0.0005) (chi(2); P < 0.0001). No individuals were homozygous for SLC4A1Delta27. In 355 Liksul residents, independent assortment of these two deletion polymorphisms resulted in 14 SLC4A1Delta27 carriers heterozygous for GYPCDeltaex3 and one SLC4A1Delta27 carrier homozygous for GYPCDeltaex3 (Fisher's exact test; P = 0.8040). While homozygosity for SLC4A1Delta27 appears to be nonviable, the GYPCDeltaex3 allele is not lethal when combined with SLC4A1Delta27. Neither mutation was associated with altered susceptibility to asymptomatic Plasmodium falciparum or P. vivax infection. While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.","While these erythrocyte polymorphisms apparently have no effect on blood-stage malaria infection, their contribution to susceptibility to clinical malaria morbidity requires further study.",10,10,Cross-Sectional Studies,1
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people.,10,1,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics.,10,2,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. ,10,3,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",This cross-sectional study was conducted in a French public university hospital. ,10,4,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.","The inclusion criteria were 18\u2009years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. ",10,5,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). ,10,6,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. ,10,7,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",The final sample had a median age of 31\u2009years and included 54 (50%) Female-to-Male individuals. ,10,8,Cross-Sectional Studies,0
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.","In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). ",10,9,Cross-Sectional Studies,3
27068099,"The assessment of co-existing psychological and psychiatric disorders is advocated in the Standards of Care for the health of transsexual people. This study aimed to determine the psychopathological characteristics of transsexuals based on a large sample of French individuals and to identify whether these characteristics differed according to the individual's sociodemographic or clinical characteristics. The aim of this study was to determine the psychopathological characteristics of transsexuals from a large sample of French individuals and whether these differed by sociodemographic or clinical characteristics. This cross-sectional study was conducted in a French public university hospital. The inclusion criteria were 18 years or older, diagnosis of gender dysphoria, and eligibility for a standardized sex reassignment procedure. Personality characteristics were assessed using the Minnesota Multiphasic Personality Inventory 2 (MMPI-2). A total of 108 individuals provided a valid MMPI-2 between January 2007 and December 2010. The final sample had a median age of 31 years and included 54 (50%) Female-to-Male individuals. In multivariate models, hormonal therapy status was significantly related to the scales of MMPI-2 (Psychasthenia and Masculinity/Femininity). Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.",Personality assessment can help a multidisciplinary gender dysphoria team detect potential psychopathological factors of vulnerability.,10,10,Cross-Sectional Studies,0
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.","To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters.\xa0",8,1,Cross-Sectional Studies,0
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.","Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. ",8,2,Cross-Sectional Studies,0
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.",The dental arch\xa0dimensions were measured. ,8,3,Cross-Sectional Studies,0
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.","The measured parameters were arch length, arch width, Bolton's ratio, and arch form.",8,4,Cross-Sectional Studies,0
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.","The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016.\xa0",8,5,Cross-Sectional Studies,0
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.","Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than\xa00.05). ",8,6,Cross-Sectional Studies,3
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.","The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. ",8,7,Cross-Sectional Studies,0
29332114,"To determine the dental arch dimensions and arch forms in a sample of Saudi orthodontic patients, to investigate the prevalence of Bolton anterior and overall tooth size discrepancies, and to compare the effect of gender on the measured parameters. Methods: This study is a biometric analysis of dental casts of 149 young adults recruited from different orthodontic centers in Jeddah, Saudi Arabia. The dental arch dimensions were measured. The measured parameters were arch length, arch width, Bolton's ratio, and arch form. The data were analyzed using IBM SPSS software version 22.0 (IBM Corporation, New York, USA); this cross-sectional study was conducted between April 2015 and May 2016. Results: Dental arch measurements, including inter-canine and inter-molar distance, were found to be significantly greater in males than females (p less than 0.05). The most prevalent dental arch forms were narrow tapered (50.3%) and narrow ovoid (34.2%), respectively. The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. There was no significant difference between males and females regarding Bolton's ratio. Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.","The prevalence of tooth size discrepancy in all cases was 43.6% for anterior ratio and 24.8% for overall ratio. ', ""The mean Bolton's anterior ratio in all malocclusion classes was 79.81%, whereas the mean Bolton's overall ratio was 92.21%. "", ""There was no significant difference between males and females regarding Bolton's ratio.\xa0"", 'Conclusion: The most prevalent arch form was narrow tapered, followed by narrow ovoid. ', ""Males generally had larger dental arch measurements than females, and the prevalence of tooth size discrepancy was more in Bolton's anterior teeth ratio than in overall ratio.""]",8,8,Cross-Sectional Studies,3
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.",Hypertension is a common global health problem including China. ,14,1,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.",14,2,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.",A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. ,14,3,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.",Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. ,14,4,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. ",14,5,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.",Awareness of hypertension was based on self-report. ,14,6,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.",An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.,14,7,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). ",14,8,Cross-Sectional Studies,3
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","Among individuals with hypertension, 55.5% were aware of their condition. ",14,9,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. ",14,10,Cross-Sectional Studies,3
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. ",14,11,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.",The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).,14,12,Cross-Sectional Studies,0
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. ",14,13,Cross-Sectional Studies,3
31490374,"Hypertension is a common global health problem including China. This study aimed to assess the prevalence and awareness of hypertension, and evaluate risk factors associated with hypertension among multi-ethnic population in northwest China using a random sampling cross-sectional data.A cross-sectional survey was conducted between 2014 and 2015 as part of a nationwide survey using stratified four-stage random sampling in Xinjiang. Hypertension was defined as mean systolic blood pressure (SBP) and/or diastolic blood pressure (DBP) ≥140/90 mm Hg and/or taking anti-hypertensive medication. In addition, the prevalence of hypertension (SBP ≥ 130 or DBP ≥ 80 mm Hg) was also estimated according to the 2017 American College of Cardiology (ACC)/American Heart Association (AHA) High Blood Pressure Guideline. Awareness of hypertension was based on self-report. An optimized risk score model was used to assess the risk and determine the predictive power of risk factors on hypertension.Totally 6722 subjects aged ≥18 years were enrolled and prevalence of hypertension was 24.3%, while the prevalence of hypertension based on the 2017 ACC/AHA guideline was approximately twice as high as that based on 2010 Chinese guideline (37.6%). Among individuals with hypertension, 55.5% were aware of their condition. Six potential factors were estimated to be associated with increased risk of hypertension including age, ethnicity, marital status, body mass index (BMI), waistline circumference, and comorbidity. In the analyses of calculated risk score, BMI ≥ 28.0 corresponded to the highest risk score of 23 points. The area under the receiver operation curve for the multivariable prediction model was 0.803 (95%CI: 0.789-0.813).There is a considerable prevalence of hypertension among Xinjiang adults, northwest China; awareness of hypertension is low. Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.","Excess weight loss may be a vital strategy for controlling hypertension, particularly if accompanied with other preventive measures in this region.",14,14,Cross-Sectional Studies,2
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.",The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. ,11,1,Cross-Sectional Studies,0
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.",The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. ,11,2,Cross-Sectional Studies,0
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain.",11,3,Cross-Sectional Studies,0
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. ",11,4,Cross-Sectional Studies,0
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. ",11,5,Cross-Sectional Studies,0
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. ",11,6,Cross-Sectional Studies,0
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. ",11,7,Cross-Sectional Studies,3
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. ",11,9,Cross-Sectional Studies,3
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. ",11,10,Cross-Sectional Studies,1
21516338,"The present study explored in a sample of Flemish pain patients the role of prayer as a possible individual factor in pain management. The focus on prayer as a personal religious factor fits with the current religious landscape in Western-Europe where personal religious factors are more important than organizational dimensions of religion. Our study is framed in the transactional theory of stress and coping by testing first, whether prayer was related with pain severity and pain tolerance and second, whether cognitive positive re-appraisal was a mediating mechanism in the association between prayer and pain. We expected that prayer would be related to pain tolerance in reducing the impact of the pain on patient's daily life, but not necessarily to pain severity. A cross-sectional questionnaire design was adopted in order to measure demographics, prayer, pain outcomes (i.e., pain severity and pain tolerance), and cognitive positive re-appraisal. Two hundred and two chronic pain (CP) patients, all members of a Flemish national patients association, completed the questionnaires. Correlational analyses showed that prayer was significantly related with pain tolerance, but not with pain severity. However, ancillary analyses revealed a moderational effect of religious affiliation in the relationship between prayer and pain severity as well as pain tolerance. Furthermore, mediation analysis revealed that cognitive positive re-appraisal was indeed an underlying mechanism in the relationship between prayer and pain tolerance. This study affirms the importance to distinguish between pain severity and pain tolerance, and indicates that prayer can play a role in pain management, especially for religious pain patients. Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.","Further, the findings can be framed within the transactional theory of stress and coping as the results indicate that positive re-appraisal might be an important underlying mechanism in the association between prayer and pain.",11,11,Cross-Sectional Studies,3
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.",Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. ,12,1,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. ",12,2,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","TB was monitored for 2 years in wild field voles in Kielder Forest, UK. ",12,3,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. ",12,4,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.",A prevalence of 5.2% of individuals with lesions was recorded (n=2791). ,12,5,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. ",12,6,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). ",12,7,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","TB prevalence varied, with between 0% and 50% of voles infected per site. ",12,8,Cross-Sectional Studies,0
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. ",12,10,Cross-Sectional Studies,3
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. ",12,11,Cross-Sectional Studies,3
18005472,"Vole tuberculosis (TB; Mycobacterium microti) is an understudied endemic infection. Despite progressing slowly, it causes severe clinical pathology and overt symptoms in its rodent host. TB was monitored for 2 years in wild field voles in Kielder Forest, UK. The prevalence of characteristic cutaneous TB lesions was monitored longitudinally at 4 sites, with individuals live-trapped and repeatedly monitored. A prevalence of 5.2% of individuals with lesions was recorded (n=2791). In a cross-sectional study, 27 sites were monitored bi-annually, with TB assessed by post-mortem examination for macroscopic lesions, and by culture and histopathology. Seventy-nine voles (10.78%; n=733) were positive for mycobacteria, with the highest prevalence in spring (13.15%; n=327). TB prevalence varied, with between 0% and 50% of voles infected per site. Prevalence increased with age (mass), and apparent seasonality was due to a higher proportion of older animals in spring. Survival analysis supported this result, with cutaneous lesions only manifesting in the advanced stages of infection, and therefore only being found on older voles. The body condition of individuals with lesions declined at the time when the lesion was first recorded, when compared to individuals without lesions, suggesting there may be an acute phase of infection during its advanced stage. Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.","Although predicted survival following the appearance of a cutaneous lesion was lower than for uninfected individuals, this was not significant.",12,12,Cross-Sectional Studies,3
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",Truck drivers are at increased risk of diet- and physical activity-related chronic diseases.,14,1,Cross-Sectional Studies,3
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.","Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors.",14,2,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. ,14,3,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.","The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. ",14,4,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.","The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. ",14,5,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",Over 85% of survey respondents worked more than 9 hrs per day. ,14,6,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",Nearly 75% acknowledged the need to make changes to improve their health. ,14,7,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. ,14,8,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.","Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. ",14,9,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. ,14,10,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.","Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. ",14,11,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",This is almost double the proportion found in the general population. ,14,12,Cross-Sectional Studies,0
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.","These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. ",14,13,Cross-Sectional Studies,1
31325297,"Truck drivers are at increased risk of diet- and physical activity-related chronic diseases. Despite this, there is a paucity of data about Australian truck drivers' diet and physical activity behaviors. A multiple choice and short response survey was distributed to truck drivers attending an Australian Truck Show. The survey asked about self-reported health, source of health information, number of serves of fruit, vegetables, unhealthy food, and sugary drink consumed per day, and frequency of moderate- and vigorous-intensity physical activity per week. The survey was completed by 231 truck drivers---almost all were male, with a mean age of 46 (range 20 to 71) years. Over 85% of survey respondents worked more than 9 hrs per day. Nearly 75% acknowledged the need to make changes to improve their health. Half consumed fewer serves of fruit and 88% consumed fewer serves of vegetables than national recommendations. Over 63% consumed at least one serve of unhealthy foods per day, and 65% drank at least one can of sugary drink per day. Most (80%) undertook less than moderate- and vigorous-intensity physical activity levels provided in national recommendations. Of concern, almost 90% of drivers had above the recommended body mass index---approximately 60% were obese. This is almost double the proportion found in the general population. These findings highlight the importance of health promotion to help drivers make better choices about their health behaviors, which are often underpinned by the limitations of their work environment. Health promotion in transport industry workplaces should be an important topic for future research.",Health promotion in transport industry workplaces should be an important topic for future research.,14,14,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.",Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). ,12,1,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.",The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia.,12,2,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.","This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire.",12,3,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.",Participants were evaluated for their self-medication practices via 4 items,12,4,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.",Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. ,12,5,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.","Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. ",12,6,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.","The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. ",12,7,Cross-Sectional Studies,0
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.","Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. ",12,8,Cross-Sectional Studies,3
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.","However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. ",12,9,Cross-Sectional Studies,3
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.",Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. ,12,10,Cross-Sectional Studies,3
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.",The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. ,12,11,Cross-Sectional Studies,3
30071006,"Self-medication with analgesics in dental pain management is a common practice as most of these medicines are available over-the-counter (OTC). The study aims to examine the relationship between beliefs about medicines and self-medication with analgesics in dental pain management in Malaysia. This cross-sectional study was conducted among conveniently sampled patients attending dental clinics, located in Kuala Lumpur, Malaysia to assess association between self-medication with analgesics and patient's beliefs about medicines via Beliefs about Medicines Questionnaire. Participants were evaluated for their self-medication practices via 4 items. Further assessment was done via Quantitative Analgesic Questionnaire (QAQ) regarding the analgesics taken. Statistical analyses were performed using SPSS version 24, with 0.05 as level of significance. The prevalence of self-medication with analgesics was 29.4%, with 95.6% of the participants took analgesics when necessary. Participants practising self-medication for dental pain reported more positive beliefs in General-Necessity (13.04 vs. 9.98, p = 0.001) than those not practising self-medication. However, these participants had weaker beliefs in General-Harm (12.00 vs. 10.29, p = 0.006) and General-Overuse (11.38 vs. 10.31, p = 0.032) than those not practising self-medication. Participants beliefs in General-Harm (r = -0.243; p = 0.003) and General-Overuse (r = -0.203; p = 0.012) were negatively correlated with total QAQ point. The study found that individuals who practised self-medication had stronger beliefs about the benefits of medicines and weaker beliefs in viewing medicines as harmful and overused. Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.",Findings can guide public education to improve the safety aspects of self-medication with analgesics in dental practice.,12,12,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. ,12,1,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.","Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age.",12,2,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i.,12,3,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.","A total of 137 preschool children on O\'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. ",12,4,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.","Once technical issues were resolved, screening was fast and well tolerated. ",12,5,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",Possible vision abnormalities were found in 11 of the 137 children (8%). ,12,6,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. ,12,7,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i.,12,8,Cross-Sectional Studies,2
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. ,12,9,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children.,12,10,Cross-Sectional Studies,1
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value.,12,11,Cross-Sectional Studies,0
26468424,"The goal of early childhood vision screening is to detect subnormal vision and amblyopic risk factors that threaten visual development so that treatment can be initiated early to yield the highest benefit. Hand-held, portable, instrument-based vision screening devices can be used in children as young as 6 months of age. We assessed the feasibility of hand-held photoscreeners to screen for vision disorders in pre-school children in Hawai'i. A total of 137 preschool children on O'ahu in the ""Tutu and Me""/Partners in Development program were screened at 6 different locations using the Plusoptix S12 hand-held photoscreener. Once technical issues were resolved, screening was fast and well tolerated. Possible vision abnormalities were found in 11 of the 137 children (8%). Poor compliance for follow-up with formal vision examination limited our ability to confirm these abnormalities. We conclude that photoscreening has the potential to facilitate early childhood vision screening in Hawai'i. The optimal referral criteria for use in Hawai'i will need to be determined after considering the age of the screening population and the available medical resources in Hawai'i. Early detection of treatable eye disorders has far-reaching benefits for the visual development and long term health and well-being of children. A comprehensive early childhood vision screening program in Hawai'i utilizing automated hand-held photoscreeners may have public health value. Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.",Such a program should integrate referral to an eye care professional for confirmation and management of vision disorders of at-risk children found on screening.,12,12,Cross-Sectional Studies,0
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.",There has been a growing burden of anxiety among Nepalese adolescents. ,8,1,Cross-Sectional Studies,0
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.",Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. ,8,2,Cross-Sectional Studies,0
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.","The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. ",8,3,Cross-Sectional Studies,0
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.","The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. ",8,4,Cross-Sectional Studies,0
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.","Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. ",8,5,Cross-Sectional Studies,3
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.","Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. ",8,6,Cross-Sectional Studies,3
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.","Finally, more social support from close friends was related to lower social anxiety. ",8,7,Cross-Sectional Studies,3
32240242,"There has been a growing burden of anxiety among Nepalese adolescents. Social anxiety in particular is one of the commonly reported symptoms indicating mental health problem among adolescents. The purpose of this study was to assess social anxiety, and identify how social support, emotion regulation and mindfulness uniquely contribute to social anxiety among adolescents in Birgunj, Nepal. The study was conducted by using a self-administered questionnaire among 384 adolescents (65.4% boys; M = 16.05 years, SD = 1.39) studying at secondary schools of Birgunj. Results show that there was a positive correlation between social anxiety symptoms and age, and girls reported more symptoms. Traits such as non-acceptance of emotions, lack of clarity and lack of awareness of emotions were related to increased social anxiety; while acting with awareness, non-reactivity, and better ability to describe emotions was related to decreased social anxiety. Finally, more social support from close friends was related to lower social anxiety. These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.","These results suggest that improving emotion regulation, dispositional mindfulness, and social support may be helpful for adolescents who are at risk of, or are suffering from, social anxiety.",8,8,Cross-Sectional Studies,2
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. ",13,1,Cross-Sectional Studies,0
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. ",13,2,Cross-Sectional Studies,0
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. ",13,3,Cross-Sectional Studies,0
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. ",13,4,Cross-Sectional Studies,0
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. ",13,5,Cross-Sectional Studies,0
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.",Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India).,13,6,Cross-Sectional Studies,0
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. ",13,7,Cross-Sectional Studies,0
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). ",13,8,Cross-Sectional Studies,3
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). ",13,9,Cross-Sectional Studies,3
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). ",13,10,Cross-Sectional Studies,3
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.","Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. ",13,11,Cross-Sectional Studies,3
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.",Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. ,13,12,Cross-Sectional Studies,3
30149686,"Few studies assess built environment correlates of active commuting in low-and-middle-income countries (LMICs), but the different context could yield distinct findings. Policies and investments to promote active commuting remain under-developed in LMICs like India, which grapples with traffic congestion, lack of activity-supportive infrastructure, poor enforcement of traffic rules and regulations, air pollution, and overcrowding. This cross-sectional study investigated associations between home neighborhood environment characteristics and active commuting in Chennai, India. Adults (N = 370, 47.2% female, mean age =37.9 years) were recruited from 155 wards in the metropolitan area of Chennai in southern India between January and June 2015. Participants self-reported their usual mode of commute to work, with responses recoded into three categories: (1) multi-modal or active commuting (walking and bicycling; n = 56); (2) public transit (n = 52); and (3) private transport (n = 111). Environmental attributes around participants' homes were assessed using the Neighborhood Environment Walkability Scale for India (NEWS-India). Associations between environmental characteristics and likelihood of active commuting and public transit use were modeled using logistic regression with private transport (driving alone or carpool) as the reference category, adjusting for age, gender, and household car ownership. Consistent with other international studies, participants living in neighborhoods with a mix of land uses and a transit stop within a 10-minute walk from home were more likely to use active commuting (both p < 0.01). Land-use mix was significantly associated with the use of public transit compared to private transport (adjusted odds ratio (aOR) =5.2, p = 0.002). Contrary to findings in high-income countries, the odds of active commuting were reduced with improved safety from crime (aOR =0.2, p = 0.003), aesthetics (aOR =0.2, p = 0.05), and street connectivity (aOR =0.2, p = 0.003). Different environmental attributes were associated with active commuting, suggesting that these relationships are complex and may distinctly differ from those in high-income countries. Unexpected inverse associations of perceived safety from crime and aesthetics with active commuting emphasize the need for high-quality epidemiologic studies with greater context specificity in the study of physical activity in LMICs. Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.",Findings have public health implications for India and suggest that caution should be taken when translating evidence across countries.,13,13,Cross-Sectional Studies,0
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.",The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. ,10,1,Cross-Sectional Studies,0
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.","Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. ",10,2,Cross-Sectional Studies,0
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.",WHOQOL-HIV-BREF was used to measure QoL. ,10,3,Cross-Sectional Studies,0
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.",Data about other variables of interest were obtained from medical records. ,10,4,Cross-Sectional Studies,0
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.",Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. ,10,5,Cross-Sectional Studies,0
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.","The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR\u2009=\u20091.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200\u2009cells/mm3 (PR\u2009=\u20091.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR\u2009=\u20091.60 (95% CI: 1.09, 2.36)). ",10,6,Cross-Sectional Studies,3
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.","However, all factors examined were associated with at least one QoL domain. ",10,7,Cross-Sectional Studies,3
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.","Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. ",10,8,Cross-Sectional Studies,3
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.",This highlights the importance of educational interventions and ARV treatment in HIV patients. ,10,9,Cross-Sectional Studies,0
27460628,"The purpose of our study was to assess quality of life (QoL) among Georgian HIV-infected individuals and to examine factors associated with QoL. Our cross-sectional study sample consisted of 201 HIV-infected adult outpatients recruited at the National AIDS Center in Tbilisi, Georgia. WHOQOL-HIV-BREF was used to measure QoL. Data about other variables of interest were obtained from medical records. Modified Poisson regression with robust variance estimates was performed to create a predictive model of factors that influenced QoL. The study results showed the following factors as predictors of good general QoL: antiretroviral (ARV) treatment (prevalence ratio (PR)=2.87 (95% CI: 1.45, 5.67)); higher education level (PR = 1.51 (95% CI: 1.05, 2.17)); CD4 cells ≥200 cells/mm3 (PR = 1.83 (95% CI: 1.13, 2.94)); and age ≥40 years (PR = 1.60 (95% CI: 1.09, 2.36)). However, all factors examined were associated with at least one QoL domain. Our study suggests that HIV-infected individuals younger than 40 years and those with lower education level are more likely to have poorer QoL, while those receiving ARV treatment tend to have better QoL. This highlights the importance of educational interventions and ARV treatment in HIV patients. Future research should seek to implement additional evidence-based actions to improve QoL in this population.",Future research should seek to implement additional evidence-based actions to improve QoL in this population.,10,10,Cross-Sectional Studies,0
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.","Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings.  51% of participants were classified as multidimensionally poor (MPI>0.33). ",12,1,Cross-Sectional Studies,1
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.","To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012.",12,2,Cross-Sectional Studies,0
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.","Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI).",12,3,Cross-Sectional Studies,0
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.",51% of participants were classified as multidimensionally poor (MPI>0.33). ,12,4,Cross-Sectional Studies,0
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.",Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). ,12,5,Cross-Sectional Studies,3
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.",There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). ,12,6,Cross-Sectional Studies,3
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.",Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). ,12,7,Cross-Sectional Studies,0
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.","Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). ",12,8,Cross-Sectional Studies,1
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.","Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. ",12,9,Cross-Sectional Studies,3
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.","Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). ",12,10,Cross-Sectional Studies,3
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.",We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. ,12,11,Cross-Sectional Studies,3
27054362,"Delays in seeking appropriate healthcare can increase the case fatality of acute febrile illnesses, and circuitous routes of care-seeking can have a catastrophic financial impact upon patients in low-income settings. To investigate the relationship between poverty and pre-hospital delays for patients with acute febrile illnesses, we recruited a cross-sectional, convenience sample of 527 acutely ill adults and children aged over 6 months, with a documented fever ≥38.0 °C and symptoms of up to 14 days' duration, presenting to a tertiary referral hospital in Chittagong, Bangladesh, over the course of one year from September 2011 to September 2012. Participants were classified according to the socioeconomic status of their households, defined by the Oxford Poverty and Human Development Initiative's multidimensional poverty index (MPI). 51% of participants were classified as multidimensionally poor (MPI>0.33). Median time from onset of any symptoms to arrival at hospital was 22 hours longer for MPI poor adults compared to non-poor adults (123 vs. 101 hours) rising to a difference of 26 hours with adjustment in a multivariate regression model (95% confidence interval 7 to 46 hours; P = 0.009). There was no difference in delays for children from poor and non-poor households (97 vs. 119 hours; P = 0.394). Case fatality was 5.9% vs. 0.8% in poor and non-poor individuals respectively (P = 0.001)-5.1% vs. 0.0% for poor and non-poor adults (P = 0.010) and 6.4% vs. 1.8% for poor and non-poor children (P = 0.083). Deaths were attributed to central nervous system infection (11), malaria (3), urinary tract infection (2), gastrointestinal infection (1) and undifferentiated sepsis (1). Both poor and non-poor households relied predominantly upon the (often informal) private sector for medical advice before reaching the referral hospital, but MPI poor participants were less likely to have consulted a qualified doctor. Poor participants were more likely to attribute delays in decision-making and travel to a lack of money (P<0.001), and more likely to face catastrophic expenditure of more than 25% of monthly household income (P<0.001). We conclude that multidimensional poverty is associated with greater pre-hospital delays and expenditure in this setting. Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.",Closer links between health and development agendas could address these consequences of poverty and streamline access to adequate healthcare.,12,12,Cross-Sectional Studies,2
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.",There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. ,10,1,Cross-Sectional Studies,0
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.","However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. ",10,2,Cross-Sectional Studies,0
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.","We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n\u202f=\u202f5764, median age 28.8\u202fyears) in south India. ",10,3,Cross-Sectional Studies,0
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.","We examined associations between the neighborhood availability (vendor density per km2 within 400\u202fm and 1600\u202fm buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. ",10,4,Cross-Sectional Studies,0
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.","In fully adjusted models, higher density of fruit/vegetable vendors within 400\u202fm of participant households was associated with lower systolic blood pressure [-0.09\u202fmmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10\u202fmmHg, 95% CI: -0.17, -0.04). ",10,5,Cross-Sectional Studies,3
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.","Higher density of highly processed/take-away food vendors within 400\u202fm of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22\u202fmm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03\u202fmmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03\u202fmmHg, 95% CI: 0.01, 0.05). ",10,6,Cross-Sectional Studies,3
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.","However, within 1600\u202fm buffer, only association with blood pressure remained robust. ",10,7,Cross-Sectional Studies,3
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.",No associations were found for between neighborhood accessibility and cardiovascular risk factors. ,10,8,Cross-Sectional Studies,3
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.","Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. ",10,9,Cross-Sectional Studies,3
31473412,"There has been increasing interest in associations between neighborhood food environments and cardiovascular risk factors. However, results from high-income countries remain inconsistent, and there has been limited research from low- and middle-income countries. We conducted a cross-sectional analysis of the third wave follow-up of the Andhra Pradesh children and parents study (APCAPS) (n = 5764, median age 28.8 years) in south India. We examined associations between the neighborhood availability (vendor density per km2 within 400 m and 1600 m buffers of households) and accessibility (distance from the household to the nearest vendor) of fruit/vegetable and highly processed/take-away food vendors with 11 cardiovascular risk factors, including adiposity measures, glucose-insulin, blood pressure, and lipid profile. In fully adjusted models, higher density of fruit/vegetable vendors within 400 m of participant households was associated with lower systolic blood pressure [-0.09 mmHg, 95% confidence interval (CI): -0.17, -0.02] and diastolic blood pressure (-0.10 mmHg, 95% CI: -0.17, -0.04). Higher density of highly processed/take-away food vendors within 400 m of participant households was associated with higher Body Mass Index (0.01 Kg/m2, 95% CI: 0.00, 0.01), waist circumference (0.22 mm, 95% CI: 0.05, 0.39), systolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.06), and diastolic blood pressure (0.03 mmHg, 95% CI: 0.01, 0.05). However, within 1600 m buffer, only association with blood pressure remained robust. No associations were found for between neighborhood accessibility and cardiovascular risk factors. Lower density of fruit/vegetable vendors, and higher density of highly processed/take-away food vendors were associated with adverse cardiovascular risk profiles. Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.",Public health policies regarding neighborhood food environments should be encouraged in south India and other rural communities in south Asia.,10,10,Cross-Sectional Studies,0
